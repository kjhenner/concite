{"text": "The NonExplicit Relation Detection and Sense and Argument Position classification tasks are cast as supervised classification using AdaBoost algorithm ( Freund and Schapire , 1997 ) implemented in icsiboost ( Favre et al. , 2007 ) .", "metadata": {"citing_string": "Freund and Schapire , 1997", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2107", "citing_paper": "W11-2148"}}
{"text": "With the release of Penn Discourse Treebank ( PDTB ) ( Prasad et al. , 2008 ) , the researchers have developed discourse parsers for all ( e.g. ( Lin et al. , 2014 ) or some ( e.g. ( Ghosh et al. , 2011 ) ) discourse relation types in the PDTB definition , or addressed particular discourse parsing subtasks ( Pitler and Nenkova , 2009 ) .", "metadata": {"citing_string": "Ghosh et al. , 2011", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W00-1412", "citing_paper": "W04-2304"}}
{"text": "Even though previous work on discourse parsing ( e.g. ( Ghosh et al. , 2011 ; Stepanov and Riccardi , 2013 ) found these features useful in token-level sequence labeling approach to Argument Span Extraction using gold parse trees , they were excluded from the submitted models since in greedy hill climbing their contributions were negative .", "metadata": {"citing_string": "Ghosh et al. , 2011", "section_number": 4, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-1902", "citing_paper": "W11-2148"}}
{"text": "The parser makes use of tokenlevel sequence labeling with Conditional Random Fields ( Lafferty et al. , 2001 ) for identification of connective and argument spans ; and classification for identification of relation senses and argument configurations .", "metadata": {"citing_string": "Lafferty et al. , 2001", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0419", "citing_paper": "W10-4304"}}
{"text": "With the release of Penn Discourse Treebank ( PDTB ) ( Prasad et al. , 2008 ) , the researchers have developed discourse parsers for all ( e.g. ( Lin et al. , 2014 ) or some ( e.g. ( Ghosh et al. , 2011 ) ) discourse relation types in the PDTB definition , or addressed particular discourse parsing subtasks ( Pitler and Nenkova , 2009 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-1614", "citing_paper": "W11-2032"}}
{"text": "The approach structures discourse parsing into a pipeline of several subtasks , mimicking the Penn Discourse Treebank ( PDTB ) ( Prasad et al. , 2008 ) annotation procedure as in ( Lin et al. , 2014 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0825", "citing_paper": "W11-2159"}}
{"text": "Even though in 9 % of all inter-sentential relations Argument 1 is located in non-adjacent previous sentence ( Prasad et al. , 2008 ) , this heuristic is widely used ( Lin et al. , 2014 ; Stepanov and Riccardi , 2013 ) , and is known as Previous Sentence Heuristic .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-0135", "citing_paper": "W01-0725"}}
{"text": "With the release of Penn Discourse Treebank ( PDTB ) ( Prasad et al. , 2008 ) , the researchers have developed discourse parsers for all ( e.g. ( Lin et al. , 2014 ) or some ( e.g. ( Ghosh et al. , 2011 ) ) discourse relation types in the PDTB definition , or addressed particular discourse parsing subtasks ( Pitler and Nenkova , 2009 ) .", "metadata": {"citing_string": "Pitler and Nenkova , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2040", "citing_paper": "W13-2815"}}
{"text": "First , last , and first 3 words of each argument ( from ( Pitler et al. , 2009 ; Rutherford and Xue , 2014 ) ) ; 6 .", "metadata": {"citing_string": "Pitler et al. , 2009", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2219", "citing_paper": "W08-0907"}}
{"text": "In PDTB , the Non-Explicit discourse relations -- Implicit , AltLex , and EntRel -- are annotated for pairs of adjacent sentences except the pairs that were already annotated as explicit discourse relations ( Prasad et al. , 2007 ) .", "metadata": {"citing_string": "Prasad et al. , 2007", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2044", "citing_paper": "W12-2021"}}
{"text": "With the release of Penn Discourse Treebank ( PDTB ) ( Prasad et al. , 2008 ) , the researchers have developed discourse parsers for all ( e.g. ( Lin et al. , 2014 ) or some ( e.g. ( Ghosh et al. , 2011 ) ) discourse relation types in the PDTB definition , or addressed particular discourse parsing subtasks ( Pitler and Nenkova , 2009 ) .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1915", "citing_paper": "W09-3734"}}
{"text": "The approach structures discourse parsing into a pipeline of several subtasks , mimicking the Penn Discourse Treebank ( PDTB ) ( Prasad et al. , 2008 ) annotation procedure as in ( Lin et al. , 2014 ) .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1220", "citing_paper": "W12-3149"}}
{"text": "Even though in 9 % of all inter-sentential relations Argument 1 is located in non-adjacent previous sentence ( Prasad et al. , 2008 ) , this heuristic is widely used ( Lin et al. , 2014 ; Stepanov and Riccardi , 2013 ) , and is known as Previous Sentence Heuristic .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-1902", "citing_paper": "W09-0419"}}
{"text": "We have experimented with two approaches : ( 1 ) flat -- directly classifying into full spectrum of senses including class , type and subtype ( Prasad et al. , 2008 ) ; and ( 2 ) hierarchical -- first classifying into 4 top level senses ( Comparison , Contingency , Expansion and Temporal ) and then into the rest of the levels .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2203", "citing_paper": "W00-1412"}}
{"text": "To reduce the sparseness in ( Rutherford and Xue , 2014 ) the authors map the tokens to Brown Clusters ( Turian et al. , 2010 ) and improve the classification into top-level senses .", "metadata": {"citing_string": "Rutherford and Xue , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-1137", "citing_paper": "W05-0203"}}
{"text": "The discourse parser submitted for the CoNLL 2015 Shared Task is the extension of the parser described in ( Stepanov and Riccardi , 2013 ; Stepanov and Riccardi , 2014 ) .", "metadata": {"citing_string": "Stepanov and Riccardi , 2013", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-2107", "citing_paper": "W12-0111"}}
{"text": "Even though in 9 % of all inter-sentential relations Argument 1 is located in non-adjacent previous sentence ( Prasad et al. , 2008 ) , this heuristic is widely used ( Lin et al. , 2014 ; Stepanov and Riccardi , 2013 ) , and is known as Previous Sentence Heuristic .", "metadata": {"citing_string": "Stepanov and Riccardi , 2013", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W04-2415", "citing_paper": "W07-0737"}}
{"text": "Even though previous work on discourse parsing ( e.g. ( Ghosh et al. , 2011 ; Stepanov and Riccardi , 2013 ) found these features useful in token-level sequence labeling approach to Argument Span Extraction using gold parse trees , they were excluded from the submitted models since in greedy hill climbing their contributions were negative .", "metadata": {"citing_string": "Stepanov and Riccardi , 2013", "section_number": 4, "is_self_cite": true, "intent_label": "background", "cited_paper": "W14-2410", "citing_paper": "W00-1403"}}
{"text": "In the literature the task was reported as having a very high baseline ( e.g. ( Stepanov and Riccardi , 2013 ) , 95 % for whole PDTB ) .", "metadata": {"citing_string": "Stepanov and Riccardi , 2013", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W13-2209", "citing_paper": "W06-1403"}}
{"text": "The discourse parser submitted for the CoNLL 2015 Shared Task is the extension of the parser described in ( Stepanov and Riccardi , 2013 ; Stepanov and Riccardi , 2014 ) .", "metadata": {"citing_string": "Stepanov and Riccardi , 2014", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W05-1302", "citing_paper": "W10-4229"}}
{"text": "Additionally , for argument/relation-level features Brown Clusters ( Turian et al. , 2010 ) are used .", "metadata": {"citing_string": "Turian et al. , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-1710", "citing_paper": "W11-2139"}}
{"text": "To reduce the sparseness in ( Rutherford and Xue , 2014 ) the authors map the tokens to Brown Clusters ( Turian et al. , 2010 ) and improve the classification into top-level senses .", "metadata": {"citing_string": "Turian et al. , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0907", "citing_paper": "W00-1403"}}
{"text": "Discourse parsing is a challenging Natural Language Processing ( NLP ) task that has utility for many other NLP tasks such as summarization , opinion mining , etc. ( Webber et al. , 2011 ) .", "metadata": {"citing_string": "Webber et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2117", "citing_paper": "W03-2127"}}
{"text": "ping framework in which the typological features required for prediction of grammatical errors are approximated from automatically extracted ESL morpho-syntactic features using the method of ( Berzak et al. , 2014 ) .", "metadata": {"citing_string": "Berzak et al. , 2014", "section_number": 2, "is_self_cite": true, "intent_label": "result", "cited_paper": "W12-1511", "citing_paper": "W04-0825"}}
{"text": "The current investigation is most closely related to studies that demonstrate that ESL signal can be used to infer pairwise similarities between native languages ( Nagata and Whittaker , 2013 ; Berzak et al. , 2014 ) and in particular , tie", "metadata": {"citing_string": "Berzak et al. , 2014", "section_number": 3, "is_self_cite": true, "intent_label": "result", "cited_paper": "W09-0419", "citing_paper": "W11-1902"}}
{"text": "Given such data , we propose a bootstrapping strategy which uses the method proposed in ( Berzak et al. , 2014 ) in order to approximate the typology of the native language from morpho-syntactic features in ESL .", "metadata": {"citing_string": "Berzak et al. , 2014", "section_number": 7, "is_self_cite": true, "intent_label": "result", "cited_paper": "W07-0720", "citing_paper": "W10-4304"}}
{"text": "The current investigation is most closely related to studies that demonstrate that ESL signal can be used to infer pairwise similarities between native languages ( Nagata and Whittaker , 2013 ; Berzak et al. , 2014 ) and in particular , tie", "metadata": {"citing_string": "Berzak et al. , 2014", "section_number": 3, "is_self_cite": true, "intent_label": "result", "cited_paper": "W08-2127", "citing_paper": "W14-3411"}}
{"text": "Rooted in the comparative linguistics tradition , CA was first suggested by Fries ( 1945 ) and formalized by Lado ( 1957 ) .", "metadata": {"citing_string": "Fries ( 1945 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4163", "citing_paper": "W11-0120"}}
{"text": "Cross linguistic-transfer was extensively studied in SLA , Linguistics and Psychology ( Odlin , 1989 ; Gass and Selinker , 1992 ; Jarvis and Pavlenko , 2007 ) .", "metadata": {"citing_string": "Gass and Selinker , 1992", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2253", "citing_paper": "W02-1610"}}
{"text": "Previous studies ( Daum Â´ e III , 2009 ; Georgi et al. , 2010 ) have estimated that only 14 % of all the language-feature combinations in the database have documented values .", "metadata": {"citing_string": "Georgi et al. , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0209", "citing_paper": "W11-1304"}}
{"text": "Cross linguistic-transfer was extensively studied in SLA , Linguistics and Psychology ( Odlin , 1989 ; Gass and Selinker , 1992 ; Jarvis and Pavlenko , 2007 ) .", "metadata": {"citing_string": "Jarvis and Pavlenko , 2007", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2139", "citing_paper": "W09-3734"}}
{"text": "To this end , we perform a Kruskal-Wallis ( KW ) test ( Kruskal and Wallis , 1952 ) for each error type5 .", "metadata": {"citing_string": "Kruskal and Wallis , 1952", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W01-0725", "citing_paper": "W11-2138"}}
{"text": "The basic hypothesis of CA was formulated by Lado ( 1957 ) , who suggested that `` we can predict and describe the patterns that will cause difficulty in learning , and those that will not cause difficulty , by comparing systematically the language and culture to be learned with the native language and culture of the student '' .", "metadata": {"citing_string": "Lado ( 1957 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W90-0112", "citing_paper": "W03-0312"}}
{"text": "Rooted in the comparative linguistics tradition , CA was first suggested by Fries ( 1945 ) and formalized by Lado ( 1957 ) .", "metadata": {"citing_string": "Lado ( 1957 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2159", "citing_paper": "W11-2044"}}
{"text": "5We chose the non-parametric KW rank-based test over ANOVA , as according to the Shapiro-Wilk ( 1965 ) and Levene ( 1960 ) tests , the assumptions of normality and homogeneity of variance do not hold for our data .", "metadata": {"citing_string": "Levene ( 1960 )", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-3113", "citing_paper": "W10-4163"}}
{"text": "We further extend our analysis by performing pairwise post-hoc Mann-Whitney ( MW ) tests ( Mann and Whitney , 1947 ) in order to determine the number of language pairs that significantly differ with respect to their native speakers ' error fractions in ESL .", "metadata": {"citing_string": "Mann and Whitney , 1947", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1902", "citing_paper": "W04-2304"}}
{"text": "The current investigation is most closely related to studies that demonstrate that ESL signal can be used to infer pairwise similarities between native languages ( Nagata and Whittaker , 2013 ; Berzak et al. , 2014 ) and in particular , tie", "metadata": {"citing_string": "Nagata and Whittaker , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "result", "cited_paper": "W11-2040", "citing_paper": "W11-2107"}}
{"text": "The FCE corpus has an elaborate error annotation scheme ( Nicholls , 2003 ) and high quality of error annotations , making it particularly suitable for our investigation .", "metadata": {"citing_string": "Nicholls , 2003", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-1403", "citing_paper": "W04-0843"}}
{"text": "Differently from the SLA tradition , which emphasizes manual analysis of error case studies ( Odlin , 1989 ) , we address the heart of this controversy from a computational data-driven perspective , focusing on the issue of predictive power .", "metadata": {"citing_string": "Odlin , 1989", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0120", "citing_paper": "W90-0112"}}
{"text": "Cross linguistic-transfer was extensively studied in SLA , Linguistics and Psychology ( Odlin , 1989 ; Gass and Selinker , 1992 ; Jarvis and Pavlenko , 2007 ) .", "metadata": {"citing_string": "Odlin , 1989", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0402", "citing_paper": "W14-3346"}}
{"text": "Previous work on grammatical error correction that examined determiner and preposition errors ( Rozovskaya and Roth , 2011 ; Rozovskaya and Roth , 2014 ) incorporated native language specific priors in models that are otherwise trained on standard English text .", "metadata": {"citing_string": "Rozovskaya and Roth , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1106", "citing_paper": "W13-2253"}}
{"text": "Previous work on grammatical error correction that examined determiner and preposition errors ( Rozovskaya and Roth , 2011 ; Rozovskaya and Roth , 2014 ) incorporated native language specific priors in models that are otherwise trained on standard English text .", "metadata": {"citing_string": "Rozovskaya and Roth , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0604", "citing_paper": "W13-2806"}}
{"text": "A related line of inquiry which is closer to our work deals with the identification of ESL syntactic patterns that are specific to speakers of different native languages ( Swanson and Charniak , 2013 ; Swanson and Charniak , 2014 ) .", "metadata": {"citing_string": "Swanson and Charniak , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3152", "citing_paper": "W11-0809"}}
{"text": "A related line of inquiry which is closer to our work deals with the identification of ESL syntactic patterns that are specific to speakers of different native languages ( Swanson and Charniak , 2013 ; Swanson and Charniak , 2014 ) .", "metadata": {"citing_string": "Swanson and Charniak , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-0111", "citing_paper": "W11-2159"}}
{"text": "Much of this work focuses on experimentation with different feature sets ( Tetreault et al. , 2013 ) , including features derived from the CA framework ( Wong and Dras , 2009 ) .", "metadata": {"citing_string": "Tetreault et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2044", "citing_paper": "W12-3130"}}
{"text": "From its inception , CA was criticized for the lack of a solid predictive theory ( Wardhaugh , 1970 ; Whitman and Jackson , 1972 ) , leading to an ongoing scientific debate on the relevance of comparison based approaches .", "metadata": {"citing_string": "Wardhaugh , 1970", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0604", "citing_paper": "W13-1902"}}
{"text": "From its inception , CA was criticized for the lack of a solid predictive theory ( Wardhaugh , 1970 ; Whitman and Jackson , 1972 ) , leading to an ongoing scientific debate on the relevance of comparison based approaches .", "metadata": {"citing_string": "Whitman and Jackson , 1972", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-2021", "citing_paper": "W08-2127"}}
{"text": "Much of this work focuses on experimentation with different feature sets ( Tetreault et al. , 2013 ) , including features derived from the CA framework ( Wong and Dras , 2009 ) .", "metadata": {"citing_string": "Wong and Dras , 2009", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2159", "citing_paper": "W13-1715"}}
{"text": "We obtain ESL essays from the Cambridge First Certificate in English ( FCE ) learner corpus ( Yannakoudakis et al. , 2011 ) , a publicly available subset of the Cambridge Learner Corpus ( CLC ) 1 .", "metadata": {"citing_string": "Yannakoudakis et al. , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2219", "citing_paper": "W07-0720"}}
{"text": "For example , in the task of identifying paraphrase texts , Bach et al. ( 2014 ) has used discourse information to compute the similarity score between two sentences or Somasundaran et al. ( 2009 ) has used discourse relations to improve the performance of the opinion polarity classification task .", "metadata": {"citing_string": "Bach et al. ( 2014 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1220", "citing_paper": "W01-1615"}}
{"text": "We use CRF + + ( Taku Kudo , 2005 ) , an implementation of the Conditional Random Fields ( John Lafferty et al , 2001 ) to train models from the training data sets .", "metadata": {"citing_string": "Lafferty et al , 2001", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-1403", "citing_paper": "W10-2704"}}
{"text": "Ghosh et al. ( 2011 , 2012 ) have used CRFs with a set of local and global features to recognize arguments of discourses from texts .", "metadata": {"citing_string": "Ghosh et al. ( 2011", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0720", "citing_paper": "W10-1916"}}
{"text": "We use CRF + + ( Taku Kudo , 2005 ) , an implementation of the Conditional Random Fields ( John Lafferty et al , 2001 ) to train models from the training data sets .", "metadata": {"citing_string": "Kudo , 2005", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-1511", "citing_paper": "W06-3113"}}
{"text": "This model used in the above step are built by using the Sequential Minimal Optimization algorithm ( SMO ) , a fast algorithm for training support vector machines ( John Platt , 1998 ) , with some simple features such as : connective words ; type of discourses ( SS or 2CS ) ; does the first character of connective words capital or not ?", "metadata": {"citing_string": "Platt , 1998", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-1111", "citing_paper": "W11-2121"}}
{"text": "For example , in the task of identifying paraphrase texts , Bach et al. ( 2014 ) has used discourse information to compute the similarity score between two sentences or Somasundaran et al. ( 2009 ) has used discourse relations to improve the performance of the opinion polarity classification task .", "metadata": {"citing_string": "Somasundaran et al. ( 2009 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4148", "citing_paper": "W04-1218"}}
{"text": "Lin et al. ( 2009 ) have used supervised learning method to build a maximum entropy classifier to identify implicit relations .", "metadata": {"citing_string": "Lin et al. ( 2009 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3304", "citing_paper": "W13-2806"}}
{"text": "This table also includes the top two systems in the shared task , AI-KU ( Baskaya et al. , 2013 ) and Unimelb ( Lau et al. , 2013 ) , as well as Wang-15 ( Wang et al. , 2015 ) .", "metadata": {"citing_string": "Baskaya et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-2410", "citing_paper": "W14-2410"}}
{"text": "Finally , Wang-15 uses Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) to model the word sense and topic jointly .", "metadata": {"citing_string": "Blei et al. , 2003", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1302", "citing_paper": "W11-2107"}}
{"text": "For example , Ng et al. ( 2003 ) , Chan and Ng ( 2005 ) , and Zhong and Ng ( 2009 ) used Chinese-English parallel corpora to extract samples for training their supervised WSD system .", "metadata": {"citing_string": "Chan and Ng ( 2005 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W90-0112", "citing_paper": "W10-2704"}}
{"text": "To extract training data from the MultiUN parallel corpus , we follow the approach described in ( Chan and Ng , 2005 ) and select the ChineseEnglish part of the MultiUN corpus .", "metadata": {"citing_string": "Chan and Ng , 2005", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2231", "citing_paper": "W98-1419"}}
{"text": "The average time needed to manually assign Chinese translations to the word senses of one word type for noun , adjective , and verb is 20 , 25 , and 40 minutes respectively ( Chan , 2008 ) .", "metadata": {"citing_string": "Chan , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0843", "citing_paper": "W09-3938"}}
{"text": "Diab ( 2004 ) proposed an unsupervised bootstrapping method to automatically generate a senseannotated dataset .", "metadata": {"citing_string": "Diab ( 2004 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4304", "citing_paper": "W04-0405"}}
{"text": "The four all-words WSD shared tasks are SensEval-2 ( Edmonds and Cotton , 2001 ) , SensEval-3 task 1 ( Snyder and Palmer , 2004 ) , and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 ( Pradhan et al. , 2007 ; Navigli et al. , 2007 ) .", "metadata": {"citing_string": "Edmonds and Cotton , 2001", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-3113", "citing_paper": "W09-1111"}}
{"text": "Since the main purpose of this paper is to build and release a publicly available training set for word sense disambiguation systems , we selected the MultiUN corpus ( MUN ) ( Eisele and Chen , 2010 ) produced in the EuroMatrixPlus project1 .", "metadata": {"citing_string": "Eisele and Chen , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1902", "citing_paper": "W13-2219"}}
{"text": "Some researchers believe that , in some cases , WSI methods may perform better than WSD systems ( Jurgens and Klapaftis , 2013 ; Wang et al. , 2015 ) .", "metadata": {"citing_string": "Jurgens and Klapaftis , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0604", "citing_paper": "W05-1302"}}
{"text": "Some researchers believe that , in some cases , WSI methods may perform better than WSD systems ( Jurgens and Klapaftis , 2013 ; Wang et al. , 2015 ) .", "metadata": {"citing_string": "Jurgens and Klapaftis , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0809", "citing_paper": "W01-1615"}}
{"text": "See ( Jurgens and Klapaftis , 2013 ) for details of the evaluation metrics .", "metadata": {"citing_string": "Jurgens and Klapaftis , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W90-0112", "citing_paper": "W10-4304"}}
{"text": "This table also includes the top two systems in the shared task , AI-KU ( Baskaya et al. , 2013 ) and Unimelb ( Lau et al. , 2013 ) , as well as Wang-15 ( Wang et al. , 2015 ) .", "metadata": {"citing_string": "Lau et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2415", "citing_paper": "W14-3346"}}
{"text": "For word sense disambiguation , some systems are based on supervised machine learning algorithms ( Lee et al. , 2004 ; Zhong and Ng , 2010 ) , while others use ontologies and other structured knowledge sources ( Ponzetto and Navigli , 2010 ; Agirre et al. , 2014 ; Moro et al. , 2014 ) .", "metadata": {"citing_string": "Lee et al. , 2004", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3149", "citing_paper": "W11-0809"}}
{"text": "Tokenization and word segmentation : The English side of the corpus is tokenized using the Penn TreeBank tokenizer3 , while the Chinese side of the corpus is segmented using the Chinese word segmenter of ( Low et al. , 2005 ) .", "metadata": {"citing_string": "Low et al. , 2005", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W10-1007"}}
{"text": "There are several sense-annotated datasets for WSD ( Miller et al. , 1993 ; Ng and Lee , 1996 ; Passonneau et al. , 2012 ) .", "metadata": {"citing_string": "Miller et al. , 1993", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0308", "citing_paper": "W00-1220"}}
{"text": "In order to improve the coverage of the training set , we augment it by adding samples from SEMCOR ( SC ) ( Miller et al. , 1993 ) and the DSO cor -", "metadata": {"citing_string": "Miller et al. , 1993", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2309", "citing_paper": "W05-1208"}}
{"text": "First , we employ the same method used in ( Ng et al. , 2003 ; Chan and Ng , 2005 ) to semi-automatically annotate one million training samples based on the WordNet sense inventory ( Miller , 1995 ) and release the annotated corpus for public use .", "metadata": {"citing_string": "Miller , 1995", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W01-1615", "citing_paper": "W12-2021"}}
{"text": "For word sense disambiguation , some systems are based on supervised machine learning algorithms ( Lee et al. , 2004 ; Zhong and Ng , 2010 ) , while others use ontologies and other structured knowledge sources ( Ponzetto and Navigli , 2010 ; Agirre et al. , 2014 ; Moro et al. , 2014 ) .", "metadata": {"citing_string": "Moro et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1104", "citing_paper": "W11-2134"}}
{"text": "The four all-words WSD shared tasks are SensEval-2 ( Edmonds and Cotton , 2001 ) , SensEval-3 task 1 ( Snyder and Palmer , 2004 ) , and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 ( Pradhan et al. , 2007 ; Navigli et al. , 2007 ) .", "metadata": {"citing_string": "Navigli et al. , 2007", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2231", "citing_paper": "W13-1902"}}
{"text": "The approaches used to solve this problem can be roughly categorized into two main classes : Word Sense Disambiguation ( WSD ) and Word Sense Induction ( WSI ) ( Navigli , 2009 ) .", "metadata": {"citing_string": "Navigli , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1104", "citing_paper": "W10-1010"}}
{"text": "There are several sense-annotated datasets for WSD ( Miller et al. , 1993 ; Ng and Lee , 1996 ; Passonneau et al. , 2012 ) .", "metadata": {"citing_string": "Ng and Lee , 1996", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-1710", "citing_paper": "W00-1220"}}
{"text": "There are several sense-annotated datasets for WSD ( Miller et al. , 1993 ; Ng and Lee , 1996 ; Passonneau et al. , 2012 ) .", "metadata": {"citing_string": "Ng and Lee , 1996", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1915", "citing_paper": "W00-1220"}}
{"text": "For example , Ng et al. ( 2003 ) , Chan and Ng ( 2005 ) , and Zhong and Ng ( 2009 ) used Chinese-English parallel corpora to extract samples for training their supervised WSD system .", "metadata": {"citing_string": "Ng et al. ( 2003 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0720", "citing_paper": "W12-3149"}}
{"text": "Word alignment : After tokenizing the texts , GIZA + + ( Och and Ney , 2000 ) is used to align English and Chinese words .", "metadata": {"citing_string": "Och and Ney , 2000", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0737", "citing_paper": "W11-2121"}}
{"text": "There are several sense-annotated datasets for WSD ( Miller et al. , 1993 ; Ng and Lee , 1996 ; Passonneau et al. , 2012 ) .", "metadata": {"citing_string": "Passonneau et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W11-2134"}}
{"text": "For word sense disambiguation , some systems are based on supervised machine learning algorithms ( Lee et al. , 2004 ; Zhong and Ng , 2010 ) , while others use ontologies and other structured knowledge sources ( Ponzetto and Navigli , 2010 ; Agirre et al. , 2014 ; Moro et al. , 2014 ) .", "metadata": {"citing_string": "Ponzetto and Navigli , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W09-3938"}}
{"text": "The four all-words WSD shared tasks are SensEval-2 ( Edmonds and Cotton , 2001 ) , SensEval-3 task 1 ( Snyder and Palmer , 2004 ) , and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 ( Pradhan et al. , 2007 ; Navigli et al. , 2007 ) .", "metadata": {"citing_string": "Pradhan et al. , 2007", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1715", "citing_paper": "W14-3411"}}
{"text": "The four all-words WSD shared tasks are SensEval-2 ( Edmonds and Cotton , 2001 ) , SensEval-3 task 1 ( Snyder and Palmer , 2004 ) , and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 ( Pradhan et al. , 2007 ; Navigli et al. , 2007 ) .", "metadata": {"citing_string": "Snyder and Palmer , 2004", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2806", "citing_paper": "W05-1617"}}
{"text": "Unimelb relies on Hierarchical Dirichlet Process ( Teh et al. , 2006 ) to identify the sense of a target word using positional word features .", "metadata": {"citing_string": "Teh et al. , 2006", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3734", "citing_paper": "W09-0419"}}
{"text": "Some researchers believe that , in some cases , WSI methods may perform better than WSD systems ( Jurgens and Klapaftis , 2013 ; Wang et al. , 2015 ) .", "metadata": {"citing_string": "Wang et al. , 2015", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W01-0725", "citing_paper": "W11-1304"}}
{"text": "This table also includes the top two systems in the shared task , AI-KU ( Baskaya et al. , 2013 ) and Unimelb ( Lau et al. , 2013 ) , as well as Wang-15 ( Wang et al. , 2015 ) .", "metadata": {"citing_string": "Wang et al. , 2015", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0405", "citing_paper": "W13-1902"}}
{"text": "For example , Ng et al. ( 2003 ) , Chan and Ng ( 2005 ) , and Zhong and Ng ( 2009 ) used Chinese-English parallel corpora to extract samples for training their supervised WSD system .", "metadata": {"citing_string": "Zhong and Ng ( 2009 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0317", "citing_paper": "W11-2107"}}
{"text": "For word sense disambiguation , some systems are based on supervised machine learning algorithms ( Lee et al. , 2004 ; Zhong and Ng , 2010 ) , while others use ontologies and other structured knowledge sources ( Ponzetto and Navigli , 2010 ; Agirre et al. , 2014 ; Moro et al. , 2014 ) .", "metadata": {"citing_string": "Zhong and Ng , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0317", "citing_paper": "W04-0825"}}
{"text": "Second , we train an open source supervised WSD system , IMS ( Zhong and Ng , 2010 ) , using our data and evaluate it against standard WSD and WSI benchmarks .", "metadata": {"citing_string": "Zhong and Ng , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1733", "citing_paper": "W13-2806"}}
{"text": "For the WSD system , we use IMS ( Zhong and Ng , 2010 ) in our experiments .", "metadata": {"citing_string": "Zhong and Ng , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-3130", "citing_paper": "W08-2127"}}
{"text": "In Table 4 , IMS ( original ) refers to the IMS system trained with the original training instances as reported in ( Zhong and Ng , 2010 ) .", "metadata": {"citing_string": "Zhong and Ng , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2203", "citing_paper": "W04-0209"}}
{"text": "Such world interaction consists of database access in semantic parsing ( Kwiatowski et al. ( 2013 ) , Berant et al. ( 2013 ) , or Goldwasser and Roth ( 2013 ) , inter alia ) .", "metadata": {"citing_string": "Berant et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1420", "citing_paper": "W08-0308"}}
{"text": "Following the techniques described in Bertsekas ( 2011 ) to generate random sequences for incremental optimization , we compared cyclic order ( K epochs of T examples in fixed order ) , randomized order ( sampling datapoints with replacement ) , and random shuffling of datapoints after each cycle , and found nearly identical regret curves for all three scenarios .", "metadata": {"citing_string": "Bertsekas ( 2011 )", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2231", "citing_paper": "W11-2044"}}
{"text": "perceptron cycling theorem ( Block and Levin , 1970 ; Gelfand et al. , 2010 ) should suffice to show a similar bound .", "metadata": {"citing_string": "Block and Levin , 1970", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0843", "citing_paper": "W11-2159"}}
{"text": "Compared to batch learning , its advantages include memory efficiency , due to parameter updates being performed on the basis of single examples , and runtime efficiency , where a constant number of passes over the training sample is sufficient for convergence ( Bottou and Bousquet , 2004 ) .", "metadata": {"citing_string": "Bottou and Bousquet , 2004", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-2021", "citing_paper": "W13-3605"}}
{"text": "Lastly , feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning ( Sutton and Barto , 1998 ) or in online learning with limited feedback , e.g. , multi-armed bandit models ( Cesa-Bianchi and Lugosi , 2006 ) .", "metadata": {"citing_string": "Cesa-Bianchi and Lugosi , 2006", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3304", "citing_paper": "W13-2815"}}
{"text": "First we present a generalization bound for the case of online learning on a sequence of random examples , based on generalization bounds for expected average regret as given by Cesa-Bianchi et al. ( 2004 ) .", "metadata": {"citing_string": "Cesa-Bianchi et al. ( 2004 )", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0312", "citing_paper": "W10-1902"}}
{"text": "The generalization bound tells us how far the expected average regret E [ REGT ] ( or average risk , in terms of Cesa-Bianchi et al. ( 2004 ) ) is from the average regret that we actually observe in a specific instantiation of the algorithm .", "metadata": {"citing_string": "Cesa-Bianchi et al. ( 2004 )", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0809", "citing_paper": "W04-2304"}}
{"text": "Online learning has been applied for discriminative training in SMT , based on perceptron-type algorithms ( Shen et al. ( 2004 ) , Watanabe et al. ( 2006 ) , Liang et al. ( 2006 ) , Yu et al. ( 2013 ) , inter alia ) , or large-margin approaches ( Tillmann and Zhang ( 2006 ) , Watanabe et al. ( 2007 ) , Chiang et al. ( 2008 ) , Chiang et al. ( 2009 ) , Chiang ( 2012 ) , inter alia ) .", "metadata": {"citing_string": "Chiang et al. ( 2008 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1715", "citing_paper": "W12-0111"}}
{"text": "Online learning has been applied for discriminative training in SMT , based on perceptron-type algorithms ( Shen et al. ( 2004 ) , Watanabe et al. ( 2006 ) , Liang et al. ( 2006 ) , Yu et al. ( 2013 ) , inter alia ) , or large-margin approaches ( Tillmann and Zhang ( 2006 ) , Watanabe et al. ( 2007 ) , Chiang et al. ( 2008 ) , Chiang et al. ( 2009 ) , Chiang ( 2012 ) , inter alia ) .", "metadata": {"citing_string": "Chiang et al. ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-3130", "citing_paper": "W11-2138"}}
{"text": "We show experimentally that learning from surrogate `` hope '' derivations ( Chiang , 2012 ) minimizes regret and TER , thus favoring surrogacy modes that admit an underlying linear model , over `` local '' updates ( Liang et al. , 2006 ) or `` oracle '' derivations ( Sokolov et al. , 2013 ) , for which learning does not converge .", "metadata": {"citing_string": "Chiang , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-2410", "citing_paper": "W13-4068"}}
{"text": "Online learning has been applied for discriminative training in SMT , based on perceptron-type algorithms ( Shen et al. ( 2004 ) , Watanabe et al. ( 2006 ) , Liang et al. ( 2006 ) , Yu et al. ( 2013 ) , inter alia ) , or large-margin approaches ( Tillmann and Zhang ( 2006 ) , Watanabe et al. ( 2007 ) , Chiang et al. ( 2008 ) , Chiang et al. ( 2009 ) , Chiang ( 2012 ) , inter alia ) .", "metadata": {"citing_string": "Chiang ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2203", "citing_paper": "W00-1412"}}
{"text": "This corresponds to what Chiang ( 2012 ) termed `` hope derivations '' .", "metadata": {"citing_string": "Chiang ( 2012 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3152", "citing_paper": "W14-0201"}}
{"text": "In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; Collins , 2002 ) .", "metadata": {"citing_string": "Collins , 2002", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0809", "citing_paper": "W98-1419"}}
{"text": "For example , the objective function actually optimized in Liang et al. 's ( 2006 ) application of Collins ' ( 2002 ) structure perceptron has been analyzed by Gimpel and Smith ( 2012 ) as a non-convex ramp loss function ( McAllester and Keshet , 2011 ; Do et al. , 2008 ; Collobert et al. , 2006 ) .", "metadata": {"citing_string": "Collobert et al. , 2006", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2139", "citing_paper": "W08-2127"}}
{"text": "Recent approaches extend online parameter updating by online phrase extraction ( W Â¨ aschle et al. ( 2013 ) , Bertoldi et al. ( 2014 ) , Denkowski et al. ( 2014 ) , Green et al. ( 2014 ) , inter alia ) .", "metadata": {"citing_string": "Denkowski et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3329", "citing_paper": "W12-1511"}}
{"text": "For example , the objective function actually optimized in Liang et al. 's ( 2006 ) application of Collins ' ( 2002 ) structure perceptron has been analyzed by Gimpel and Smith ( 2012 ) as a non-convex ramp loss function ( McAllester and Keshet , 2011 ; Do et al. , 2008 ; Collobert et al. , 2006 ) .", "metadata": {"citing_string": "Do et al. , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0720", "citing_paper": "W13-2203"}}
{"text": "Parallel data ( europarl + news-comm , 1.64 M sentences ) were similarly pre-processed and aligned with fast align ( Dyer et al. , 2013 ) .", "metadata": {"citing_string": "Dyer et al. , 2013", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2231", "citing_paper": "W12-3130"}}
{"text": "The latest incarnations are able to handle millions of features and millions of parallel sentences ( Simianer et al. ( 2012 ) , Eidelmann ( 2012 ) , Watanabe ( 2012 ) , Green et al. ( 2013 ) , inter alia ) .", "metadata": {"citing_string": "Eidelmann ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3152", "citing_paper": "W06-0135"}}
{"text": "In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; Collins , 2002 ) .", "metadata": {"citing_string": "Freund and Schapire , 1999", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-1614", "citing_paper": "W10-1916"}}
{"text": "For example , the objective function actually optimized in Liang et al. 's ( 2006 ) application of Collins ' ( 2002 ) structure perceptron has been analyzed by Gimpel and Smith ( 2012 ) as a non-convex ramp loss function ( McAllester and Keshet , 2011 ; Do et al. , 2008 ; Collobert et al. , 2006 ) .", "metadata": {"citing_string": "Gimpel and Smith ( 2012 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1733", "citing_paper": "W13-2203"}}
{"text": "Furthermore , an online learning framework such as coactive learning covers problems such as changing n-best lists after each update that were explicitly excluded from the batch analysis of Gimpel and Smith ( 2012 ) and considered fixed in the analysis of Sun et al. ( 2013 ) .", "metadata": {"citing_string": "Gimpel and Smith ( 2012 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1420", "citing_paper": "W10-4163"}}
{"text": "Such world interaction consists of database access in semantic parsing ( Kwiatowski et al. ( 2013 ) , Berant et al. ( 2013 ) , or Goldwasser and Roth ( 2013 ) , inter alia ) .", "metadata": {"citing_string": "Goldwasser and Roth ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2134", "citing_paper": "W08-2231"}}
{"text": "The latest incarnations are able to handle millions of features and millions of parallel sentences ( Simianer et al. ( 2012 ) , Eidelmann ( 2012 ) , Watanabe ( 2012 ) , Green et al. ( 2013 ) , inter alia ) .", "metadata": {"citing_string": "Green et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-4068", "citing_paper": "W11-2040"}}
{"text": "Recent approaches extend online parameter updating by online phrase extraction ( W Â¨ aschle et al. ( 2013 ) , Bertoldi et al. ( 2014 ) , Denkowski et al. ( 2014 ) , Green et al. ( 2014 ) , inter alia ) .", "metadata": {"citing_string": "Green et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W13-2209"}}
{"text": "We replicated a similar Moses system using the same monolingual and parallel data : a 5-gram language model was estimated with the KenLM toolkit ( Heafield , 2011 ) on news.en data ( 48.65 M sentences , 1.13 B tokens ) , pre-processed with the tools from the cdec toolkit ( Dyer et al. , 2010 ) .", "metadata": {"citing_string": "Heafield , 2011", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2032", "citing_paper": "W04-1218"}}
{"text": "To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( Koehn et al. , 2007 ) with dense features .", "metadata": {"citing_string": "Koehn et al. , 2007", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0317", "citing_paper": "W14-0201"}}
{"text": "Such world interaction consists of database access in semantic parsing ( Kwiatowski et al. ( 2013 ) , Berant et al. ( 2013 ) , or Goldwasser and Roth ( 2013 ) , inter alia ) .", "metadata": {"citing_string": "Kwiatowski et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0120", "citing_paper": "W06-3113"}}
{"text": "For example , the objective function actually optimized in Liang et al. 's ( 2006 ) application of Collins ' ( 2002 ) structure perceptron has been analyzed by Gimpel and Smith ( 2012 ) as a non-convex ramp loss function ( McAllester and Keshet , 2011 ; Do et al. , 2008 ; Collobert et al. , 2006 ) .", "metadata": {"citing_string": "McAllester and Keshet , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-0135", "citing_paper": "W04-1218"}}
{"text": "This framework allows us to make three main contributions : â¢ Firstly , the proof techniques of Shivaswamy and Joachims ( 2012 ) are a simple and elegant tool for a theoretical analysis of perceptron-style algorithms that date back to the perceptron mistake bound of Novikoff ( 1962 ) .", "metadata": {"citing_string": "Novikoff ( 1962 )", "section_number": 2, "is_self_cite": false, "intent_label": "result", "cited_paper": "W14-1807", "citing_paper": "W10-1916"}}
{"text": "A proof for Theorem 1 is similar to the proof of Shivaswamy and Joachims ( 2012 ) and the original mistake bound for the perceptron of Novikoff ( 1962 ) .1 The theorem can be interpreted as follows : we expect lower average regret for higher values of Î± ; due to the dominant term T , regret will approach the minimum of the accumulated slack ( in case feedback structures violate Equation ( 2 ) ) or 0 ( in case of strictly Î±-informative feedback ) .", "metadata": {"citing_string": "Novikoff ( 1962 )", "section_number": 4, "is_self_cite": false, "intent_label": "result", "cited_paper": "W13-2806", "citing_paper": "W12-2021"}}
{"text": "This was done by MERT optimization ( Och , 2003 ) towards post-edits under the TER target metric .", "metadata": {"citing_string": "Och , 2003", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0405", "citing_paper": "W14-3411"}}
{"text": "To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( Koehn et al. , 2007 ) with dense features .", "metadata": {"citing_string": "Potet et al. , 2010", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W13-1715"}}
{"text": "or post-edits on the output from similar SMT systems , are used as for online learning ( CesaBianchi et al. ( 2008 ) , L Â´ opez-Salcedo et al. ( 2012 ) , Mart Â´ Ä±nez-G Â´ omez et al. ( 2012 ) , Saluja et al. ( 2012 ) , Saluja and Zhang ( 2014 ) , inter alia ) .", "metadata": {"citing_string": "Saluja and Zhang ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0405", "citing_paper": "W03-0312"}}
{"text": "or post-edits on the output from similar SMT systems , are used as for online learning ( CesaBianchi et al. ( 2008 ) , L Â´ opez-Salcedo et al. ( 2012 ) , Mart Â´ Ä±nez-G Â´ omez et al. ( 2012 ) , Saluja et al. ( 2012 ) , Saluja and Zhang ( 2014 ) , inter alia ) .", "metadata": {"citing_string": "Saluja et al. ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-1406", "citing_paper": "W08-1137"}}
{"text": "Online learning algorithms can be given a theoretical analysis in the framework of online convex optimization ( Shalev-Shwartz , 2012 ) , however , the application of online learning techniques to SMT sacrifices convexity because of latent derivation variables , and because of surrogate translations replacing human references that are unreachable in the decoder search space .", "metadata": {"citing_string": "Shalev-Shwartz , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0308", "citing_paper": "W14-1807"}}
{"text": "Online learning has been applied for discriminative training in SMT , based on perceptron-type algorithms ( Shen et al. ( 2004 ) , Watanabe et al. ( 2006 ) , Liang et al. ( 2006 ) , Yu et al. ( 2013 ) , inter alia ) , or large-margin approaches ( Tillmann and Zhang ( 2006 ) , Watanabe et al. ( 2007 ) , Chiang et al. ( 2008 ) , Chiang et al. ( 2009 ) , Chiang ( 2012 ) , inter alia ) .", "metadata": {"citing_string": "Shen et al. ( 2004 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-1111", "citing_paper": "W03-2127"}}
{"text": "The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning ( Shivaswamy and Joachims , 2012 ) .", "metadata": {"citing_string": "Shivaswamy and Joachims , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "result", "cited_paper": "W12-2021", "citing_paper": "W08-2117"}}
{"text": "Our work builds on the framework of coactive learning , introduced by Shivaswamy and Joachims ( 2012 ) .", "metadata": {"citing_string": "Shivaswamy and Joachims ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "result", "cited_paper": "W00-1403", "citing_paper": "W03-1710"}}
{"text": "A proof for Theorem 1 is similar to the proof of Shivaswamy and Joachims ( 2012 ) and the original mistake bound for the perceptron of Novikoff ( 1962 ) .1 The theorem can be interpreted as follows : we expect lower average regret for higher values of Î± ; due to the dominant term T , regret will approach the minimum of the accumulated slack ( in case feedback structures violate Equation ( 2 ) ) or 0 ( in case of strictly Î±-informative feedback ) .", "metadata": {"citing_string": "Shivaswamy and Joachims ( 2012 )", "section_number": 4, "is_self_cite": false, "intent_label": "result", "cited_paper": "W04-0853", "citing_paper": "W05-1617"}}
{"text": "To quantify the amount of information in the weak feedback , Shivaswamy and Joachims ( 2012 ) define a notion of Î±-informative feedback , which we generalize as follows for the case of latent derivations .", "metadata": {"citing_string": "Shivaswamy and Joachims ( 2012 )", "section_number": 4, "is_self_cite": false, "intent_label": "result", "cited_paper": "W09-1111", "citing_paper": "W14-1807"}}
{"text": "The main difference between the above result and the result of Shivaswamy and Joachims ( 2012 ) is the term DT following from the rescaled distance of latent derivations .", "metadata": {"citing_string": "Shivaswamy and Joachims ( 2012 )", "section_number": 4, "is_self_cite": false, "intent_label": "result", "cited_paper": "W11-2138", "citing_paper": "W06-1420"}}
{"text": "The latest incarnations are able to handle millions of features and millions of parallel sentences ( Simianer et al. ( 2012 ) , Eidelmann ( 2012 ) , Watanabe ( 2012 ) , Green et al. ( 2013 ) , inter alia ) .", "metadata": {"citing_string": "Simianer et al. ( 2012 )", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W10-2704", "citing_paper": "W13-2219"}}
{"text": "We show that weak feedback structures correspond to improvements in TER ( Snover et al. , 2006 ) over predicted structures , and that learning from weak feedback minimizes regret and TER .", "metadata": {"citing_string": "Snover et al. , 2006", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4163", "citing_paper": "W01-1615"}}
{"text": "We also extend our own previous work ( Sokolov et al. , 2015 ) with theory and experiments for online-tobatch conversion , and with experiments on coactive learning from surrogate translations .", "metadata": {"citing_string": "Sokolov et al. , 2015", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-1715", "citing_paper": "W11-2044"}}
{"text": "Our analysis covers the approach of Liang et al. ( 2006 ) and supersedes Sun et al. ( 2013 ) 's analysis of the latent perceptron by providing simpler proofs and by adding a generalization analysis .", "metadata": {"citing_string": "Sun et al. ( 2013 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-1807", "citing_paper": "W10-4163"}}
{"text": "Our theoretical analysis is easily extendable to the full information case of Sun et al. ( 2013 ) .", "metadata": {"citing_string": "Sun et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0317", "citing_paper": "W01-1615"}}
{"text": "Algorithm 1 is called '' Feedbackbased Latent Perceptron '' to stress the fact that it only uses weak feedback to its predictions for learning , but does not necessarily observe optimal structures as in the full information case ( Sun et al. , 2013 ) .", "metadata": {"citing_string": "Sun et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1733", "citing_paper": "W11-1902"}}
{"text": "Lastly , feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning ( Sutton and Barto , 1998 ) or in online learning with limited feedback , e.g. , multi-armed bandit models ( Cesa-Bianchi and Lugosi , 2006 ) .", "metadata": {"citing_string": "Sutton and Barto , 1998", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-1902", "citing_paper": "W09-1111"}}
{"text": "Online learning has been applied for discriminative training in SMT , based on perceptron-type algorithms ( Shen et al. ( 2004 ) , Watanabe et al. ( 2006 ) , Liang et al. ( 2006 ) , Yu et al. ( 2013 ) , inter alia ) , or large-margin approaches ( Tillmann and Zhang ( 2006 ) , Watanabe et al. ( 2007 ) , Chiang et al. ( 2008 ) , Chiang et al. ( 2009 ) , Chiang ( 2012 ) , inter alia ) .", "metadata": {"citing_string": "Tillmann and Zhang ( 2006 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2134", "citing_paper": "W01-1615"}}
{"text": "Online learning has been applied for discriminative training in SMT , based on perceptron-type algorithms ( Shen et al. ( 2004 ) , Watanabe et al. ( 2006 ) , Liang et al. ( 2006 ) , Yu et al. ( 2013 ) , inter alia ) , or large-margin approaches ( Tillmann and Zhang ( 2006 ) , Watanabe et al. ( 2007 ) , Chiang et al. ( 2008 ) , Chiang et al. ( 2009 ) , Chiang ( 2012 ) , inter alia ) .", "metadata": {"citing_string": "Watanabe et al. ( 2006 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-3149", "citing_paper": "W04-0843"}}
{"text": "Online learning has been applied for discriminative training in SMT , based on perceptron-type algorithms ( Shen et al. ( 2004 ) , Watanabe et al. ( 2006 ) , Liang et al. ( 2006 ) , Yu et al. ( 2013 ) , inter alia ) , or large-margin approaches ( Tillmann and Zhang ( 2006 ) , Watanabe et al. ( 2007 ) , Chiang et al. ( 2008 ) , Chiang et al. ( 2009 ) , Chiang ( 2012 ) , inter alia ) .", "metadata": {"citing_string": "Watanabe et al. ( 2007 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2040", "citing_paper": "W03-1710"}}
{"text": "The latest incarnations are able to handle millions of features and millions of parallel sentences ( Simianer et al. ( 2012 ) , Eidelmann ( 2012 ) , Watanabe ( 2012 ) , Green et al. ( 2013 ) , inter alia ) .", "metadata": {"citing_string": "Watanabe ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0907", "citing_paper": "W09-0419"}}
{"text": "Online learning has been applied for discriminative training in SMT , based on perceptron-type algorithms ( Shen et al. ( 2004 ) , Watanabe et al. ( 2006 ) , Liang et al. ( 2006 ) , Yu et al. ( 2013 ) , inter alia ) , or large-margin approaches ( Tillmann and Zhang ( 2006 ) , Watanabe et al. ( 2007 ) , Chiang et al. ( 2008 ) , Chiang et al. ( 2009 ) , Chiang ( 2012 ) , inter alia ) .", "metadata": {"citing_string": "Yu et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2806", "citing_paper": "W13-1902"}}
{"text": "Agirre et al. ( 2009 ) partitioned the wordsim353 dataset into two subsets , one focused on similarity and the other on association .", "metadata": {"citing_string": "Agirre et al. ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-0135", "citing_paper": "W10-4304"}}
{"text": "As our word embeddings are designed to support word similarity rather than relatedness , we focus on the similarity subset of this dataset , according to the division presented in ( Agirre et al. , 2009 ) .", "metadata": {"citing_string": "Agirre et al. , 2009", "section_number": 7, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1733", "citing_paper": "W06-3113"}}
{"text": "The Strudel system ( Baroni et al. , 2010 ) represents a word using the clusters of lexico-syntactic patterns in which it occurs .", "metadata": {"citing_string": "Baroni et al. , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-0102", "citing_paper": "W07-0720"}}
{"text": "VSM approaches have resulted in highly useful word embeddings , obtaining high quality results on various semantic tasks ( Baroni et al. , 2014 ) .", "metadata": {"citing_string": "Baroni et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0419", "citing_paper": "W13-1915"}}
{"text": "3PPMI was shown useful for various co-occurrence models ( Baroni et al. , 2014 ) .", "metadata": {"citing_string": "Baroni et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-1807", "citing_paper": "W09-0419"}}
{"text": "More Recently , Neural Networks have become prominent in word representation learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert et al. , 2011 ; Mikolov et al. , 2013a ; Pennington et al. , 2014 , inter alia ) .", "metadata": {"citing_string": "Bengio et al. , 2003", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0405", "citing_paper": "W01-1615"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Bengio et al. , 2003", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0120", "citing_paper": "W10-4229"}}
{"text": "In previous work patterns were used to represent a variety of semantic relations , including hyponymy ( Hearst , 1992 ) , meronymy ( Berland and Charniak , 1999 ) and antonymy ( Lin et al. , 2003 ) .", "metadata": {"citing_string": "Berland and Charniak , 1999", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2127", "citing_paper": "W08-0317"}}
{"text": "Patterns ( symmetric or not ) were found useful in a variety of NLP tasks , including identification of word relations such as hyponymy ( Hearst , 1992 ) , meronymy ( Berland and Charniak , 1999 ) and antonymy ( Lin et al. , 2003 ) .", "metadata": {"citing_string": "Berland and Charniak , 1999", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2159", "citing_paper": "W04-1106"}}
{"text": "Bollegala et al. ( 2015 ) replaced bag-of-words contexts with various patterns ( lexical , POS and dependency ) .", "metadata": {"citing_string": "Bollegala et al. ( 2015 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2415", "citing_paper": "W14-3346"}}
{"text": "Recently , Yih et al. ( 2012 ) , Chang et al. ( 2013 ) and Ono et al. ( 2015 ) proposed word representation methods that assign dissimilar vectors to antonyms .", "metadata": {"citing_string": "Chang et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2253", "citing_paper": "W09-0604"}}
{"text": "Our corpus consists of four datasets : ( a ) The 2012 and 2013 crawled news articles from the ACL 2014 workshop on statistical machine translation ( Bojar et al. , 2014 ) ; 8 ( b ) The One Billion Word Benchmark of Chelba et al. ( 2013 ) ; 9 ( c ) The UMBC corpus ( Han et al. , 2013 ) ; 10 and ( d ) The September 2014 dump of the English Wikipedia .11", "metadata": {"citing_string": "Chelba et al. ( 2013 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-1111", "citing_paper": "W04-0405"}}
{"text": "For recent surveys , see ( Turney et al. , 2010 ; Clark , 2012 ; Erk , 2012 ) .", "metadata": {"citing_string": "Clark , 2012", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1220", "citing_paper": "W10-1902"}}
{"text": "More Recently , Neural Networks have become prominent in word representation learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert et al. , 2011 ; Mikolov et al. , 2013a ; Pennington et al. , 2014 , inter alia ) .", "metadata": {"citing_string": "Collobert and Weston , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0308", "citing_paper": "W04-1106"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Collobert and Weston , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-0201", "citing_paper": "W04-2304"}}
{"text": "More Recently , Neural Networks have become prominent in word representation learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert et al. , 2011 ; Mikolov et al. , 2013a ; Pennington et al. , 2014 , inter alia ) .", "metadata": {"citing_string": "Collobert et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2107", "citing_paper": "W11-2040"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Collobert et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1220", "citing_paper": "W08-1137"}}
{"text": "Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition ( Widdows and Dorow , 2002 ) , word clustering ( Davidov and Rappoport , 2006 ) and classification of words to semantic categories ( Schwartz et al. , 2014 ) .", "metadata": {"citing_string": "Davidov and Rappoport , 2006", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W04-2304", "citing_paper": "W09-1111"}}
{"text": "Dorow et al. ( 2005 ) and Davidov and Rappoport ( 2006 ) used them to perform unsupervised clustering of words .", "metadata": {"citing_string": "Davidov and Rappoport ( 2006 )", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W11-2139", "citing_paper": "W11-2159"}}
{"text": "In this work we follow ( Davidov and Rappoport , 2006 ) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text .", "metadata": {"citing_string": "Davidov and Rappoport , 2006", "section_number": 4, "is_self_cite": true, "intent_label": "method", "cited_paper": "W14-3329", "citing_paper": "W11-1406"}}
{"text": "tackle sentence level tasks such as identification of sarcasm ( Tsur et al. , 2010 ) , sentiment analysis ( Davidov et al. , 2010 ) and authorship attribution ( Schwartz et al. , 2013 ) .", "metadata": {"citing_string": "Davidov et al. , 2010", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W13-2203", "citing_paper": "W11-0402"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Dhillon et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2253", "citing_paper": "W11-2121"}}
{"text": "Dorow et al. ( 2005 ) and Davidov and Rappoport ( 2006 ) used them to perform unsupervised clustering of words .", "metadata": {"citing_string": "Dorow et al. ( 2005 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3938", "citing_paper": "W10-1902"}}
{"text": "For recent surveys , see ( Turney et al. , 2010 ; Clark , 2012 ; Erk , 2012 ) .", "metadata": {"citing_string": "Erk , 2012", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1304", "citing_paper": "W04-0825"}}
{"text": "Feng et al. ( 2013 ) used SPs to build a connotation lexicon , and Schwartz et al. ( 2014 ) used SPs to perform minimally supervised classification of words into semantic categories .", "metadata": {"citing_string": "Feng et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3329", "citing_paper": "W11-1902"}}
{"text": "The most prominent patterns in these works are `` X and Y '' and `` X or Y '' ( Widdows and Dorow , 2002 ; Feng et al. , 2013 ) .", "metadata": {"citing_string": "Feng et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1733", "citing_paper": "W11-2121"}}
{"text": "Indeed , when evaluating these VSMs with datasets such as wordsim353 ( Finkelstein et al. , 2001 ) , where the word pair scores re -", "metadata": {"citing_string": "Finkelstein et al. , 2001", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4148", "citing_paper": "W12-2021"}}
{"text": "The wordsim353 dataset ( Finkelstein et al. , 2001 ) is frequently used for evaluating word representations .", "metadata": {"citing_string": "Finkelstein et al. , 2001", "section_number": 7, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0825", "citing_paper": "W08-0317"}}
{"text": "Our corpus consists of four datasets : ( a ) The 2012 and 2013 crawled news articles from the ACL 2014 workshop on statistical machine translation ( Bojar et al. , 2014 ) ; 8 ( b ) The One Billion Word Benchmark of Chelba et al. ( 2013 ) ; 9 ( c ) The UMBC corpus ( Han et al. , 2013 ) ; 10 and ( d ) The September 2014 dump of the English Wikipedia .11", "metadata": {"citing_string": "Han et al. , 2013", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0405", "citing_paper": "W04-0825"}}
{"text": "In previous work patterns were used to represent a variety of semantic relations , including hyponymy ( Hearst , 1992 ) , meronymy ( Berland and Charniak , 1999 ) and antonymy ( Lin et al. , 2003 ) .", "metadata": {"citing_string": "Hearst , 1992", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2121", "citing_paper": "W04-0209"}}
{"text": "Patterns ( symmetric or not ) were found useful in a variety of NLP tasks , including identification of word relations such as hyponymy ( Hearst , 1992 ) , meronymy ( Berland and Charniak , 1999 ) and antonymy ( Lin et al. , 2003 ) .", "metadata": {"citing_string": "Hearst , 1992", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3130", "citing_paper": "W06-1420"}}
{"text": "Lexico-syntactic patterns are sequences of words and wildcards ( Hearst , 1992 ) .", "metadata": {"citing_string": "Hearst , 1992", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1403", "citing_paper": "W03-0409"}}
{"text": "This relation can be accurately identified using patterns such as `` X such as Y '' and `` X like Y '' ( Hearst , 1992 ) .", "metadata": {"citing_string": "Hearst , 1992", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0409", "citing_paper": "W01-1615"}}
{"text": "As recently shown by Hill et al. ( 2014 ) , despite the impressive results VSMs that take this approach obtain on modeling word association , they are much less successful in modeling word similarity .", "metadata": {"citing_string": "Hill et al. ( 2014 )", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W05-1208", "citing_paper": "W08-0308"}}
{"text": "We experiment ( Section 6 ) with the SimLex999 dataset ( Hill et al. , 2014 ) , consisting of 999 pairs of words annotated by human subjects for similarity .", "metadata": {"citing_string": "Hill et al. , 2014", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W06-1420", "citing_paper": "W08-0907"}}
{"text": "Recently , Hill et al. ( 2014 ) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only .", "metadata": {"citing_string": "Hill et al. ( 2014 )", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W14-3411", "citing_paper": "W12-1511"}}
{"text": "We experiment with the SimLex999 dataset ( Hill et al. , 2014 ) ,6 consisting of 999 pairs of words .", "metadata": {"citing_string": "Hill et al. , 2014", "section_number": 6, "is_self_cite": true, "intent_label": "method", "cited_paper": "W04-0405", "citing_paper": "W04-2415"}}
{"text": "We experiment ( Section 6 ) with the SimLex999 dataset ( Hill et al. , 2014 ) , consisting of 999 pairs of words annotated by human subjects for similarity .", "metadata": {"citing_string": "Hill et al. , 2014", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W10-4148", "citing_paper": "W90-0112"}}
{"text": "Kozareva et al. ( 2008 ) used SPs to classify proper names ( e.g. , fish names , singer names ) .", "metadata": {"citing_string": "Kozareva et al. ( 2008 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2121", "citing_paper": "W11-2032"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Lebret and Collobert , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-2127", "citing_paper": "W13-2806"}}
{"text": "Levy and Goldberg ( 2014 ) extended the skip-gram word2vec model with negative sampling ( Mikolov et al. , 2013b ) by basing the word co-occurrence window on the dependency parse tree of the sentence .", "metadata": {"citing_string": "Levy and Goldberg ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2044", "citing_paper": "W05-1208"}}
{"text": "The modified , dependency-based , skipgram model ( Levy and Goldberg , 2014 ) .", "metadata": {"citing_string": "Levy and Goldberg , 2014", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2138", "citing_paper": "W11-0402"}}
{"text": "In previous work patterns were used to represent a variety of semantic relations , including hyponymy ( Hearst , 1992 ) , meronymy ( Berland and Charniak , 1999 ) and antonymy ( Lin et al. , 2003 ) .", "metadata": {"citing_string": "Lin et al. , 2003", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0737", "citing_paper": "W05-1302"}}
{"text": "Patterns ( symmetric or not ) were found useful in a variety of NLP tasks , including identification of word relations such as hyponymy ( Hearst , 1992 ) , meronymy ( Berland and Charniak , 1999 ) and antonymy ( Lin et al. , 2003 ) .", "metadata": {"citing_string": "Lin et al. , 2003", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-2410", "citing_paper": "W12-3149"}}
{"text": "Outside the VSM literature several works identified antonyms using word co-occurrence statistics , manually and automatically induced patterns , the WordNet lexicon and thesauri ( Lin et al. , 2003 ; Turney , 2008 ; Wang et al. , 2010 ; Mohammad et al. , 2013 ; Schulte im Walde and Koper , 2013 ; Roth and Schulte im Walde , 2014 ) .", "metadata": {"citing_string": "Lin et al. , 2003", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2415", "citing_paper": "W10-4148"}}
{"text": "To implement this mechanism , we follow ( Lin et al. , 2003 ) , who showed that two patterns are particularly indicative of antonymy -- `` from X to Y '' and `` either X or Y '' ( e.g. , `` from bottom to top '' , `` either high or low '' ) .", "metadata": {"citing_string": "Lin et al. , 2003", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3938", "citing_paper": "W13-2203"}}
{"text": "More Recently , Neural Networks have become prominent in word representation learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert et al. , 2011 ; Mikolov et al. , 2013a ; Pennington et al. , 2014 , inter alia ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2203", "citing_paper": "W14-3329"}}
{"text": "When comparing the correlation between the similarity scores derived from our learned representation and the human scores , our representation receives a Spearman correlation coefficient score ( p ) of 0.517 , outperforming six strong baselines , including the state-of-the-art word2vec ( Mikolov et al. , 2013a ) embeddings , by 5.5 -- 16.7 % .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2304", "citing_paper": "W03-0312"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0907", "citing_paper": "W06-1403"}}
{"text": "Levy and Goldberg ( 2014 ) extended the skip-gram word2vec model with negative sampling ( Mikolov et al. , 2013b ) by basing the word co-occurrence window on the dependency parse tree of the sentence .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1715", "citing_paper": "W11-2139"}}
{"text": "For example , in the skip-gram model ( Mikolov et al. , 2013a ) , the score of the ( accept , reject ) pair is 0.73 , and the score of ( long , short ) is 0.71 .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0308", "citing_paper": "W14-3304"}}
{"text": "which employs the word2phrase tool ( Mikolov et al. , 2013c ) to merge common word pairs and triples to expression tokens .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-0111", "citing_paper": "W13-2253"}}
{"text": "One of the most notable features of the skip-gram model is that some geometric relations between its vectors translate to semantic relations between the represented words ( Mikolov et al. , 2013c ) , e.g. : vwoman â vman + vking â vqueen It is therefore possible that a similar method can be applied to capture antonymy -- a useful property that our model was demonstrated to have .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 7, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0843", "citing_paper": "W11-2134"}}
{"text": "More Recently , Neural Networks have become prominent in word representation learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert et al. , 2011 ; Mikolov et al. , 2013a ; Pennington et al. , 2014 , inter alia ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-1403", "citing_paper": "W13-1915"}}
{"text": "When comparing the correlation between the similarity scores derived from our learned representation and the human scores , our representation receives a Spearman correlation coefficient score ( p ) of 0.517 , outperforming six strong baselines , including the state-of-the-art word2vec ( Mikolov et al. , 2013a ) embeddings , by 5.5 -- 16.7 % .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0843", "citing_paper": "W08-2117"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0843", "citing_paper": "W04-1218"}}
{"text": "Levy and Goldberg ( 2014 ) extended the skip-gram word2vec model with negative sampling ( Mikolov et al. , 2013b ) by basing the word co-occurrence window on the dependency parse tree of the sentence .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0604", "citing_paper": "W11-2121"}}
{"text": "For example , in the skip-gram model ( Mikolov et al. , 2013a ) , the score of the ( accept , reject ) pair is 0.73 , and the score of ( long , short ) is 0.71 .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W01-1615", "citing_paper": "W11-0402"}}
{"text": "which employs the word2phrase tool ( Mikolov et al. , 2013c ) to merge common word pairs and triples to expression tokens .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1412", "citing_paper": "W03-0409"}}
{"text": "One of the most notable features of the skip-gram model is that some geometric relations between its vectors translate to semantic relations between the represented words ( Mikolov et al. , 2013c ) , e.g. : vwoman â vman + vking â vqueen It is therefore possible that a similar method can be applied to capture antonymy -- a useful property that our model was demonstrated to have .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 7, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-1406", "citing_paper": "W09-0419"}}
{"text": "More Recently , Neural Networks have become prominent in word representation learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert et al. , 2011 ; Mikolov et al. , 2013a ; Pennington et al. , 2014 , inter alia ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-1111", "citing_paper": "W08-0308"}}
{"text": "When comparing the correlation between the similarity scores derived from our learned representation and the human scores , our representation receives a Spearman correlation coefficient score ( p ) of 0.517 , outperforming six strong baselines , including the state-of-the-art word2vec ( Mikolov et al. , 2013a ) embeddings , by 5.5 -- 16.7 % .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-1511", "citing_paper": "W10-2704"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W02-1610", "citing_paper": "W13-2815"}}
{"text": "Levy and Goldberg ( 2014 ) extended the skip-gram word2vec model with negative sampling ( Mikolov et al. , 2013b ) by basing the word co-occurrence window on the dependency parse tree of the sentence .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-1406", "citing_paper": "W10-1007"}}
{"text": "For example , in the skip-gram model ( Mikolov et al. , 2013a ) , the score of the ( accept , reject ) pair is 0.73 , and the score of ( long , short ) is 0.71 .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0120", "citing_paper": "W00-1412"}}
{"text": "which employs the word2phrase tool ( Mikolov et al. , 2013c ) to merge common word pairs and triples to expression tokens .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-0102", "citing_paper": "W03-1710"}}
{"text": "One of the most notable features of the skip-gram model is that some geometric relations between its vectors translate to semantic relations between the represented words ( Mikolov et al. , 2013c ) , e.g. : vwoman â vman + vking â vqueen It is therefore possible that a similar method can be applied to capture antonymy -- a useful property that our model was demonstrated to have .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 7, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3938", "citing_paper": "W04-1106"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Mnih and Hinton , 2009", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1104", "citing_paper": "W13-1733"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Mnih and Kavukcuoglu , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4148", "citing_paper": "W13-3605"}}
{"text": "Outside the VSM literature several works identified antonyms using word co-occurrence statistics , manually and automatically induced patterns , the WordNet lexicon and thesauri ( Lin et al. , 2003 ; Turney , 2008 ; Wang et al. , 2010 ; Mohammad et al. , 2013 ; Schulte im Walde and Koper , 2013 ; Roth and Schulte im Walde , 2014 ) .", "metadata": {"citing_string": "Mohammad et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W90-0112", "citing_paper": "W03-1710"}}
{"text": "Murphy et al. ( 2012 ) represented words through their co-occurrence with other words in syntactic dependency relations , and then used the Non-Negative Sparse Embedding ( NNSE ) method to reduce the dimension of the resulted representation .", "metadata": {"citing_string": "Murphy et al. ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3346", "citing_paper": "W00-1403"}}
{"text": "The NNSE model ( Murphy et al. , 2012 ) .", "metadata": {"citing_string": "Murphy et al. , 2012", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W02-1104", "citing_paper": "W07-0720"}}
{"text": "To generate dependency links , we use the Stanford POS Tagger ( Toutanova et al. , 2003 ) 18 and the MALT parser ( Nivre et al. , 2006 ) .19 We follow the parameters suggested by the authors .", "metadata": {"citing_string": "Nivre et al. , 2006", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2127", "citing_paper": "W11-2134"}}
{"text": "Recently , Yih et al. ( 2012 ) , Chang et al. ( 2013 ) and Ono et al. ( 2015 ) proposed word representation methods that assign dissimilar vectors to antonyms .", "metadata": {"citing_string": "Ono et al. ( 2015 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0317", "citing_paper": "W14-3411"}}
{"text": "More Recently , Neural Networks have become prominent in word representation learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert et al. , 2011 ; Mikolov et al. , 2013a ; Pennington et al. , 2014 , inter alia ) .", "metadata": {"citing_string": "Pennington et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-3815", "citing_paper": "W08-2231"}}
{"text": "Recently , there is a surge of work focusing on Neural Network ( NN ) algorithms for word representations learning ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Mnih and Hinton , 2009 ; Collobert et al. , 2011 ; Dhillon et al. , 2011 ; Mikolov et al. , 2013a ; Mnih and Kavukcuoglu , 2013 ; Lebret and Collobert , 2014 ; Pennington et al. , 2014 ) .", "metadata": {"citing_string": "Pennington et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-3815", "citing_paper": "W06-0102"}}
{"text": "GloVe ( Pennington et al. , 2014 ) 15 is a global log-bilinear regression model for word embedding generation , which trains only on the nonzero elements in a co-occurrence matrix .", "metadata": {"citing_string": "Pennington et al. , 2014", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3329", "citing_paper": "W11-1902"}}
{"text": "Research on vector spaces for word representation dates back to the early 1970 's ( Salton , 1971 ) .", "metadata": {"citing_string": "Salton , 1971", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1902", "citing_paper": "W11-2040"}}
{"text": "Outside the VSM literature several works identified antonyms using word co-occurrence statistics , manually and automatically induced patterns , the WordNet lexicon and thesauri ( Lin et al. , 2003 ; Turney , 2008 ; Wang et al. , 2010 ; Mohammad et al. , 2013 ; Schulte im Walde and Koper , 2013 ; Roth and Schulte im Walde , 2014 ) .", "metadata": {"citing_string": "Walde and Koper , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0405", "citing_paper": "W04-1106"}}
{"text": "tackle sentence level tasks such as identification of sarcasm ( Tsur et al. , 2010 ) , sentiment analysis ( Davidov et al. , 2010 ) and authorship attribution ( Schwartz et al. , 2013 ) .", "metadata": {"citing_string": "Schwartz et al. , 2013", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-2159", "citing_paper": "W12-0111"}}
{"text": "Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition ( Widdows and Dorow , 2002 ) , word clustering ( Davidov and Rappoport , 2006 ) and classification of words to semantic categories ( Schwartz et al. , 2014 ) .", "metadata": {"citing_string": "Schwartz et al. , 2014", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-1715", "citing_paper": "W09-0419"}}
{"text": "Feng et al. ( 2013 ) used SPs to build a connotation lexicon , and Schwartz et al. ( 2014 ) used SPs to perform minimally supervised classification of words into semantic categories .", "metadata": {"citing_string": "Schwartz et al. ( 2014 )", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W10-1916", "citing_paper": "W05-1302"}}
{"text": "To generate dependency links , we use the Stanford POS Tagger ( Toutanova et al. , 2003 ) 18 and the MALT parser ( Nivre et al. , 2006 ) .19 We follow the parameters suggested by the authors .", "metadata": {"citing_string": "Toutanova et al. , 2003", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0120", "citing_paper": "W12-3149"}}
{"text": "tackle sentence level tasks such as identification of sarcasm ( Tsur et al. , 2010 ) , sentiment analysis ( Davidov et al. , 2010 ) and authorship attribution ( Schwartz et al. , 2013 ) .", "metadata": {"citing_string": "Tsur et al. , 2010", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W14-2410", "citing_paper": "W04-1106"}}
{"text": "Outside the VSM literature several works identified antonyms using word co-occurrence statistics , manually and automatically induced patterns , the WordNet lexicon and thesauri ( Lin et al. , 2003 ; Turney , 2008 ; Wang et al. , 2010 ; Mohammad et al. , 2013 ; Schulte im Walde and Koper , 2013 ; Roth and Schulte im Walde , 2014 ) .", "metadata": {"citing_string": "Turney , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1916", "citing_paper": "W14-3329"}}
{"text": "Turney ( 2012 ) constructed two VSMs with the explicit goal of capturing either similarity or association .", "metadata": {"citing_string": "Turney ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0209", "citing_paper": "W05-1208"}}
{"text": "Outside the VSM literature several works identified antonyms using word co-occurrence statistics , manually and automatically induced patterns , the WordNet lexicon and thesauri ( Lin et al. , 2003 ; Turney , 2008 ; Wang et al. , 2010 ; Mohammad et al. , 2013 ; Schulte im Walde and Koper , 2013 ; Roth and Schulte im Walde , 2014 ) .", "metadata": {"citing_string": "Wang et al. , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-0135", "citing_paper": "W14-2410"}}
{"text": "Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition ( Widdows and Dorow , 2002 ) , word clustering ( Davidov and Rappoport , 2006 ) and classification of words to semantic categories ( Schwartz et al. , 2014 ) .", "metadata": {"citing_string": "Widdows and Dorow , 2002", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1208", "citing_paper": "W09-0419"}}
{"text": "Widdows and Dorow ( 2002 ) used SPs for the task of lexical acquisition .", "metadata": {"citing_string": "Widdows and Dorow ( 2002 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W05-1617"}}
{"text": "The most prominent patterns in these works are `` X and Y '' and `` X or Y '' ( Widdows and Dorow , 2002 ; Feng et al. , 2013 ) .", "metadata": {"citing_string": "Widdows and Dorow , 2002", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2304", "citing_paper": "W12-2021"}}
{"text": "Recently , Yih et al. ( 2012 ) , Chang et al. ( 2013 ) and Ono et al. ( 2015 ) proposed word representation methods that assign dissimilar vectors to antonyms .", "metadata": {"citing_string": "Yih et al. ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2032", "citing_paper": "W11-1304"}}
{"text": "â¢ Modality List : ModLex ( Al-Sabbagh et al. , 2013 ) is a manually compiled lexicon of Arabic modality triggers ( i.e. words and phrases that convey modality ) .", "metadata": {"citing_string": "Al-Sabbagh et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3130", "citing_paper": "W07-0720"}}
{"text": "Lin et al. ( 2014 ) use a CRF model that relies on character n-grams probabilities ( tri and quad grams ) , prefixes , suffixes , unicode page of the first character , capitalization case , alphanumeric case , and tweet-level language ID predictions from two off-the-shelf language identifiers : cld22 and ldig .3 They increase the size of the training data using a semi supervised CRF autoencoder approach ( Ammar et al. , 2014 ) coupled with unsupervised word embeddings .", "metadata": {"citing_string": "Ammar et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2159", "citing_paper": "W11-2040"}}
{"text": "It uses a list of all sequences of words that are tagged as ne in the training data of ShTk in addition to the named-entities from ANERGazet ( Benajiba et al. , 2007 ) to identify the namedentities in the input sentence .", "metadata": {"citing_string": "Benajiba et al. , 2007", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2806", "citing_paper": "W11-1304"}}
{"text": "Biadsy et al. ( 2009 ) presented a system that identifies dialectal words in speech through acoustic signals .", "metadata": {"citing_string": "Biadsy et al. ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0120", "citing_paper": "W07-0737"}}
{"text": "For sentence level dialect identification in Arabic , the most recent works are ( Zaidan and Callison-Burch , 2011 ) , ( Elfardy and Diab , 2013 ) , and ( Cotterell and Callison-Burch , 2014a ) .", "metadata": {"citing_string": "Cotterell and Callison-Burch , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0419", "citing_paper": "W04-2309"}}
{"text": "â¢ Zidan et al : The best results obtained by Zaidan and Callison-Burch ( 2011 ) ; â¢ Elfardy et al : The best results obtained by Elfardy and Diab ( 2013 ) ; â¢ Cotterell et al : The best result obtained by Cotterell and Callison-Burch ( 2014a ) ; â¢ All Features : This baseline combines all features from Comp-Cl and Abs-Cl to train a single decision tree classifier .", "metadata": {"citing_string": "Cotterell and Callison-Burch ( 2014", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0843", "citing_paper": "W04-0405"}}
{"text": "For sentence level dialect identification in Arabic , the most recent works are ( Zaidan and Callison-Burch , 2011 ) , ( Elfardy and Diab , 2013 ) , and ( Cotterell and Callison-Burch , 2014a ) .", "metadata": {"citing_string": "Cotterell and Callison-Burch , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W03-0312"}}
{"text": "â¢ Zidan et al : The best results obtained by Zaidan and Callison-Burch ( 2011 ) ; â¢ Elfardy et al : The best results obtained by Elfardy and Diab ( 2013 ) ; â¢ Cotterell et al : The best result obtained by Cotterell and Callison-Burch ( 2014a ) ; â¢ All Features : This baseline combines all features from Comp-Cl and Abs-Cl to train a single decision tree classifier .", "metadata": {"citing_string": "Cotterell and Callison-Burch ( 2014", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W09-0604"}}
{"text": "For sentence level dialect identification in Arabic , the most recent works are ( Zaidan and Callison-Burch , 2011 ) , ( Elfardy and Diab , 2013 ) , and ( Cotterell and Callison-Burch , 2014a ) .", "metadata": {"citing_string": "Elfardy and Diab , 2013", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W00-1220", "citing_paper": "W11-2139"}}
{"text": "We explore the same features used by Elfardy and Diab ( 2013 ) in addition to three other features that we refer to as `` Modality Features '' .", "metadata": {"citing_string": "Elfardy and Diab ( 2013 )", "section_number": 4, "is_self_cite": true, "intent_label": "background", "cited_paper": "W04-0825", "citing_paper": "W05-1208"}}
{"text": "The data is split into training ( sentTrnDB ) and test ( sentTstDB ) using the same split reported by Elfardy and Diab ( 2013 ) .", "metadata": {"citing_string": "Elfardy and Diab ( 2013 )", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-0317", "citing_paper": "W02-1610"}}
{"text": "â¢ Zidan et al : The best results obtained by Zaidan and Callison-Burch ( 2011 ) ; â¢ Elfardy et al : The best results obtained by Elfardy and Diab ( 2013 ) ; â¢ Cotterell et al : The best result obtained by Cotterell and Callison-Burch ( 2014a ) ; â¢ All Features : This baseline combines all features from Comp-Cl and Abs-Cl to train a single decision tree classifier .", "metadata": {"citing_string": "Elfardy and Diab ( 2013 )", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-2134", "citing_paper": "W04-0405"}}
{"text": "Viterbi search algorithm ( Forney , 1973 ) is then used to find the best sequence of tags for the given sentence .", "metadata": {"citing_string": "Forney , 1973", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2044", "citing_paper": "W10-1916"}}
{"text": "bAlfryq , `` By the team '' becomes `` b + Al + fryq '' ) ( Habash and Sadat , 2006 ) .", "metadata": {"citing_string": "Habash and Sadat , 2006", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0209", "citing_paper": "W14-3329"}}
{"text": "â¢ MADAMIRA : is a publicly available tool for morphological analysis and disambiguation of EDA and MSA text ( Pasha et al. , 2014 ) .4 MADAMIRA uses SAMA ( Maamouri et al. , 2010 ) to analyze the MSA words and CALIMA ( Habash et al. , 2012 ) for the EDA words .", "metadata": {"citing_string": "Habash et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W02-1104", "citing_paper": "W11-2134"}}
{"text": "Using the D3 tokenized version of the input data in addition to the classes provided by the `` Token Level Identification '' module for each word in the given sentence , we conduct a suite of experiments using the decision tree implementation by WEKA toolkit ( Hall et al. , 2009 ) to exhaustively search over all features in each group in the first phase , and then exhaustively search over all of the remaining features", "metadata": {"citing_string": "Hall et al. , 2009", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-1807", "citing_paper": "W01-1615"}}
{"text": "For all experiments , we use a decision-tree classifier as implemented in WEKA ( Hall et al. , 2009 ) toolkit .", "metadata": {"citing_string": "Hall et al. , 2009", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0809", "citing_paper": "W04-0209"}}
{"text": "â¢ MADAMIRA : is a publicly available tool for morphological analysis and disambiguation of EDA and MSA text ( Pasha et al. , 2014 ) .4 MADAMIRA uses SAMA ( Maamouri et al. , 2010 ) to analyze the MSA words and CALIMA ( Habash et al. , 2012 ) for the EDA words .", "metadata": {"citing_string": "Maamouri et al. , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0209", "citing_paper": "W10-4229"}}
{"text": "While Comp-Cl yields detailed and specific information about the tokens as it uses tokenized-level LMs , Abs-Cl is able to capture better semantic and syntactic relations between words since it can see longer context in terms of the number of words compared to that seen by Comp-Cl ( on average a span of two words in the surface-level LM corresponds to almost five words in the tokenized-level LM ) ( Rashwan et al. , 2011 ) .", "metadata": {"citing_string": "Rashwan et al. , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1617", "citing_paper": "W10-1010"}}
{"text": "( Salloum et al. , 2014 ) explored the impact of using Dialect Identification on the performance of MT and found that it improves the results .", "metadata": {"citing_string": "Salloum et al. , 2014", "section_number": 2, "is_self_cite": true, "intent_label": "background", "cited_paper": "W06-1403", "citing_paper": "W10-4229"}}
{"text": "We then use these features to train a CRF classifier using CRF + + toolkit ( Sha and Pereira , 2003 ) and we set the window size to 16.5 Figure 1 illustrates the different components of the token-level system .", "metadata": {"citing_string": "Sha and Pereira , 2003", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-1304", "citing_paper": "W14-3346"}}
{"text": "We first classify each word in the input sentence to be one of the following six tags as defined in the shared task for `` Language Identification in Code-Switched Data '' in the first workshop on computational approaches to code-switching [ ShTk ] ( Solorio et al. , 2014 ) :", "metadata": {"citing_string": "Solorio et al. , 2014", "section_number": 4, "is_self_cite": true, "intent_label": "method", "cited_paper": "W08-1137", "citing_paper": "W12-2021"}}
{"text": "For sentence level dialect identification in Arabic , the most recent works are ( Zaidan and Callison-Burch , 2011 ) , ( Elfardy and Diab , 2013 ) , and ( Cotterell and Callison-Burch , 2014a ) .", "metadata": {"citing_string": "Zaidan and Callison-Burch , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W02-1610", "citing_paper": "W14-1807"}}
{"text": "For the sentence level component , we evaluate our approach against all published results on the Arabic `` Online Commentaries ( AOC ) '' publicly available dataset ( Zaidan and Callison-Burch , 2011 ) .", "metadata": {"citing_string": "Zaidan and Callison-Burch , 2011", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4304", "citing_paper": "W04-2415"}}
{"text": "A number of pioneers take the discourse relations identification as a classification task ( Marcu and Echihabi , 2002 ; Pitler et al. , 2009 ; Duverle and Prendinger , 2009 ) by the construction of features like lexical , syntactic and constituent features .", "metadata": {"citing_string": "Duverle and Prendinger , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2415", "citing_paper": "W13-1902"}}
{"text": "Some take the argument segmentation task as a semantic role detection task ( Wellner and Pustejovsky , 2007 ) and a sequence labeling task ( Ghosh et al. , 2011 ) .", "metadata": {"citing_string": "Ghosh et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2138", "citing_paper": "W00-1220"}}
{"text": "The connectives list : A list contains 100 discourse connectives in the PDTB and three syntactic categories form ( Knott , 1996 ) .", "metadata": {"citing_string": "Knott , 1996", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0853", "citing_paper": "W10-1010"}}
{"text": "Besides , much research about discourse parsing working on the PDTB appears ( Prasad et al. , 2010 ; Lin et al. , 2009 ) and they put more attention on the `` harder '' part labelling the arguments .", "metadata": {"citing_string": "Lin et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4229", "citing_paper": "W03-0409"}}
{"text": "After the deadline of evaluation , we made some improvements in the module of implicit relation classifier inspired by ( Lin et al. , 2009 ) .", "metadata": {"citing_string": "Lin et al. , 2009", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2121", "citing_paper": "W09-0419"}}
{"text": "Lin ( Lin et al. , 2014 ) designed an end-to-end discourse parser with the PDTB including the explicit , implicit sense and the argument spans identification .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-2021", "citing_paper": "W09-0419"}}
{"text": "Our system mainly follows the work of ( Lin et al. , 2014 ) , which consists of two parts : the explicit relation parser and the non-explicit relation parser .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W02-1610", "citing_paper": "W11-2159"}}
{"text": "There is considerable interest in discourse parsing , both as an end in itself and as an intermediate step in a variety of NLP applications like question answering ( Verberne et al. , 2007 ) , text summarization ( Louis et al. , 2010 ) , sentiment analysis and opinion mining ( Somasundaran , 2010 ) .", "metadata": {"citing_string": "Louis et al. , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-0201", "citing_paper": "W02-1610"}}
{"text": "A number of pioneers take the discourse relations identification as a classification task ( Marcu and Echihabi , 2002 ; Pitler et al. , 2009 ; Duverle and Prendinger , 2009 ) by the construction of features like lexical , syntactic and constituent features .", "metadata": {"citing_string": "Marcu and Echihabi , 2002", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3411", "citing_paper": "W13-2209"}}
{"text": "A number of pioneers take the discourse relations identification as a classification task ( Marcu and Echihabi , 2002 ; Pitler et al. , 2009 ; Duverle and Prendinger , 2009 ) by the construction of features like lexical , syntactic and constituent features .", "metadata": {"citing_string": "Pitler et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2304", "citing_paper": "W11-2139"}}
{"text": "This has been addressed with the release of Penn Discourse Treebank ( PDTB ) 2.0 corpus ( Prasad et al. , 2008 ) which provides detailed annotations about the discourse relations and argument spans addresses this problem .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3304", "citing_paper": "W11-2121"}}
{"text": "In this model , we classified the Arg1 's locations into two classes : Arg1 was located within the same sentence of the connective ( SS ) or in the previous sentence of connective ( PS ) ( Prasad et al. , 2008 ) .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W14-3346"}}
{"text": "Besides , much research about discourse parsing working on the PDTB appears ( Prasad et al. , 2010 ; Lin et al. , 2009 ) and they put more attention on the `` harder '' part labelling the arguments .", "metadata": {"citing_string": "Prasad et al. , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-2127", "citing_paper": "W11-1902"}}
{"text": "There is considerable interest in discourse parsing , both as an end in itself and as an intermediate step in a variety of NLP applications like question answering ( Verberne et al. , 2007 ) , text summarization ( Louis et al. , 2010 ) , sentiment analysis and opinion mining ( Somasundaran , 2010 ) .", "metadata": {"citing_string": "Somasundaran , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1218", "citing_paper": "W04-0825"}}
{"text": "There is considerable interest in discourse parsing , both as an end in itself and as an intermediate step in a variety of NLP applications like question answering ( Verberne et al. , 2007 ) , text summarization ( Louis et al. , 2010 ) , sentiment analysis and opinion mining ( Somasundaran , 2010 ) .", "metadata": {"citing_string": "Verberne et al. , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W13-2219"}}
{"text": "Some take the argument segmentation task as a semantic role detection task ( Wellner and Pustejovsky , 2007 ) and a sequence labeling task ( Ghosh et al. , 2011 ) .", "metadata": {"citing_string": "Wellner and Pustejovsky , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W13-2253"}}
{"text": "Shallow Discourse Parsing ( Xue et al. , 2015 ) is the CoNLL shared task this year1 which takes a piece of newswire text as input and returns all the discourse relations in the form of a discourse connective ( explicit or implicit ) taking two arguments ( which can be clauses , sentences , or multisentence segments ) in JSON format .", "metadata": {"citing_string": "Xue et al. , 2015", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-2410", "citing_paper": "W11-0402"}}
{"text": "Although that is not feasible , Asuncion et al. ( 2009 ) demonstrate that hyperaparameter optimization in LDA topic models helps to bring the performance of alternative inference algorithms into approximate agreement .", "metadata": {"citing_string": "Asuncion et al. ( 2009 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W11-2121"}}
{"text": "As in the work of Asuncion et al. ( 2009 ) , we add an algorithmic gamma prior ( b ( Â· ) -- G ( Î± , Q ) ) for smoothing by adding Î± â 1 b ( Â· ) to the numerator and Q to the denominator of Equations 5-8 .", "metadata": {"citing_string": "Asuncion et al. ( 2009 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2231", "citing_paper": "W13-2209"}}
{"text": "The LDC-labeled Enron emails dataset is described by Berry et al. ( 2001 ) .", "metadata": {"citing_string": "Berry et al. ( 2001 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0720", "citing_paper": "W13-2806"}}
{"text": "Figure 4 demonstrates that three ITEMRESP inference algorithms , Gibbs sampling ( Gibbs ) , meanfield variational inference ( Var ) , and the iterated conditional modes algorithm ( ICM ) ( Besag , 1986 ) , are brought into better agreement after optimizing their hyperparameters via grid search .", "metadata": {"citing_string": "Besag , 1986", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3346", "citing_paper": "W13-2203"}}
{"text": "Latent Dirichlet Allocation ( Blei et al. , 2003 , LDA ) models text documents as admixtures of word distributions , or topics .", "metadata": {"citing_string": "Blei et al. , 2003", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0402", "citing_paper": "W09-1111"}}
{"text": "Others model the data generatively ( Bragg et al. , 2013 ; Lam and Stork , 2005 ; Felt et al. , 2014 ; Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Bragg et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-0203", "citing_paper": "W10-4148"}}
{"text": "MOMRESP represents a typical data-generative model ( Bragg et al. , 2013 ; Felt et al. , 2014 ; Lam and Stork , 2005 ; Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Bragg et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1715", "citing_paper": "W06-1403"}}
{"text": "The 20 Newsgroups , WebKB , Cade12 , Reuters8 , and Reuters52 datasets are described in more detail by Cardoso-Cachopo ( 2007 ) .", "metadata": {"citing_string": "Cardoso-Cachopo ( 2007 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-1916", "citing_paper": "W10-2704"}}
{"text": "Alternative inference algorithms have been proposed for crowdsourcing models ( Dalvi et al. , 2013 ; Ghosh et al. , 2011 ; Karger et al. , 2013 ; Zhang et al. , 2014 ) .", "metadata": {"citing_string": "Dalvi et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1218", "citing_paper": "W00-1412"}}
{"text": "Most crowdsourcing models extend the itemresponse model of Dawid and Skene ( 1979 ) .", "metadata": {"citing_string": "Dawid and Skene ( 1979 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-1111", "citing_paper": "W08-1137"}}
{"text": "Others model the data generatively ( Bragg et al. , 2013 ; Lam and Stork , 2005 ; Felt et al. , 2014 ; Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Felt et al. , 2014", "section_number": 2, "is_self_cite": true, "intent_label": "background", "cited_paper": "W08-2127", "citing_paper": "W10-1007"}}
{"text": "MOMRESP represents a typical data-generative model ( Bragg et al. , 2013 ; Felt et al. , 2014 ; Lam and Stork , 2005 ; Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Felt et al. , 2014", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W14-2410"}}
{"text": "Felt et al. ( 2015 ) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts ( Ng and Jordan , 2001 ) -- especially early in the learning curve .", "metadata": {"citing_string": "Felt et al. ( 2015 )", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W10-4148", "citing_paper": "W11-2044"}}
{"text": "For ITEMRESP , MOMRESP , and LOGRESP we use the variational inference methods presented by Felt et al. ( 2015 ) .", "metadata": {"citing_string": "Felt et al. ( 2015 )", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W08-1137", "citing_paper": "W06-1403"}}
{"text": "We base our experimental setup on the annotations gathered by Felt et al. ( 2015 ) ,3 who paid CrowdFlower annotators to relabel 1000 documents from the well-known 20 Newsgroups classification dataset .", "metadata": {"citing_string": "Felt et al. ( 2015 )", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-2219", "citing_paper": "W08-0317"}}
{"text": "Alternative inference algorithms have been proposed for crowdsourcing models ( Dalvi et al. , 2013 ; Ghosh et al. , 2011 ; Karger et al. , 2013 ; Zhang et al. , 2014 ) .", "metadata": {"citing_string": "Ghosh et al. , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1610", "citing_paper": "W06-3113"}}
{"text": "A growing body of work extends the itemresponse model to account for variables such as item difficulty ( Whitehill et al. , 2009 ; Passonneau and Carpenter , 2013 ; Zhou et al. , 2012 ) , annotator trustworthiness ( Hovy et al. , 2013 ) , correlation among various combinations of these variables ( Zhou et al. , 2014 ) , and change in annotator behavior over time ( Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Hovy et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-1511", "citing_paper": "W08-2127"}}
{"text": "Many data-aware crowdsourcing models condition the labels on the data ( Jin and Ghahramani , 2002 ; Raykar et al. , 2010 ; Liu et al. , 2012 ; Yan et al. , 2014 ) , possibly because discriminative classifiers dominate supervised machine learning .", "metadata": {"citing_string": "Jin and Ghahramani , 2002", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3346", "citing_paper": "W10-2704"}}
{"text": "Data-conditional approaches typically model data features conditionally using a log-linear model ( Jin and Ghahramani , 2002 ; Raykar et al. , 2010 ; Liu et al. , 2012 ; Yan et al. , 2014 ) .", "metadata": {"citing_string": "Jin and Ghahramani , 2002", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3346", "citing_paper": "W11-2107"}}
{"text": "A common solution to this problem is to obtain multiple redundant human judgments , or annotations ,1 relying on the observation that , in aggregate , non-experts often rival or exceed experts by averaging over individual error patterns ( Surowiecki , 2005 ; Snow et al. , 2008 ; Jurgens , 2013 ) .", "metadata": {"citing_string": "Jurgens , 2013", "section_number": 1, "is_self_cite": false, "intent_label": "background", "cited_paper": "W98-1419", "citing_paper": "W04-1218"}}
{"text": "Alternative inference algorithms have been proposed for crowdsourcing models ( Dalvi et al. , 2013 ; Ghosh et al. , 2011 ; Karger et al. , 2013 ; Zhang et al. , 2014 ) .", "metadata": {"citing_string": "Karger et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-1137", "citing_paper": "W13-1733"}}
{"text": "Others model the data generatively ( Bragg et al. , 2013 ; Lam and Stork , 2005 ; Felt et al. , 2014 ; Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Lam and Stork , 2005", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W11-2138"}}
{"text": "MOMRESP represents a typical data-generative model ( Bragg et al. , 2013 ; Felt et al. , 2014 ; Lam and Stork , 2005 ; Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Lam and Stork , 2005", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3130", "citing_paper": "W10-4148"}}
{"text": "Although pre-calculated LDA topics as features can inform a crowdsourcing model ( Levenberg et al. , 2014 ) , supervised LDA ( sLDA ) provides a principled way of incorporating document class labels and topics into a single model , allowing topic variables and response variables to co-inform one another in joint inference .", "metadata": {"citing_string": "Levenberg et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W01-0725", "citing_paper": "W03-1710"}}
{"text": "To isolate the effectiveness of joint inference in CSLDA , we compare against the pipeline alternative where topics are inferred first and then held constant during inference ( Levenberg et al. , 2014 ) .", "metadata": {"citing_string": "Levenberg et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-1218", "citing_paper": "W08-0621"}}
{"text": "Some crowdsourcing work regards labeled data not as an end in itself , but rather as a means to train classifiers ( Lin et al. , 2014 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2209", "citing_paper": "W11-0317"}}
{"text": "Many data-aware crowdsourcing models condition the labels on the data ( Jin and Ghahramani , 2002 ; Raykar et al. , 2010 ; Liu et al. , 2012 ; Yan et al. , 2014 ) , possibly because discriminative classifiers dominate supervised machine learning .", "metadata": {"citing_string": "Liu et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-2410", "citing_paper": "W13-2815"}}
{"text": "Data-conditional approaches typically model data features conditionally using a log-linear model ( Jin and Ghahramani , 2002 ; Raykar et al. , 2010 ; Liu et al. , 2012 ; Yan et al. , 2014 ) .", "metadata": {"citing_string": "Liu et al. , 2012", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-3130", "citing_paper": "W00-1403"}}
{"text": "In follow-up work , Liu et al. ( 2012 ) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM .", "metadata": {"citing_string": "Liu et al. ( 2012 )", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1220", "citing_paper": "W04-2415"}}
{"text": "For example , when sLDA is given movie reviews labeled with sentiment , inferred topics cluster around sentimentheavy words ( Mcauliffe and Blei , 2007 ) , which may be quite different from the topics inferred by unsupervised LDA .", "metadata": {"citing_string": "Mcauliffe and Blei , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1610", "citing_paper": "W05-1617"}}
{"text": "For efficiency , therefore , we use the fixed-point updates of Minka ( 2000 ) .", "metadata": {"citing_string": "Minka ( 2000 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-1710", "citing_paper": "W06-0102"}}
{"text": "Felt et al. ( 2015 ) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts ( Ng and Jordan , 2001 ) -- especially early in the learning curve .", "metadata": {"citing_string": "Ng and Jordan , 2001", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2806", "citing_paper": "W11-2107"}}
{"text": "For example , this mapping could depend not just on inferred topical content but also directly on data features ( c.f. Nguyen et al. ( 2013 ) ) or learned embedded feature representations .", "metadata": {"citing_string": "Nguyen et al. ( 2013 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3149", "citing_paper": "W11-0317"}}
{"text": "A growing body of work extends the itemresponse model to account for variables such as item difficulty ( Whitehill et al. , 2009 ; Passonneau and Carpenter , 2013 ; Zhou et al. , 2012 ) , annotator trustworthiness ( Hovy et al. , 2013 ) , correlation among various combinations of these variables ( Zhou et al. , 2014 ) , and change in annotator behavior over time ( Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Passonneau and Carpenter , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-2127", "citing_paper": "W14-0201"}}
{"text": "The fact-finding literature assigns trust scores to assertions made by untrusted sources ( Pasternack and Roth , 2010 ) .", "metadata": {"citing_string": "Pasternack and Roth , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0419", "citing_paper": "W14-3346"}}
{"text": "Many data-aware crowdsourcing models condition the labels on the data ( Jin and Ghahramani , 2002 ; Raykar et al. , 2010 ; Liu et al. , 2012 ; Yan et al. , 2014 ) , possibly because discriminative classifiers dominate supervised machine learning .", "metadata": {"citing_string": "Raykar et al. , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2127", "citing_paper": "W10-1007"}}
{"text": "Data-conditional approaches typically model data features conditionally using a log-linear model ( Jin and Ghahramani , 2002 ; Raykar et al. , 2010 ; Liu et al. , 2012 ; Yan et al. , 2014 ) .", "metadata": {"citing_string": "Raykar et al. , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W11-2032"}}
{"text": "Others model the data generatively ( Bragg et al. , 2013 ; Lam and Stork , 2005 ; Felt et al. , 2014 ; Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Simpson and Roberts , 2015", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4229", "citing_paper": "W03-2127"}}
{"text": "MOMRESP represents a typical data-generative model ( Bragg et al. , 2013 ; Felt et al. , 2014 ; Lam and Stork , 2005 ; Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Simpson and Roberts , 2015", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1104", "citing_paper": "W02-1610"}}
{"text": "A growing body of work extends the itemresponse model to account for variables such as item difficulty ( Whitehill et al. , 2009 ; Passonneau and Carpenter , 2013 ; Zhou et al. , 2012 ) , annotator trustworthiness ( Hovy et al. , 2013 ) , correlation among various combinations of these variables ( Zhou et al. , 2014 ) , and change in annotator behavior over time ( Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Simpson and Roberts , 2015", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1902", "citing_paper": "W11-2121"}}
{"text": "Simpson et al. ( 2013 ) identify annotator clusters using community detection algorithms but do not address annotator hierarchy or scalable confusion representations .", "metadata": {"citing_string": "Simpson et al. ( 2013 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2806", "citing_paper": "W01-0725"}}
{"text": "A common solution to this problem is to obtain multiple redundant human judgments , or annotations ,1 relying on the observation that , in aggregate , non-experts often rival or exceed experts by averaging over individual error patterns ( Surowiecki , 2005 ; Snow et al. , 2008 ; Jurgens , 2013 ) .", "metadata": {"citing_string": "Surowiecki , 2005", "section_number": 1, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0809", "citing_paper": "W13-2203"}}
{"text": "Welinder et al. ( 2010 ) carefully model the process of annotating objects in images , including variables for item difficulty , item class , and classconditional perception noise .", "metadata": {"citing_string": "Welinder et al. ( 2010 )", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-2127", "citing_paper": "W11-2032"}}
{"text": "A growing body of work extends the itemresponse model to account for variables such as item difficulty ( Whitehill et al. , 2009 ; Passonneau and Carpenter , 2013 ; Zhou et al. , 2012 ) , annotator trustworthiness ( Hovy et al. , 2013 ) , correlation among various combinations of these variables ( Zhou et al. , 2014 ) , and change in annotator behavior over time ( Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Whitehill et al. , 2009", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2415", "citing_paper": "W04-0853"}}
{"text": "Many data-aware crowdsourcing models condition the labels on the data ( Jin and Ghahramani , 2002 ; Raykar et al. , 2010 ; Liu et al. , 2012 ; Yan et al. , 2014 ) , possibly because discriminative classifiers dominate supervised machine learning .", "metadata": {"citing_string": "Yan et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-0135", "citing_paper": "W11-0317"}}
{"text": "Data-conditional approaches typically model data features conditionally using a log-linear model ( Jin and Ghahramani , 2002 ; Raykar et al. , 2010 ; Liu et al. , 2012 ; Yan et al. , 2014 ) .", "metadata": {"citing_string": "Yan et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2134", "citing_paper": "W03-2127"}}
{"text": "Alternative inference algorithms have been proposed for crowdsourcing models ( Dalvi et al. , 2013 ; Ghosh et al. , 2011 ; Karger et al. , 2013 ; Zhang et al. , 2014 ) .", "metadata": {"citing_string": "Zhang et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2253", "citing_paper": "W10-1902"}}
{"text": "A growing body of work extends the itemresponse model to account for variables such as item difficulty ( Whitehill et al. , 2009 ; Passonneau and Carpenter , 2013 ; Zhou et al. , 2012 ) , annotator trustworthiness ( Hovy et al. , 2013 ) , correlation among various combinations of these variables ( Zhou et al. , 2014 ) , and change in annotator behavior over time ( Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Zhou et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W90-0112", "citing_paper": "W13-2209"}}
{"text": "A growing body of work extends the itemresponse model to account for variables such as item difficulty ( Whitehill et al. , 2009 ; Passonneau and Carpenter , 2013 ; Zhou et al. , 2012 ) , annotator trustworthiness ( Hovy et al. , 2013 ) , correlation among various combinations of these variables ( Zhou et al. , 2014 ) , and change in annotator behavior over time ( Simpson and Roberts , 2015 ) .", "metadata": {"citing_string": "Zhou et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0209", "citing_paper": "W14-3329"}}
{"text": "The C&C tool Boxer ( Bos , 2014 ) , developed by Johan Bos , is a toolkit for creating the semantic representation of sentences , developed by Johan Bos .", "metadata": {"citing_string": "Bos , 2014", "section_number": 7, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-0312", "citing_paper": "W09-1111"}}
{"text": "( Ghosh et al. , 2014 ) used a linear tagging approach based on Conditional Random Fields .", "metadata": {"citing_string": "Ghosh et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W98-1419", "citing_paper": "W10-4148"}}
{"text": "A hybrid approach was explored by ( Kong et al. , 2014 ) taking advantages of both the linear tagging and sub-tree extraction by using a constituent based approach .", "metadata": {"citing_string": "Kong et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2231", "citing_paper": "W11-0120"}}
{"text": "For non-explicit relations , ( Lin et al. , 2009 ) have used word-pair features , which was the Cartesian product of all words from Arg1 and Arg2 .", "metadata": {"citing_string": "Lin et al. , 2009", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2107", "citing_paper": "W13-2209"}}
{"text": "Previously , an end-to-end model by Lin et al. ( 2014 ) was developed which used only syntactic features from parse trees and improved the discourse parser performance .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-2410", "citing_paper": "W06-0102"}}
{"text": "( Pitler and Nenkova , 2009 ) achieved an F-Score of 94.19 % which was extended by ( Lin et al. , 2014 ) to get an F-score of 95.36 % .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1412", "citing_paper": "W11-1406"}}
{"text": "( Lin et al. , 2014 ) used an extension of the syntactic features used by ( Pitler and Nenkova , 2009 ) , which resulted in a higher F-score of 95.36 % .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W13-2806"}}
{"text": "We have used a simple heuristic-based baseline parser as done by Lin et al. ( 2014 ) for implicit connectives .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 8, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1403", "citing_paper": "W08-1137"}}
{"text": "( Lin et al. , 2014 ) reported an F-Score of 86.77 % using connective-based features over the PDTB corpus .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 9, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1406", "citing_paper": "W14-2410"}}
{"text": "Interest in discourse parsing has increased after the release of the Penn Discourse TreeBank ( PDTB ) ( Miltsakaki et al. , 2004 ) .", "metadata": {"citing_string": "Miltsakaki et al. , 2004", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2415", "citing_paper": "W08-0621"}}
{"text": "There are distribution statistics from ( Miltsakaki et al. , 2004 ) which will prove beneficial in our algorithm .", "metadata": {"citing_string": "Miltsakaki et al. , 2004", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0825", "citing_paper": "W11-1304"}}
{"text": "( Pitler and Nenkova , 2009 ) achieved an F-Score of 94.19 % which was extended by ( Lin et al. , 2014 ) to get an F-score of 95.36 % .", "metadata": {"citing_string": "Pitler and Nenkova , 2009", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0209", "citing_paper": "W08-1137"}}
{"text": "( Lin et al. , 2014 ) used an extension of the syntactic features used by ( Pitler and Nenkova , 2009 ) , which resulted in a higher F-score of 95.36 % .", "metadata": {"citing_string": "Pitler and Nenkova , 2009", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-1010", "citing_paper": "W00-1412"}}
{"text": "Simple heuristic-based approaches have also shown reasonably high accuracy ( Xue et al. , 2015 ) .", "metadata": {"citing_string": "Xue et al. , 2015", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2309", "citing_paper": "W04-0209"}}
{"text": "For instance , the standard automatic metrics for certain tasks -- such as BLEU in machine translation , or ROUGE-N and NIST in summarization or natural language generation -- were evaluated , reaching correlation coefficients well above .80 ( Papineni et al. , 2002 ; Lin , 2004 ; Belz and Reiter , 2006 ; Callison-Burch et al. , 2007 ) .", "metadata": {"citing_string": "Belz and Reiter , 2006", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1403", "citing_paper": "W11-2138"}}
{"text": "In other NLP tasks , several studies have examined how metrics correlate with human judgments , including machine translation , summarization and natural language generation ( Papineni et al. , 2002 ; Lin , 2004 ; Belz and Reiter , 2006 ; Callison-Burch et al. , 2007 ) .", "metadata": {"citing_string": "Belz and Reiter , 2006", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1902", "citing_paper": "W13-2253"}}
{"text": "The three most commonly used metrics are those from the CoNLL 2006 -- 7 shared tasks ( Buchholz and Marsi , 2006 ) : unlabeled attachment score ( UAS ) , label accuracy ( LA ) , both introduced by Eisner ( 1996 ) , and labeled attachment score ( LAS ) , the pivotal dependency parsing metric introduced by Nivre et al. ( 2004 ) .", "metadata": {"citing_string": "Buchholz and Marsi , 2006", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0825", "citing_paper": "W10-3815"}}
{"text": "For instance , the standard automatic metrics for certain tasks -- such as BLEU in machine translation , or ROUGE-N and NIST in summarization or natural language generation -- were evaluated , reaching correlation coefficients well above .80 ( Papineni et al. , 2002 ; Lin , 2004 ; Belz and Reiter , 2006 ; Callison-Burch et al. , 2007 ) .", "metadata": {"citing_string": "Callison-Burch et al. , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2121", "citing_paper": "W04-1106"}}
{"text": "In other NLP tasks , several studies have examined how metrics correlate with human judgments , including machine translation , summarization and natural language generation ( Papineni et al. , 2002 ; Lin , 2004 ; Belz and Reiter , 2006 ; Callison-Burch et al. , 2007 ) .", "metadata": {"citing_string": "Callison-Burch et al. , 2007", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4163", "citing_paper": "W11-2159"}}
{"text": "The three most commonly used metrics are those from the CoNLL 2006 -- 7 shared tasks ( Buchholz and Marsi , 2006 ) : unlabeled attachment score ( UAS ) , label accuracy ( LA ) , both introduced by Eisner ( 1996 ) , and labeled attachment score ( LAS ) , the pivotal dependency parsing metric introduced by Nivre et al. ( 2004 ) .", "metadata": {"citing_string": "Eisner ( 1996 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3734", "citing_paper": "W08-0308"}}
{"text": "Even though downstream evaluation is critical in assessing the usefulness of parses , it also presents non-trivial challenges in choosing the appropriate downstream tasks ( Elming et al. , 2013 ) , we see human judgments as an important supplement to extrinsic evaluation .", "metadata": {"citing_string": "Elming et al. , 2013", "section_number": 2, "is_self_cite": true, "intent_label": "background", "cited_paper": "W05-0203", "citing_paper": "W04-0825"}}
{"text": "In order to aggregate over the annotations , we use an item-response model ( Hovy et al. , 2013 ) .", "metadata": {"citing_string": "Hovy et al. , 2013", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-2410", "citing_paper": "W14-1807"}}
{"text": "For instance , the standard automatic metrics for certain tasks -- such as BLEU in machine translation , or ROUGE-N and NIST in summarization or natural language generation -- were evaluated , reaching correlation coefficients well above .80 ( Papineni et al. , 2002 ; Lin , 2004 ; Belz and Reiter , 2006 ; Callison-Burch et al. , 2007 ) .", "metadata": {"citing_string": "Lin , 2004", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-3815", "citing_paper": "W13-2806"}}
{"text": "For instance , ROUGE-1 ( Lin , 2004 ) correlates strongly with perceived summary quality , with a coefficient of 0.99 .", "metadata": {"citing_string": "Lin , 2004", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2127", "citing_paper": "W01-1615"}}
{"text": "In other NLP tasks , several studies have examined how metrics correlate with human judgments , including machine translation , summarization and natural language generation ( Papineni et al. , 2002 ; Lin , 2004 ; Belz and Reiter , 2006 ; Callison-Burch et al. , 2007 ) .", "metadata": {"citing_string": "Lin , 2004", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2032", "citing_paper": "W11-2159"}}
{"text": "Manning ( 2011 ) discusses similar issues for part-ofspeech tagging .", "metadata": {"citing_string": "Manning ( 2011 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0419", "citing_paper": "W02-1104"}}
{"text": "For the parsing systems , we follow McDonald and Nivre ( 2007 ) and use the second order MST ( McDonald et al. , 2005 ) , as well as Malt parser with pseudo-projectivization ( Nivre and Nilsson , 2005 ) and default parameters .", "metadata": {"citing_string": "McDonald and Nivre ( 2007 )", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0825", "citing_paper": "W04-1218"}}
{"text": "For the parsing systems , we follow McDonald and Nivre ( 2007 ) and use the second order MST ( McDonald et al. , 2005 ) , as well as Malt parser with pseudo-projectivization ( Nivre and Nilsson , 2005 ) and default parameters .", "metadata": {"citing_string": "McDonald et al. , 2005", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2117", "citing_paper": "W03-2127"}}
{"text": "For the parsing systems , we follow McDonald and Nivre ( 2007 ) and use the second order MST ( McDonald et al. , 2005 ) , as well as Malt parser with pseudo-projectivization ( Nivre and Nilsson , 2005 ) and default parameters .", "metadata": {"citing_string": "Nivre and Nilsson , 2005", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1403", "citing_paper": "W11-2134"}}
{"text": "The three most commonly used metrics are those from the CoNLL 2006 -- 7 shared tasks ( Buchholz and Marsi , 2006 ) : unlabeled attachment score ( UAS ) , label accuracy ( LA ) , both introduced by Eisner ( 1996 ) , and labeled attachment score ( LAS ) , the pivotal dependency parsing metric introduced by Nivre et al. ( 2004 ) .", "metadata": {"citing_string": "Nivre et al. ( 2004 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-3815", "citing_paper": "W04-0209"}}
{"text": "Several authors have pointed out problems with these metrics ; they are both sensitive to annotation guidelines ( Schwartz et al. , 2012 ; Tsarfaty et al. , 2011 ) , and they fail to say anything about how parsers fare on rare , but important linguistic constructions ( Nivre et al. , 2010 ) .", "metadata": {"citing_string": "Nivre et al. , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3346", "citing_paper": "W06-1420"}}
{"text": "Parsing metrics are sensitive to the choice of annotation scheme ( Schwartz et al. , 2012 ; Tsarfaty et al. , 2011 ) and fail to capture how parsers fare on important linguistic constructions ( Nivre et al. , 2010 ) .", "metadata": {"citing_string": "Nivre et al. , 2010", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0120", "citing_paper": "W13-3605"}}
{"text": "LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing ( Oepen et al. , 2015 ) .2 LCP is triggered by a verb ( i.e. , set of nodes Vverb ) and checks whether all its core arguments match , i.e. , all outgoing dependency edges except for punctuation .", "metadata": {"citing_string": "Oepen et al. , 2015", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W02-1104", "citing_paper": "W11-2139"}}
{"text": "For instance , the standard automatic metrics for certain tasks -- such as BLEU in machine translation , or ROUGE-N and NIST in summarization or natural language generation -- were evaluated , reaching correlation coefficients well above .80 ( Papineni et al. , 2002 ; Lin , 2004 ; Belz and Reiter , 2006 ; Callison-Burch et al. , 2007 ) .", "metadata": {"citing_string": "Papineni et al. , 2002", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0604", "citing_paper": "W10-1010"}}
{"text": "The same holds for BLEU and human judgments of machine translation quality ( Papineni et al. , 2002 ) .", "metadata": {"citing_string": "Papineni et al. , 2002", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2806", "citing_paper": "W13-2203"}}
{"text": "In other NLP tasks , several studies have examined how metrics correlate with human judgments , including machine translation , summarization and natural language generation ( Papineni et al. , 2002 ; Lin , 2004 ; Belz and Reiter , 2006 ; Callison-Burch et al. , 2007 ) .", "metadata": {"citing_string": "Papineni et al. , 2002", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W09-3734"}}
{"text": "For the final figure of seven different parsing metrics , on top of the previous five , in our experiments we also include the neutral edge direction metric ( NED ) ( Schwartz et al. , 2011 ) , and tree edit distance ( TED ) ( Tsarfaty et al. , 2011 ; Tsarfaty et al. , 2012 ) .3", "metadata": {"citing_string": "Schwartz et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0825", "citing_paper": "W06-1403"}}
{"text": "Several authors have pointed out problems with these metrics ; they are both sensitive to annotation guidelines ( Schwartz et al. , 2012 ; Tsarfaty et al. , 2011 ) , and they fail to say anything about how parsers fare on rare , but important linguistic constructions ( Nivre et al. , 2010 ) .", "metadata": {"citing_string": "Schwartz et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1106", "citing_paper": "W11-1902"}}
{"text": "Parsing metrics are sensitive to the choice of annotation scheme ( Schwartz et al. , 2012 ; Tsarfaty et al. , 2011 ) and fail to capture how parsers fare on important linguistic constructions ( Nivre et al. , 2010 ) .", "metadata": {"citing_string": "Schwartz et al. , 2012", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2253", "citing_paper": "W04-2304"}}
{"text": "Several authors have pointed out problems with these metrics ; they are both sensitive to annotation guidelines ( Schwartz et al. , 2012 ; Tsarfaty et al. , 2011 ) , and they fail to say anything about how parsers fare on rare , but important linguistic constructions ( Nivre et al. , 2010 ) .", "metadata": {"citing_string": "Tsarfaty et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2806", "citing_paper": "W13-3605"}}
{"text": "Several authors have pointed out problems with these metrics ; they are both sensitive to annotation guidelines ( Schwartz et al. , 2012 ; Tsarfaty et al. , 2011 ) , and they fail to say anything about how parsers fare on rare , but important linguistic constructions ( Nivre et al. , 2010 ) .", "metadata": {"citing_string": "Tsarfaty et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0825", "citing_paper": "W04-0209"}}
{"text": "Parsing metrics are sensitive to the choice of annotation scheme ( Schwartz et al. , 2012 ; Tsarfaty et al. , 2011 ) and fail to capture how parsers fare on important linguistic constructions ( Nivre et al. , 2010 ) .", "metadata": {"citing_string": "Tsarfaty et al. , 2011", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-0111", "citing_paper": "W03-0409"}}
{"text": "For the final figure of seven different parsing metrics , on top of the previous five , in our experiments we also include the neutral edge direction metric ( NED ) ( Schwartz et al. , 2011 ) , and tree edit distance ( TED ) ( Tsarfaty et al. , 2011 ; Tsarfaty et al. , 2012 ) .3", "metadata": {"citing_string": "Tsarfaty et al. , 2012", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W02-1610", "citing_paper": "W05-1302"}}
{"text": "Children will eventually generalize learned concepts or accept synonyms in a way that violates these principles ( Baldwin , 1992 ) , but these assumptions aid in the initial acquisition of concepts .", "metadata": {"citing_string": "Baldwin , 1992", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4163", "citing_paper": "W14-3304"}}
{"text": "To determine the similarity of new properties and objects to the system 's previous knowledge of such descriptors , we use a k-Nearest Neighbor classifier ( k-NN ) with Mahalanobis distance metric ( Mahalanobis , 1936 ) , distance weighting , and class weighting using the method described in Brown and Koplowitz ( 1979 ) .", "metadata": {"citing_string": "Brown and Koplowitz ( 1979 )", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-2410", "citing_paper": "W04-0853"}}
{"text": "Language learners additionally make use of the pragmatic assumptions of Conventionality , that speakers agree upon the meaning of a word , and Contrast , that different words have different meanings ( Clark , 2009 ) .", "metadata": {"citing_string": "Clark , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4163", "citing_paper": "W11-2121"}}
{"text": "The task of training a classifier from `` bags '' of instances with a label applying to only some of the instances contained within is referred to as Multiple-Instance Learning ( MIL ) ( Dietterich , 1997 ) , and is the machine-learning analogue of cross-situational learning .", "metadata": {"citing_string": "Dietterich , 1997", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W03-2127"}}
{"text": "Online MIL methods so far have been used for object tracking ( Li et al. , 2010 ) , and Dindo and Zambuto ( 2010 ) apply MIL to grounded language learning , but we are not aware of any research that investigates the application of online MIL to studying cognitive models of incremental grounded language learning .", "metadata": {"citing_string": "Dindo and Zambuto ( 2010 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2815", "citing_paper": "W10-1916"}}
{"text": "Our end goal of using natural language to learn from visual scenes is similar to work by Krishnamurthy and Kollar ( 2013 ) and Yu and Siskind ( 2013 ) , and our emphasis on attributes is related to work by Farhadi et al. ( 2009 ) .", "metadata": {"citing_string": "Farhadi et al. ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "result", "cited_paper": "W06-1403", "citing_paper": "W10-4163"}}
{"text": "There is a wide range of methods used in MIL and a number of different assumptions that can be made to fit the task at hand ( Foulds and Frank , 2010 ) .", "metadata": {"citing_string": "Foulds and Frank , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1915", "citing_paper": "W12-3149"}}
{"text": "Fortunately , adult speakers tend to understand the limitation of these cues in certain situations and adjust their speech in accordance to Grice 's Maxim of Quantity when referring to objects : be only as informative as necessary ( Grice , 1975 ) .", "metadata": {"citing_string": "Grice , 1975", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4304", "citing_paper": "W12-3149"}}
{"text": "Color is represented in LAB space for perceptual similarity to humans using Euclidean distance , shape is captured using scaleand rotation-invariant 25-dimensional Zernike moments ( Khotanzad and Hong , 1990 ) , and texture is captured using 13-dimensional Haralick features ( Haralick et al. , 1973 ) .", "metadata": {"citing_string": "Haralick et al. , 1973", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W08-2231"}}
{"text": "Color is represented in LAB space for perceptual similarity to humans using Euclidean distance , shape is captured using scaleand rotation-invariant 25-dimensional Zernike moments ( Khotanzad and Hong , 1990 ) , and texture is captured using 13-dimensional Haralick features ( Haralick et al. , 1973 ) .", "metadata": {"citing_string": "Khotanzad and Hong , 1990", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0419", "citing_paper": "W02-1104"}}
{"text": "Kollar et al. ( 2013 ) also incorporate quantifier and pragmatic constraints on reference resolution in a setting similar to ours .", "metadata": {"citing_string": "Kollar et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W09-0604"}}
{"text": "We use these probabilities to incorporate the phenomenon of overspecification in our model , where , contrary to a strict interpretation of Grice 's Maxim of Quantity , speakers will give more information than is needed to identify a referent ( Koolen et al. , 2011 ) .", "metadata": {"citing_string": "Koolen et al. , 2011", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2134", "citing_paper": "W11-2139"}}
{"text": "The correct value for overspecification probability for a given situation depends on a number of factors such as scene complexity and descriptor type ( Koolen et al. , 2011 ) , but we have not yet incorporated these factors into our overspecification probability in this work .", "metadata": {"citing_string": "Koolen et al. , 2011", "section_number": 8, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3304", "citing_paper": "W08-2117"}}
{"text": "Our end goal of using natural language to learn from visual scenes is similar to work by Krishnamurthy and Kollar ( 2013 ) and Yu and Siskind ( 2013 ) , and our emphasis on attributes is related to work by Farhadi et al. ( 2009 ) .", "metadata": {"citing_string": "Krishnamurthy and Kollar ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "result", "cited_paper": "W08-2127", "citing_paper": "W13-1715"}}
{"text": "If the descriptor is a class name , we instead choose the Zernike shape feature , implementing the shape bias children show in word learning ( Landau et al. , 1998 ) .", "metadata": {"citing_string": "Landau et al. , 1998", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0825", "citing_paper": "W05-1302"}}
{"text": "Online MIL methods so far have been used for object tracking ( Li et al. , 2010 ) , and Dindo and Zambuto ( 2010 ) apply MIL to grounded language learning , but we are not aware of any research that investigates the application of online MIL to studying cognitive models of incremental grounded language learning .", "metadata": {"citing_string": "Li et al. , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0308", "citing_paper": "W08-0317"}}
{"text": "To determine the similarity of new properties and objects to the system 's previous knowledge of such descriptors , we use a k-Nearest Neighbor classifier ( k-NN ) with Mahalanobis distance metric ( Mahalanobis , 1936 ) , distance weighting , and class weighting using the method described in Brown and Koplowitz ( 1979 ) .", "metadata": {"citing_string": "Mahalanobis , 1936", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2203", "citing_paper": "W13-1915"}}
{"text": "The integration of language in a multi-modal task is a burgeoning area of research , with the grounded data being any of a range of possible situations , from objects on a table ( Matuszek et al. , 2012 ) to wetlab experiments ( Naim et al. , 2014 ) .", "metadata": {"citing_string": "Matuszek et al. , 2012", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-1710", "citing_paper": "W06-1403"}}
{"text": "The focus of this work is in evaluating referring expressions as in work by Mohan et al. ( 2013 ) , although without any dialogue for disambiguation .", "metadata": {"citing_string": "Mohan et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W02-1610", "citing_paper": "W00-1220"}}
{"text": "The integration of language in a multi-modal task is a burgeoning area of research , with the grounded data being any of a range of possible situations , from objects on a table ( Matuszek et al. , 2012 ) to wetlab experiments ( Naim et al. , 2014 ) .", "metadata": {"citing_string": "Naim et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-1710", "citing_paper": "W11-2121"}}
{"text": "We continue our previous work ( Perera and Allen , 2013 ) with two evaluations to demonstrate the effectiveness of applying the principles of Quantity , Contrast , and Conventionality , as well as incorporating quantifier constraints , negative information , and classification in the training step .", "metadata": {"citing_string": "Perera and Allen , 2013", "section_number": 2, "is_self_cite": true, "intent_label": "background", "cited_paper": "W09-3734", "citing_paper": "W08-0907"}}
{"text": "In previous work , we showed that the accuracy of the system 's automatic choice of representative features can reach 78 % after about 50 demonstrations of objects presented one at a time ( Perera and Allen , 2013 ) .", "metadata": {"citing_string": "Perera and Allen , 2013", "section_number": 4, "is_self_cite": true, "intent_label": "background", "cited_paper": "W04-1218", "citing_paper": "W07-0737"}}
{"text": "More details about the description generation process can be found in our previous paper ( Perera and Allen , 2013 ) .", "metadata": {"citing_string": "Perera and Allen , 2013", "section_number": 6, "is_self_cite": true, "intent_label": "background", "cited_paper": "W98-1419", "citing_paper": "W05-1208"}}
{"text": "The use of continuous features ensures that primitive concepts are grounded solely in perception and not higher-order conceptual models ( Perera and Allen , 2014 ) .", "metadata": {"citing_string": "Perera and Allen , 2014", "section_number": 9, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-0317", "citing_paper": "W10-2704"}}
{"text": "Because the squared Mahalanobis distance is equal to the number of standard deviations from the mean of the data assuming a normal distribution ( Rencher , 2003 ) , we can convert the Mahalanobis distance to a probability measure to be used in probabilistic reasoning .", "metadata": {"citing_string": "Rencher , 2003", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2138", "citing_paper": "W04-0209"}}
{"text": "We use a tutor-directed approach to training our system where the speaker presents objects to the system and describes them , as in work by Skocaj et al. ( 2011 ) .", "metadata": {"citing_string": "Skocaj et al. ( 2011 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-0203", "citing_paper": "W14-2410"}}
{"text": "However , other experiments show evidence for XSL in adults and children in certain situations ( Smith and Yu , 2008 ; Smith et al. , 2011 ) .", "metadata": {"citing_string": "Smith and Yu , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W11-2148"}}
{"text": "However , other experiments show evidence for XSL in adults and children in certain situations ( Smith and Yu , 2008 ; Smith et al. , 2011 ) .", "metadata": {"citing_string": "Smith et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2127", "citing_paper": "W14-2410"}}
{"text": "load ( Trueswell et al. , 2013 ) .", "metadata": {"citing_string": "Trueswell et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1208", "citing_paper": "W13-1915"}}
{"text": "Our end goal of using natural language to learn from visual scenes is similar to work by Krishnamurthy and Kollar ( 2013 ) and Yu and Siskind ( 2013 ) , and our emphasis on attributes is related to work by Farhadi et al. ( 2009 ) .", "metadata": {"citing_string": "Yu and Siskind ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "result", "cited_paper": "W12-3152", "citing_paper": "W10-4229"}}
{"text": "Similarly , while sentences labeled with named entities are scarce , gazetteers and databases are more readily available ( Bollacker et al. , 2008 ) .", "metadata": {"citing_string": "Bollacker et al. , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1412", "citing_paper": "W13-1902"}}
{"text": "We use the Danish , Dutch , German , Greek , English , Italian , Portuguese , Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks ( Buchholz and Marsi , 2006 ; Nivre et al. , 2007 ) .", "metadata": {"citing_string": "Buchholz and Marsi , 2006", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1902", "citing_paper": "W11-0317"}}
{"text": "There has been substantial research on how best to build models using such type-level supervision , for POS tagging , super sense tagging , NER , and relation extraction ( Craven et al. , 1999 ; Smith and Eisner , 2005 ; Carlson et al. , 2009 ; Mintz et al. , 2009 ; Johannsen et al. , 2014 ) , inter alia , focussing on parametric forms and loss functions for model training .", "metadata": {"citing_string": "Carlson et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-3605", "citing_paper": "W10-4304"}}
{"text": "Methods for training with a small labeled set have also been developed ( SÃ¸gaard , 2011 ; Garrette and Baldridge , 2013 ; Duong et al. , 2014 ) , but there have not been studies on the utility of a small labeled set for model selection versus model training .", "metadata": {"citing_string": "Duong et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1403", "citing_paper": "W04-0209"}}
{"text": "Methods for training with a small labeled set have also been developed ( SÃ¸gaard , 2011 ; Garrette and Baldridge , 2013 ; Duong et al. , 2014 ) , but there have not been studies on the utility of a small labeled set for model selection versus model training .", "metadata": {"citing_string": "Garrette and Baldridge , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3411", "citing_paper": "W11-0317"}}
{"text": "While some previous work used criteria based on the type-level supervision only ( Smith and Eisner , 2005 ; Goldwater and Griffiths , 2007 ) , much prior work used a labeled set for model selection ( Vaswani et al. , 2010 ; Soderland and Weld , 2014 ) .", "metadata": {"citing_string": "Goldwater and Griffiths , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-3149", "citing_paper": "W11-2107"}}
{"text": "There has been substantial research on how best to build models using such type-level supervision , for POS tagging , super sense tagging , NER , and relation extraction ( Craven et al. , 1999 ; Smith and Eisner , 2005 ; Carlson et al. , 2009 ; Mintz et al. , 2009 ; Johannsen et al. , 2014 ) , inter alia , focussing on parametric forms and loss functions for model training .", "metadata": {"citing_string": "Johannsen et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2415", "citing_paper": "W04-0405"}}
{"text": "There has been substantial research on how best to build models using such type-level supervision , for POS tagging , super sense tagging , NER , and relation extraction ( Craven et al. , 1999 ; Smith and Eisner , 2005 ; Carlson et al. , 2009 ; Mintz et al. , 2009 ; Johannsen et al. , 2014 ) , inter alia , focussing on parametric forms and loss functions for model training .", "metadata": {"citing_string": "Mintz et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2032", "citing_paper": "W05-0203"}}
{"text": "A simple way to use a small labeled set for parameter training together with a larger unlabeled set in our type-supervised learning setting , is to do semi-supervised model training as follows ( Nigam et al. , 2000 ) : Starting with our training loss function defined using a lexicon lex and unlabeled set TU L ( B ; lex , TU , hm ) , we define a combined loss function using both the unlabeled set TU and the labeled set TL : L ( B ; lex , TU , hm ) + AL ( B ; lex , TL , hm ) .", "metadata": {"citing_string": "Nigam et al. , 2000", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-1304", "citing_paper": "W04-2415"}}
{"text": "We use the Danish , Dutch , German , Greek , English , Italian , Portuguese , Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks ( Buchholz and Marsi , 2006 ; Nivre et al. , 2007 ) .", "metadata": {"citing_string": "Nivre et al. , 2007", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2159", "citing_paper": "W13-2209"}}
{"text": "We map the POS labels in the CoNLL datasets to the universal POS tagset ( Petrov et al. , 2012 ) .", "metadata": {"citing_string": "Petrov et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-0409", "citing_paper": "W13-1915"}}
{"text": "There has been substantial research on how best to build models using such type-level supervision , for POS tagging , super sense tagging , NER , and relation extraction ( Craven et al. , 1999 ; Smith and Eisner , 2005 ; Carlson et al. , 2009 ; Mintz et al. , 2009 ; Johannsen et al. , 2014 ) , inter alia , focussing on parametric forms and loss functions for model training .", "metadata": {"citing_string": "Smith and Eisner , 2005", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1403", "citing_paper": "W13-1915"}}
{"text": "There has been substantial research on how best to build models using such type-level supervision , for POS tagging , super sense tagging , NER , and relation extraction ( Craven et al. , 1999 ; Smith and Eisner , 2005 ; Carlson et al. , 2009 ; Mintz et al. , 2009 ; Johannsen et al. , 2014 ) , inter alia , focussing on parametric forms and loss functions for model training .", "metadata": {"citing_string": "Smith and Eisner , 2005", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3329", "citing_paper": "W11-2148"}}
{"text": "Methods for training with a small labeled set have also been developed ( SÃ¸gaard , 2011 ; Garrette and Baldridge , 2013 ; Duong et al. , 2014 ) , but there have not been studies on the utility of a small labeled set for model selection versus model training .", "metadata": {"citing_string": "SÃ¸gaard , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3411", "citing_paper": "W09-1111"}}
{"text": "While some previous work used criteria based on the type-level supervision only ( Smith and Eisner , 2005 ; Goldwater and Griffiths , 2007 ) , much prior work used a labeled set for model selection ( Vaswani et al. , 2010 ; Soderland and Weld , 2014 ) .", "metadata": {"citing_string": "Vaswani et al. , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0312", "citing_paper": "W13-3605"}}
{"text": "Several prior works used a labeled set for supervised hyper-parameter selection even when only type-level supervision is assumed to be available for training ( Vaswani et al. , 2010 ; Soderland and Weld , 2014 ) .", "metadata": {"citing_string": "Vaswani et al. , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4229", "citing_paper": "W06-0102"}}
{"text": "We initialize the transition and emission distributions of the HMM using unambiguous words as proposed by ( Zhang and DeNero , 2014 ) .", "metadata": {"citing_string": "Zhang and DeNero , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2127", "citing_paper": "W03-0409"}}
{"text": "Learning experiments In our experiments , we used type-constrained logistic regression with L2-regularization and type-constrained ( averaged ) structured perceptron ( Collins , 2002 ; T Â¨ ackstr Â¨ om et al. , 2013 ) .", "metadata": {"citing_string": "Collins , 2002", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-0201", "citing_paper": "W09-0419"}}
{"text": "Generally , readers seem to be most likely to fixate and re-fixate on nouns ( Furtner et al. , 2009 ) .", "metadata": {"citing_string": "Furtner et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0604", "citing_paper": "W10-4148"}}
{"text": "Matthies and SÃ¸gaard ( 2013 ) present results that suggest that individual variation among ( academically trained ) subjects ' reading behavior was not a greater source of error than variation within subjects , showing that it is possible to predict fixations across readers .", "metadata": {"citing_string": "Matthies and SÃ¸gaard ( 2013 )", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W09-3938", "citing_paper": "W10-4148"}}
{"text": "Eye movements during reading is a wellestablished proxy for cognitive processing , and it is well-known that readers are more likely to fixate on words from open syntactic categories ( verbs , nouns , adjectives ) than on closed category items like prepositions and conjunctions ( Rayner , 1998 ; Nilsson and Nivre , 2009 ) .", "metadata": {"citing_string": "Nilsson and Nivre , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W01-0725", "citing_paper": "W10-4229"}}
{"text": "We mapped the gold labels to the 12 Universal POS ( Petrov et al. , 2011 ) , but discarded the category X due to data sparsity .", "metadata": {"citing_string": "Petrov et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0604", "citing_paper": "W06-1420"}}
{"text": "It is established and unchallenged that function words are fixated on about 35 % of the time and content words are fixated on about 85 % of the time ( Rayner and Duffy , 1988 ) .", "metadata": {"citing_string": "Rayner and Duffy , 1988", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-1710", "citing_paper": "W06-0102"}}
{"text": "Eye movements during reading is a wellestablished proxy for cognitive processing , and it is well-known that readers are more likely to fixate on words from open syntactic categories ( verbs , nouns , adjectives ) than on closed category items like prepositions and conjunctions ( Rayner , 1998 ; Nilsson and Nivre , 2009 ) .", "metadata": {"citing_string": "Rayner , 1998", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2415", "citing_paper": "W11-0809"}}
{"text": "Features There are many different features for exploring cognitive load during reading ( Rayner , 1998 ) .", "metadata": {"citing_string": "Rayner , 1998", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1304", "citing_paper": "W10-4229"}}
{"text": "This is potentially useful , since eye-tracking data becomes more and more readily available with the emergence of eye trackers in mainstream consumer products ( San Agustin et al. , 2010 ) .", "metadata": {"citing_string": "Agustin et al. , 2010", "section_number": 2, "is_self_cite": true, "intent_label": "background", "cited_paper": "W08-0317", "citing_paper": "W04-0209"}}
{"text": "This is the normalizer in Turian et al. ( 2010 ) with Ï = 1.0 .", "metadata": {"citing_string": "Turian et al. ( 2010 )", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3329", "citing_paper": "W04-0825"}}
{"text": "Arsoy et al. ( 2012 ) have preliminarily examined deep NLMs for speech recognition , however , we believe , this is the first work that puts deep NLMs into the context of MT.", "metadata": {"citing_string": "Arsoy et al. ( 2012 )", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3346", "citing_paper": "W04-0209"}}
{"text": "It is worth mentioning another active line of research in building end-to-end neural MT systems ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Luong et al. , 2015 ; Jean et al. , 2015 ) .", "metadata": {"citing_string": "Bahdanau et al. , 2015", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W11-0402"}}
{"text": "Neural language models are fundamentally feed-forward networks as described in ( Bengio et al. , 2003 ) , but not necessarily limited to only a single hidden layer .", "metadata": {"citing_string": "Bengio et al. , 2003", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2139", "citing_paper": "W09-3734"}}
{"text": "Deep neural networks ( DNNs ) have been successful in learning more complex functions than shallow ones ( Bengio , 2009 ) and exceled in many challenging tasks such as in speech ( Hinton et al. , 2012 ) and vision ( Krizhevsky et al. , 2012 ) .", "metadata": {"citing_string": "Bengio , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2121", "citing_paper": "W06-0135"}}
{"text": "Our MT models are built using the Phrasal MT toolkit ( Cer et al. , 2010 ) .", "metadata": {"citing_string": "Cer et al. , 2010", "section_number": 4, "is_self_cite": true, "intent_label": "method", "cited_paper": "W09-3938", "citing_paper": "W00-1220"}}
{"text": "Devlin et al. ( 2014 ) reported only a small gain when decoding with a two-layer NLM over a single layer one .", "metadata": {"citing_string": "Devlin et al. ( 2014 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1403", "citing_paper": "W07-0737"}}
{"text": "Devlin et al. ( 2014 ) used a modified version of the cross-entropy objective , the self-normalized one .", "metadata": {"citing_string": "Devlin et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0419", "citing_paper": "W11-0317"}}
{"text": "4We used an alignment heuristic similar to Devlin et al. ( 2014 ) but applicable to our phrase-based MT system .", "metadata": {"citing_string": "Devlin et al. ( 2014 )", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3304", "citing_paper": "W10-1902"}}
{"text": "5As a reference point , though not directly comparable , Devlin et al. ( 2014 ) achieved 0.68 for I log ZI on a different test set with the same self-normalized constant Î± = 0.1 .", "metadata": {"citing_string": "Devlin et al. ( 2014 )", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1715", "citing_paper": "W13-4068"}}
{"text": "In addition to the standard dense feature set6 , we include a variety of sparse features for rules , word pairs , and word classes , as described in ( Green et al. , 2014 ) .", "metadata": {"citing_string": "Green et al. , 2014", "section_number": 4, "is_self_cite": true, "intent_label": "method", "cited_paper": "W04-0825", "citing_paper": "W09-3938"}}
{"text": "Deep neural networks ( DNNs ) have been successful in learning more complex functions than shallow ones ( Bengio , 2009 ) and exceled in many challenging tasks such as in speech ( Hinton et al. , 2012 ) and vision ( Krizhevsky et al. , 2012 ) .", "metadata": {"citing_string": "Hinton et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2127", "citing_paper": "W13-1902"}}
{"text": "It is worth mentioning another active line of research in building end-to-end neural MT systems ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Luong et al. , 2015 ; Jean et al. , 2015 ) .", "metadata": {"citing_string": "Jean et al. , 2015", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3304", "citing_paper": "W05-1302"}}
{"text": "It is worth mentioning another active line of research in building end-to-end neural MT systems ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Luong et al. , 2015 ; Jean et al. , 2015 ) .", "metadata": {"citing_string": "Kalchbrenner and Blunsom , 2013", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-1807", "citing_paper": "W03-1710"}}
{"text": "Deep neural networks ( DNNs ) have been successful in learning more complex functions than shallow ones ( Bengio , 2009 ) and exceled in many challenging tasks such as in speech ( Hinton et al. , 2012 ) and vision ( Krizhevsky et al. , 2012 ) .", "metadata": {"citing_string": "Krizhevsky et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0402", "citing_paper": "W04-0405"}}
{"text": "This has also been demonstrated in ( Sutskever et al. , 2014 ; Luong et al. , 2015 ) and is detailed in Section 3 .", "metadata": {"citing_string": "Luong et al. , 2015", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W98-1419", "citing_paper": "W13-4068"}}
{"text": "It reinforces the trend reported in ( Luong et al. , 2015 ) that better source-conditioned perplexities lead to better translation scores .", "metadata": {"citing_string": "Luong et al. , 2015", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2121", "citing_paper": "W09-3734"}}
{"text": "It is worth mentioning another active line of research in building end-to-end neural MT systems ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Luong et al. , 2015 ; Jean et al. , 2015 ) .", "metadata": {"citing_string": "Luong et al. , 2015", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W06-3113"}}
{"text": "As a result , past works have tried to use a more efficient version of the softmax such as the hierarchical softmax ( Morin , 2005 ; Mnih and Hinton , 2007 ; Mnih and Hinton , 2009 ) or the classbased one ( Mikolov et al. , 2010 ; Mikolov et al. , 2011 ) .", "metadata": {"citing_string": "Mnih and Hinton , 2007", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0720", "citing_paper": "W07-0737"}}
{"text": "As a result , past works have tried to use a more efficient version of the softmax such as the hierarchical softmax ( Morin , 2005 ; Mnih and Hinton , 2007 ; Mnih and Hinton , 2009 ) or the classbased one ( Mikolov et al. , 2010 ; Mikolov et al. , 2011 ) .", "metadata": {"citing_string": "Mnih and Hinton , 2009", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0604", "citing_paper": "W10-1916"}}
{"text": "Recently , the noise-contrastive estimation ( NCE ) technique ( Gutmann and Hyv Â¨ arinen , 2012 ) has been applied to train NLMs in ( Mnih and Teh , 2012 ; Vaswani et al. , 2013 ) to avoid explicitly computing the normalization factors .", "metadata": {"citing_string": "Mnih and Teh , 2012", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-3113", "citing_paper": "W04-0853"}}
{"text": "As a result , past works have tried to use a more efficient version of the softmax such as the hierarchical softmax ( Morin , 2005 ; Mnih and Hinton , 2007 ; Mnih and Hinton , 2009 ) or the classbased one ( Mikolov et al. , 2010 ; Mikolov et al. , 2011 ) .", "metadata": {"citing_string": "Morin , 2005", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2032", "citing_paper": "W11-0317"}}
{"text": "Unlike ( Devlin et al. , 2014 ) , we found that using the rectified linear function , max -LCB- 0 , x -RCB- , proposed in ( Nair and Hinton , 2010 ) , works better than tanh .", "metadata": {"citing_string": "Nair and Hinton , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2253", "citing_paper": "W14-1614"}}
{"text": "In contrast , reranking with deep NLMs of three or four layers are clearly better , yielding average improvements of 1.0 TER / 1.0 BLEU points over the baseline and 0.5 TER / 0.5 BLEU points over the system reranked with the 1-layer model , all of which are statisfically significant according to the test described in ( Riezler and Maxwell , 2005 ) .", "metadata": {"citing_string": "Riezler and Maxwell , 2005", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-1420", "citing_paper": "W11-0120"}}
{"text": "2Some work ( Schwenk , 2010 ; Schwenk et al. , 2012 ) utilize a smaller softmax vocabulary , called short-list .", "metadata": {"citing_string": "Schwenk et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W12-0111"}}
{"text": "For example , Schwenk ( 2010 , 2012 ) and Son et al. ( 2012 ) used shallow NLMs with a single hidden layer for reranking .", "metadata": {"citing_string": "Schwenk ( 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1902", "citing_paper": "W05-0203"}}
{"text": "2Some work ( Schwenk , 2010 ; Schwenk et al. , 2012 ) utilize a smaller softmax vocabulary , called short-list .", "metadata": {"citing_string": "Schwenk , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2044", "citing_paper": "W05-1302"}}
{"text": "For example , Schwenk ( 2010 , 2012 ) and Son et al. ( 2012 ) used shallow NLMs with a single hidden layer for reranking .", "metadata": {"citing_string": "Son et al. ( 2012 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0907", "citing_paper": "W13-1902"}}
{"text": "This has also been demonstrated in ( Sutskever et al. , 2014 ; Luong et al. , 2015 ) and is detailed in Section 3 .", "metadata": {"citing_string": "Sutskever et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W01-1615", "citing_paper": "W13-1902"}}
{"text": "It is worth mentioning another active line of research in building end-to-end neural MT systems ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Luong et al. , 2015 ; Jean et al. , 2015 ) .", "metadata": {"citing_string": "Sutskever et al. , 2014", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1715", "citing_paper": "W14-3304"}}
{"text": "Vaswani et al. ( 2013 ) considered two-layer NLMs for decoding but provided no comparison among models of various depths .", "metadata": {"citing_string": "Vaswani et al. ( 2013 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-1614", "citing_paper": "W11-2032"}}
{"text": "Recently , the noise-contrastive estimation ( NCE ) technique ( Gutmann and Hyv Â¨ arinen , 2012 ) has been applied to train NLMs in ( Mnih and Teh , 2012 ; Vaswani et al. , 2013 ) to avoid explicitly computing the normalization factors .", "metadata": {"citing_string": "Vaswani et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-1007", "citing_paper": "W04-0209"}}
{"text": "Bias terms are used for all hidden layers as well as the softmax layer as described earlier , which is slightly different from other work such as ( Vaswani et al. , 2013 ) .", "metadata": {"citing_string": "Vaswani et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0737", "citing_paper": "W13-1733"}}
{"text": "For example , feed forward neural networks were used in language modeling ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ) , and recurrent neural networks ( RNNs ) have yielded state-of-art results in language modeling ( Mikolov et al. , 2010 ) , language generation ( Sutskever et al. , 2011 ) and language understanding ( Yao et al. , 2013 ) .", "metadata": {"citing_string": "Bengio et al. , 2003", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1902", "citing_paper": "W14-3411"}}
{"text": "Another popular method of previous neural network models that we use and extend in this paper is the decomposition of input feature parameters using vector-matrix multiplication ( Bengio et al. , 2003 ; Collobert et al. , 2011 ; Collobert and Weston , 2008 ) .", "metadata": {"citing_string": "Bengio et al. , 2003", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2806", "citing_paper": "W07-0720"}}
{"text": "The use of unsupervised word embeddings in various natural language processing tasks has received much attention ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert , 2011 ) .", "metadata": {"citing_string": "Bengio et al. , 2003", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1010", "citing_paper": "W11-2107"}}
{"text": "Previous work has noticed that the vector-matrix multiplication of the frequent features takes most of the computation time during testing , so they cache these computations ( Bengio et al. , 2003 ; Devlin et al. , 2014 ; Chen and Manning , 2014 ) .", "metadata": {"citing_string": "Bengio et al. , 2003", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0317", "citing_paper": "W05-1617"}}
{"text": "These models can be divided into those whose design are motivated mostly by inducing useful vector representations ( e.g. ( Socher et al. , 2011 ; Socher et al. , 2013 ; Collobert , 2011 ) ) , and those whose design are motivated mostly by efficient inference and decoding ( e.g. ( Henderson , 2003 ; Henderson and Titov , 2010 ; Henderson et al. , 2013 ; Chen and Manning , 2014 ) ) .", "metadata": {"citing_string": "Chen and Manning , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3734", "citing_paper": "W08-0621"}}
{"text": "Previous work has noticed that the vector-matrix multiplication of these frequent feature combinations takes most of the computation time during testing , so they cache these computations ( Bengio et al. , 2003 ; Devlin et al. , 2014 ; Chen and Manning , 2014 ) .", "metadata": {"citing_string": "Chen and Manning , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-0111", "citing_paper": "W10-1007"}}
{"text": "To train a classifier to choose the best actions , previous work has proposed memory-based classifiers ( Nivre et al. , 2004 ) , SVMs ( Nivre et al. , 2006 ) , structured perceptron ( Huang et al. , 2012 ; Zhang and Clark , 2011 ) , two-layer neural networks ( Chen and Manning , 2014 ) , and Incremental Sigmoid Belief Networks ( ISBN ) ( Titov and", "metadata": {"citing_string": "Chen and Manning , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1617", "citing_paper": "W10-4163"}}
{"text": "Previous work has noticed that the vector-matrix multiplication of the frequent features takes most of the computation time during testing , so they cache these computations ( Bengio et al. , 2003 ; Devlin et al. , 2014 ; Chen and Manning , 2014 ) .", "metadata": {"citing_string": "Chen and Manning , 2014", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2253", "citing_paper": "W06-0135"}}
{"text": "We compare our model to the generative INN model ( Titov and Henderson , 2007b ) , MALT parser , MST parser , and the feed-forward neural network parser of ( Chen and Manning , 2014 ) ( `` C&M '' ) .", "metadata": {"citing_string": "Chen and Manning , 2014", "section_number": 8, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2815", "citing_paper": "W14-1807"}}
{"text": "To optimize the pruning decisions made by the parsing model , we use the parsing model itself to generate the negative examples ( Collins and Roark , 2004 ) .", "metadata": {"citing_string": "Collins and Roark , 2004", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2209", "citing_paper": "W03-0312"}}
{"text": "For example , feed forward neural networks were used in language modeling ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ) , and recurrent neural networks ( RNNs ) have yielded state-of-art results in language modeling ( Mikolov et al. , 2010 ) , language generation ( Sutskever et al. , 2011 ) and language understanding ( Yao et al. , 2013 ) .", "metadata": {"citing_string": "Collobert and Weston , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W07-0737"}}
{"text": "Another popular method of previous neural network models that we use and extend in this paper is the decomposition of input feature parameters using vector-matrix multiplication ( Bengio et al. , 2003 ; Collobert et al. , 2011 ; Collobert and Weston , 2008 ) .", "metadata": {"citing_string": "Collobert and Weston , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W98-1419", "citing_paper": "W13-1902"}}
{"text": "The use of unsupervised word embeddings in various natural language processing tasks has received much attention ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert , 2011 ) .", "metadata": {"citing_string": "Collobert and Weston , 2008", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2309", "citing_paper": "W10-3815"}}
{"text": "The information that is transferred from one task to another is embedded in the induced feature vectors in a shared latent space , which is input to another hidden layer for the target model ( Henderson et al. , 2013 ; Raina et al. , 2007 ; Collobert et al. , 2011 ; Glorot et al. , 2011 ) .", "metadata": {"citing_string": "Collobert et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2806", "citing_paper": "W10-1916"}}
{"text": "Another popular method of previous neural network models that we use and extend in this paper is the decomposition of input feature parameters using vector-matrix multiplication ( Bengio et al. , 2003 ; Collobert et al. , 2011 ; Collobert and Weston , 2008 ) .", "metadata": {"citing_string": "Collobert et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1617", "citing_paper": "W08-0308"}}
{"text": "These models can be divided into those whose design are motivated mostly by inducing useful vector representations ( e.g. ( Socher et al. , 2011 ; Socher et al. , 2013 ; Collobert , 2011 ) ) , and those whose design are motivated mostly by efficient inference and decoding ( e.g. ( Henderson , 2003 ; Henderson and Titov , 2010 ; Henderson et al. , 2013 ; Chen and Manning , 2014 ) ) .", "metadata": {"citing_string": "Collobert , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-0135", "citing_paper": "W13-2203"}}
{"text": "The use of unsupervised word embeddings in various natural language processing tasks has received much attention ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ; Collobert , 2011 ) .", "metadata": {"citing_string": "Collobert , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1915", "citing_paper": "W05-1302"}}
{"text": "Previous work has noticed that the vector-matrix multiplication of these frequent feature combinations takes most of the computation time during testing , so they cache these computations ( Bengio et al. , 2003 ; Devlin et al. , 2014 ; Chen and Manning , 2014 ) .", "metadata": {"citing_string": "Devlin et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1915", "citing_paper": "W10-4163"}}
{"text": "Previous work has noticed that the vector-matrix multiplication of the frequent features takes most of the computation time during testing , so they cache these computations ( Bengio et al. , 2003 ; Devlin et al. , 2014 ; Chen and Manning , 2014 ) .", "metadata": {"citing_string": "Devlin et al. , 2014", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3346", "citing_paper": "W13-2806"}}
{"text": "The information that is transferred from one task to another is embedded in the induced feature vectors in a shared latent space , which is input to another hidden layer for the target model ( Henderson et al. , 2013 ; Raina et al. , 2007 ; Collobert et al. , 2011 ; Glorot et al. , 2011 ) .", "metadata": {"citing_string": "Glorot et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W90-0112", "citing_paper": "W11-0809"}}
{"text": "Unlike a similar previous generative model ( Henderson and Titov , 2010 ) , the RNN is trained discriminatively to optimize a fast beam search .", "metadata": {"citing_string": "Henderson and Titov , 2010", "section_number": 1, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-1733", "citing_paper": "W04-1106"}}
{"text": "These models can be divided into those whose design are motivated mostly by inducing useful vector representations ( e.g. ( Socher et al. , 2011 ; Socher et al. , 2013 ; Collobert , 2011 ) ) , and those whose design are motivated mostly by efficient inference and decoding ( e.g. ( Henderson , 2003 ; Henderson and Titov , 2010 ; Henderson et al. , 2013 ; Chen and Manning , 2014 ) ) .", "metadata": {"citing_string": "Henderson and Titov , 2010", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W03-0312", "citing_paper": "W13-4068"}}
{"text": "As our RNN architecture , we use the neural network approximation of ISBNs ( Henderson and Titov , 2010 ) , which we refer to as an Incremental Neural Network ( INN ) .", "metadata": {"citing_string": "Henderson and Titov , 2010", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W11-1902", "citing_paper": "W10-4304"}}
{"text": "The results for the generative INN with a large beam were taken from ( Henderson and Titov , 2010 ) , which uses an architecture with 80 hidden units .", "metadata": {"citing_string": "Henderson and Titov , 2010", "section_number": 8, "is_self_cite": true, "intent_label": "method", "cited_paper": "W06-0135", "citing_paper": "W12-2021"}}
{"text": "In addition , the generative model ( `` Generative INN , large beam '' in Table 1 ) was compared extensively to state-ofart parsers on various languages and tasks in previous work ( Titov and Henderson , 2007b ; Titov and Henderson , 2007a ; Henderson et al. , 2008 ) .", "metadata": {"citing_string": "Henderson et al. , 2008", "section_number": 8, "is_self_cite": true, "intent_label": "result", "cited_paper": "W10-4163", "citing_paper": "W08-0621"}}
{"text": "These models can be divided into those whose design are motivated mostly by inducing useful vector representations ( e.g. ( Socher et al. , 2011 ; Socher et al. , 2013 ; Collobert , 2011 ) ) , and those whose design are motivated mostly by efficient inference and decoding ( e.g. ( Henderson , 2003 ; Henderson and Titov , 2010 ; Henderson et al. , 2013 ; Chen and Manning , 2014 ) ) .", "metadata": {"citing_string": "Henderson et al. , 2013", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-1715", "citing_paper": "W11-0120"}}
{"text": "The information that is transferred from one task to another is embedded in the induced feature vectors in a shared latent space , which is input to another hidden layer for the target model ( Henderson et al. , 2013 ; Raina et al. , 2007 ; Collobert et al. , 2011 ; Glorot et al. , 2011 ) .", "metadata": {"citing_string": "Henderson et al. , 2013", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W04-0853", "citing_paper": "W04-1106"}}
{"text": "This way of grouping action sequences into chunks associated with each word has been used previously for efficient pruning strategies in generative parsing ( Henderson , 2003 ) , and for synchronizing syntactic parsing and semantic role labeling in a joint model ( Henderson et al. , 2013 ) .", "metadata": {"citing_string": "Henderson et al. , 2013", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W12-1511", "citing_paper": "W90-0112"}}
{"text": "These models can be divided into those whose design are motivated mostly by inducing useful vector representations ( e.g. ( Socher et al. , 2011 ; Socher et al. , 2013 ; Collobert , 2011 ) ) , and those whose design are motivated mostly by efficient inference and decoding ( e.g. ( Henderson , 2003 ; Henderson and Titov , 2010 ; Henderson et al. , 2013 ; Chen and Manning , 2014 ) ) .", "metadata": {"citing_string": "Henderson , 2003", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W01-0725", "citing_paper": "W06-3113"}}
{"text": "So any information about the unbounded parse history can potentially be passed to the current decision through a chain of hidden vectors that reflects locality in the parse tree , and not just locality in the derivation sequence ( Henderson , 2003 ; Titov and Henderson , 2007b ) .", "metadata": {"citing_string": "Henderson , 2003", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W09-0604", "citing_paper": "W06-0102"}}
{"text": "This way of grouping action sequences into chunks associated with each word has been used previously for efficient pruning strategies in generative parsing ( Henderson , 2003 ) , and for synchronizing syntactic parsing and semantic role labeling in a joint model ( Henderson et al. , 2013 ) .", "metadata": {"citing_string": "Henderson , 2003", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W04-2309", "citing_paper": "W05-1302"}}
{"text": "We employ a search strategy where we prune at each shift action , but in between shift actions we consider all possible sequences of actions , similarly to the generative parser in ( Henderson , 2003 ) .", "metadata": {"citing_string": "Henderson , 2003", "section_number": 5, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-1715", "citing_paper": "W11-2044"}}
{"text": "This update is an approximation to a discriminative update on all incorrect parses that continue from an incorrect decision ( Henderson , 2004 ) .", "metadata": {"citing_string": "Henderson , 2004", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W05-1208", "citing_paper": "W04-2309"}}
{"text": "Discriminative learning further improves this aggressive pruning , because it can optimize for the discrete choice of whether to prune or not ( Huang et al. , 2012 ; Zhang and Clark , 2011 ) .", "metadata": {"citing_string": "Huang et al. , 2012", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0308", "citing_paper": "W11-2148"}}
{"text": "To train a classifier to choose the best actions , previous work has proposed memory-based classifiers ( Nivre et al. , 2004 ) , SVMs ( Nivre et al. , 2006 ) , structured perceptron ( Huang et al. , 2012 ; Zhang and Clark , 2011 ) , two-layer neural networks ( Chen and Manning , 2014 ) , and Incremental Sigmoid Belief Networks ( ISBN ) ( Titov and", "metadata": {"citing_string": "Huang et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2159", "citing_paper": "W13-1715"}}
{"text": "If a discriminative model uses normalized estimates for decisions , then once a wrong decision is made there is no way for the estimates to express that this decision has lead to a structure that is incompatible with the current or future lookahead string ( see ( Lafferty et al. , 2001 ) for more discussion ) .", "metadata": {"citing_string": "Lafferty et al. , 2001", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4163", "citing_paper": "W05-1208"}}
{"text": "For more details we refer the reader to ( Nivre et al. , 2004 ) .", "metadata": {"citing_string": "Nivre et al. , 2004", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-1106", "citing_paper": "W13-1902"}}
{"text": "The correct parses can be extracted from the training treebank by converting each dependency tree into its equivalent sequence of arc-eager shiftreduce parser actions ( Nivre et al. , 2004 ; Titov where 0 is the set of all parameters of the model ( namely WHH , Wemb. , WHO , and WCor ) , Tpos is the set of correct parses , and Toneg is the set of incorrect parses which the model 0 scores higher than their corresponding correct parses .", "metadata": {"citing_string": "Nivre et al. , 2004", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1412", "citing_paper": "W06-1403"}}
{"text": "The information that is transferred from one task to another is embedded in the induced feature vectors in a shared latent space , which is input to another hidden layer for the target model ( Henderson et al. , 2013 ; Raina et al. , 2007 ; Collobert et al. , 2011 ; Glorot et al. , 2011 ) .", "metadata": {"citing_string": "Raina et al. , 2007", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W90-0112", "citing_paper": "W08-2117"}}
{"text": "These models can be divided into those whose design are motivated mostly by inducing useful vector representations ( e.g. ( Socher et al. , 2011 ; Socher et al. , 2013 ; Collobert , 2011 ) ) , and those whose design are motivated mostly by efficient inference and decoding ( e.g. ( Henderson , 2003 ; Henderson and Titov , 2010 ; Henderson et al. , 2013 ; Chen and Manning , 2014 ) ) .", "metadata": {"citing_string": "Socher et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-0111", "citing_paper": "W11-2134"}}
{"text": "These models can be divided into those whose design are motivated mostly by inducing useful vector representations ( e.g. ( Socher et al. , 2011 ; Socher et al. , 2013 ; Collobert , 2011 ) ) , and those whose design are motivated mostly by efficient inference and decoding ( e.g. ( Henderson , 2003 ; Henderson and Titov , 2010 ; Henderson et al. , 2013 ; Chen and Manning , 2014 ) ) .", "metadata": {"citing_string": "Socher et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-2127", "citing_paper": "W14-3411"}}
{"text": "The MALT and MST parser scores come from ( Surdeanu and Manning , 2010 ) , which compared different parsing models using CoNLL 2008 shared task dataset , which is the same as CoNLL 2009 for English syntactic parsing .", "metadata": {"citing_string": "Surdeanu and Manning , 2010", "section_number": 8, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2309", "citing_paper": "W14-1614"}}
{"text": "For example , feed forward neural networks were used in language modeling ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ) , and recurrent neural networks ( RNNs ) have yielded state-of-art results in language modeling ( Mikolov et al. , 2010 ) , language generation ( Sutskever et al. , 2011 ) and language understanding ( Yao et al. , 2013 ) .", "metadata": {"citing_string": "Sutskever et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2304", "citing_paper": "W02-1610"}}
{"text": "So any information about the unbounded parse history can potentially be passed to the current decision through a chain of hidden vectors that reflects locality in the parse tree , and not just locality in the derivation sequence ( Henderson , 2003 ; Titov and Henderson , 2007b ) .", "metadata": {"citing_string": "Titov and Henderson , 2007", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W10-3815", "citing_paper": "W09-0604"}}
{"text": "In this paper we chose the exact definition of the parse actions that are used in ( Titov and Henderson , 2007b ) .", "metadata": {"citing_string": "Titov and Henderson , 2007", "section_number": 4, "is_self_cite": true, "intent_label": "method", "cited_paper": "W12-3152", "citing_paper": "W07-0737"}}
{"text": "We compare our model to the generative INN model ( Titov and Henderson , 2007b ) , MALT parser , MST parser , and the feed-forward neural network parser of ( Chen and Manning , 2014 ) ( `` C&M '' ) .", "metadata": {"citing_string": "Titov and Henderson , 2007", "section_number": 8, "is_self_cite": true, "intent_label": "method", "cited_paper": "W04-0825", "citing_paper": "W11-2139"}}
{"text": "So any information about the unbounded parse history can potentially be passed to the current decision through a chain of hidden vectors that reflects locality in the parse tree , and not just locality in the derivation sequence ( Henderson , 2003 ; Titov and Henderson , 2007b ) .", "metadata": {"citing_string": "Titov and Henderson , 2007", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W98-1419", "citing_paper": "W08-2231"}}
{"text": "In this paper we chose the exact definition of the parse actions that are used in ( Titov and Henderson , 2007b ) .", "metadata": {"citing_string": "Titov and Henderson , 2007", "section_number": 4, "is_self_cite": true, "intent_label": "method", "cited_paper": "W12-1511", "citing_paper": "W11-0809"}}
{"text": "We compare our model to the generative INN model ( Titov and Henderson , 2007b ) , MALT parser , MST parser , and the feed-forward neural network parser of ( Chen and Manning , 2014 ) ( `` C&M '' ) .", "metadata": {"citing_string": "Titov and Henderson , 2007", "section_number": 8, "is_self_cite": true, "intent_label": "method", "cited_paper": "W04-2309", "citing_paper": "W03-1710"}}
{"text": "For example , feed forward neural networks were used in language modeling ( Bengio et al. , 2003 ; Collobert and Weston , 2008 ) , and recurrent neural networks ( RNNs ) have yielded state-of-art results in language modeling ( Mikolov et al. , 2010 ) , language generation ( Sutskever et al. , 2011 ) and language understanding ( Yao et al. , 2013 ) .", "metadata": {"citing_string": "Yao et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3734", "citing_paper": "W01-0725"}}
{"text": "Discriminative learning further improves this aggressive pruning , because it can optimize for the discrete choice of whether to prune or not ( Huang et al. , 2012 ; Zhang and Clark , 2011 ) .", "metadata": {"citing_string": "Zhang and Clark , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-1010", "citing_paper": "W05-0203"}}
{"text": "To train a classifier to choose the best actions , previous work has proposed memory-based classifiers ( Nivre et al. , 2004 ) , SVMs ( Nivre et al. , 2006 ) , structured perceptron ( Huang et al. , 2012 ; Zhang and Clark , 2011 ) , two-layer neural networks ( Chen and Manning , 2014 ) , and Incremental Sigmoid Belief Networks ( ISBN ) ( Titov and", "metadata": {"citing_string": "Zhang and Clark , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3346", "citing_paper": "W08-0621"}}
{"text": "The model is trained on a dump of the English Wikipedia , automatically parsed with the C&C parser ( Clark and Curran , 2007 ) .", "metadata": {"citing_string": "Clark and Curran , 2007", "section_number": 4, "is_self_cite": true, "intent_label": "method", "cited_paper": "W10-4229", "citing_paper": "W11-2032"}}
{"text": "A proposal for the case of adjective-noun combinations is given by Baroni and Zamparelli ( 2010 ) ( and also Guevara ( 2010 ) ) .", "metadata": {"citing_string": "Guevara ( 2010 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-3815", "citing_paper": "W06-3113"}}
{"text": "The matrices can be parameterised by the syntactic type of the combining words or phrases ( Socher et al. , 2013 ; Hermann and Blunsom , 2013 ) .", "metadata": {"citing_string": "Hermann and Blunsom , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0120", "citing_paper": "W04-2304"}}
{"text": "Like Baroni and Zamparelli ( 2010 ) , our model also learn nouns as vectors and adjectives as matrices , but uses a skip-gram approach with negative sampling ( Mikolov et al. , 2013 ) , extended to learn matrices .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0405", "citing_paper": "W04-2304"}}
{"text": "To learn noun vectors , we use a skip-gram model with negative sampling ( Mikolov et al. , 2013 ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1304", "citing_paper": "W13-4068"}}
{"text": "Subsampling is used to decrease the number of frequent words ( Mikolov et al. , 2013 ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1403", "citing_paper": "W11-2159"}}
{"text": "An alternative which makes more use of grammatical structure is the recursive neural network approach of Socher et al. ( 2010 ) .", "metadata": {"citing_string": "Socher et al. ( 2010 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2815", "citing_paper": "W90-0112"}}
{"text": "Socher et al. ( 2012 ) extend this idea by representing the meanings of words and phrases as both a vector and a matrix , introducing a form of lexicalisation into the model .", "metadata": {"citing_string": "Socher et al. ( 2012 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0809", "citing_paper": "W05-1302"}}
{"text": "The matrices can be parameterised by the syntactic type of the combining words or phrases ( Socher et al. , 2013 ; Hermann and Blunsom , 2013 ) .", "metadata": {"citing_string": "Socher et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2209", "citing_paper": "W11-0809"}}
{"text": "It is interesting to note that TBSG also outperforms the result of the matrix-vector linear regression method ( REG-600 ) of Baroni and Zamparelli ( 2010 ) as reported by Vecchi et al. ( 2015 ) on the same dataset .", "metadata": {"citing_string": "Vecchi et al. ( 2015 )", "section_number": 4, "is_self_cite": false, "intent_label": "result", "cited_paper": "W00-1403", "citing_paper": "W13-2203"}}
{"text": "Andrew ( 2006 ) successfully apply semiCRFs to CWS .", "metadata": {"citing_string": "Andrew ( 2006 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2806", "citing_paper": "W14-2410"}}
{"text": "Note that the concatenation of labels across segments yields a bundle of morphological attributes similar to those found in the CoNLL datasets often used to train morphological taggers ( Buchholz and Marsi , 2006 ) -- thus LMS helps to unify UMS and morphological tagging .", "metadata": {"citing_string": "Buchholz and Marsi , 2006", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4229", "citing_paper": "W08-1137"}}
{"text": "Ruokolainen et al. ( 2013 ) present an averaged perceptron ( Collins , 2002 ) , a discriminative structured prediction method , for UMS .", "metadata": {"citing_string": "Collins , 2002", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-0201", "citing_paper": "W03-1710"}}
{"text": "LINGUISTICA ( Goldsmith , 2001 ) and MORFESSOR ( Creutz and Lagus , 2002 ) are built around an idea of optimally encoding the data , in the sense of minimal description length ( MDL ) .", "metadata": {"citing_string": "Creutz and Lagus , 2002", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2159", "citing_paper": "W11-2148"}}
{"text": "They employ the threestate tagset of Creutz and Lagus ( 2004 ) ( row 1 in Figure 2 ) for Arabic and Hebrew UMS .", "metadata": {"citing_string": "Creutz and Lagus ( 2004 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-0135", "citing_paper": "W01-1615"}}
{"text": "A specific form of morphological processing , morphological segmentation , has shown its utility for machine translation ( Dyer et al. , 2008 ) , sentiment analysis ( AbdulMageed et al. , 2014 ) , bilingual word alignment ( EyigÃ¶z et al. , 2013 ) , speech processing ( Creutz et al. , 2007b ) and keyword spotting ( Narasimhan et al. , 2014 ) , inter alia .", "metadata": {"citing_string": "Dyer et al. , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2304", "citing_paper": "W14-3346"}}
{"text": "A specific form of morphological processing , morphological segmentation , has shown its utility for machine translation ( Dyer et al. , 2008 ) , sentiment analysis ( AbdulMageed et al. , 2014 ) , bilingual word alignment ( EyigÃ¶z et al. , 2013 ) , speech processing ( Creutz et al. , 2007b ) and keyword spotting ( Narasimhan et al. , 2014 ) , inter alia .", "metadata": {"citing_string": "EyigÃ¶z et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0419", "citing_paper": "W04-2309"}}
{"text": "LINGUISTICA ( Goldsmith , 2001 ) and MORFESSOR ( Creutz and Lagus , 2002 ) are built around an idea of optimally encoding the data , in the sense of minimal description length ( MDL ) .", "metadata": {"citing_string": "Goldsmith , 2001", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3411", "citing_paper": "W11-2159"}}
{"text": "Kohonen et al. ( 2010a , b ) and GrÃ¶nroos et al. ( 2014 ) present variations of MORFESSOR for semi-supervised learning .", "metadata": {"citing_string": "GrÃ¶nroos et al. ( 2014 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-0111", "citing_paper": "W14-3329"}}
{"text": "Kohonen et al. ( 2010a , b ) and GrÃ¶nroos et al. ( 2014 ) present variations of MORFESSOR for semi-supervised learning .", "metadata": {"citing_string": "Kohonen et al. ( 2010", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0312", "citing_paper": "W14-3329"}}
{"text": "Kohonen et al. ( 2010a , b ) and GrÃ¶nroos et al. ( 2014 ) present variations of MORFESSOR for semi-supervised learning .", "metadata": {"citing_string": "Kohonen et al. ( 2010", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W13-2209"}}
{"text": "We use the unannotated part of Ukwabelana for Zulu ; and for Indonesian , data from Wikipedia and the corpus of Krisnawati and Schulz ( 2013 ) .", "metadata": {"citing_string": "Krisnawati and Schulz ( 2013 )", "section_number": 7, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3411", "citing_paper": "W10-1902"}}
{"text": "The segmentation data for English , Finnish and Turkish was taken from MorphoChallenge 2010 ( Kurimo et al. , 2010 ) .5 Despite typically being used for UMS tasks , the MorphoChallenge datasets do contain morpheme level", "metadata": {"citing_string": "Kurimo et al. , 2010", "section_number": 7, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2304", "citing_paper": "W13-4068"}}
{"text": "Just as linear-chain CRFs are discriminative adaptations of hidden Markov models ( Lafferty et al. , 2001 ) , semi-CRFs are an analogous adaptation of hidden semi-Markov models ( Murphy , 2002 ) .", "metadata": {"citing_string": "Lafferty et al. , 2001", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-2410", "citing_paper": "W98-1419"}}
{"text": "We employ the maximum-likelihood criterion to estimate the parameters with L-BFGS ( Liu and Nocedal , 1989 ) , agradient-based optimization algorithm .", "metadata": {"citing_string": "Liu and Nocedal , 1989", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W01-1615", "citing_paper": "W05-1208"}}
{"text": "Just as linear-chain CRFs are discriminative adaptations of hidden Markov models ( Lafferty et al. , 2001 ) , semi-CRFs are an analogous adaptation of hidden semi-Markov models ( Murphy , 2002 ) .", "metadata": {"citing_string": "Murphy , 2002", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1610", "citing_paper": "W05-1617"}}
{"text": "For completeness , we also compare CHIPMUNK with a first-order CRF and a higher-order CRF ( MÃ¼ller et al. , 2013 ) , both used the same n-gram features as CRF-MORPH , but without the LSV features .9 We evaluate all models using the traditional macro F1 of the segmentation boundaries .", "metadata": {"citing_string": "MÃ¼ller et al. , 2013", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-1710", "citing_paper": "W03-0409"}}
{"text": "Both of the baselines in Table 9 are 0th-order versions of the state-of-the-art CRF-based morphological tagger MARMOT ( MÃ¼ller et al. , 2013 ) ( since our model is type-based ) , making this a strong baseline .", "metadata": {"citing_string": "MÃ¼ller et al. , 2013", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0402", "citing_paper": "W01-0725"}}
{"text": "Current stateof-the-art models only employ character level ngram features to model word-internals ( MÃ¼ller et al. , 2013 ) .", "metadata": {"citing_string": "MÃ¼ller et al. , 2013", "section_number": 9, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1412", "citing_paper": "W05-0203"}}
{"text": "A specific form of morphological processing , morphological segmentation , has shown its utility for machine translation ( Dyer et al. , 2008 ) , sentiment analysis ( AbdulMageed et al. , 2014 ) , bilingual word alignment ( EyigÃ¶z et al. , 2013 ) , speech processing ( Creutz et al. , 2007b ) and keyword spotting ( Narasimhan et al. , 2014 ) , inter alia .", "metadata": {"citing_string": "Narasimhan et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-1807", "citing_paper": "W10-2704"}}
{"text": "The problem of joint CWS and POS tagging ( Ng and Low , 2004 ; Zhang and Clark , 2008 ) is related to LMS .", "metadata": {"citing_string": "Ng and Low , 2004", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4229", "citing_paper": "W11-2040"}}
{"text": "The model outperforms the semi-supervised model of Poon et al. ( 2009 ) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. ( 2010a ) on English , Finnish and Turkish .", "metadata": {"citing_string": "Poon et al. ( 2009 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W04-0825"}}
{"text": "The output is equivalent to that of the Porter stemmer ( Porter , 1980 ) .", "metadata": {"citing_string": "Porter , 1980", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W12-3149"}}
{"text": "Our model uses the features discussed in this section and additionally the simple n-gram context features of Ruokolainen et al. ( 2013 ) .", "metadata": {"citing_string": "Ruokolainen et al. ( 2013 )", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W10-4148", "citing_paper": "W04-1218"}}
{"text": "Ruokolainen et al. ( 2013 ) present an averaged perceptron ( Collins , 2002 ) , a discriminative structured prediction method , for UMS .", "metadata": {"citing_string": "Ruokolainen et al. ( 2013 )", "section_number": 6, "is_self_cite": false, "intent_label": "result", "cited_paper": "W13-3605", "citing_paper": "W03-0409"}}
{"text": "Our primary baseline is the state-of-the-art supervised system CRF-MORPH of Ruokolainen et al. ( 2013 ) .", "metadata": {"citing_string": "Ruokolainen et al. ( 2013 )", "section_number": 7, "is_self_cite": false, "intent_label": "result", "cited_paper": "W13-2253", "citing_paper": "W10-4304"}}
{"text": "In comparison with previous work ( Ruokolainen et al. , 2013 ) we find that our most complex model yields consistent improvements over CRFMORPH + LSV for all languages : The improvements range from > 1 for German over > 1.5 for Zulu , English , and Indonesian to > 2 for Turkish and > 4 for Finnish .", "metadata": {"citing_string": "Ruokolainen et al. , 2013", "section_number": 7, "is_self_cite": false, "intent_label": "result", "cited_paper": "W04-2304", "citing_paper": "W06-0102"}}
{"text": "Finally , Ruokolainen et al. ( 2014 ) get further consistent improvements by using features extracted from large corpora , based on the letter successor variety ( LSV ) model ( Harris , 1995 ) and on unsupervised segmentation models such as Morfessor CatMAP ( Creutz et al. , 2007a ) .", "metadata": {"citing_string": "Ruokolainen et al. ( 2014 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0419", "citing_paper": "W13-4068"}}
{"text": "We model these tasks with CHIPMUNK , a semiMarkov conditional random field ( semi-CRF ) ( Sarawagi and Cohen , 2004 ) , a model that is wellsuited for morphology .", "metadata": {"citing_string": "Sarawagi and Cohen , 2004", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W13-1902"}}
{"text": "CHIPMUNK is a supervised model implemented using the well-understood semi-Markov conditional random field ( semi-CRF ) ( Sarawagi and Cohen , 2004 ) that naturally fits the task of LMS .", "metadata": {"citing_string": "Sarawagi and Cohen , 2004", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4229", "citing_paper": "W07-0737"}}
{"text": "Wiktionary , for example , contains affix lists for all the six languages used in our experiments .4 Providing a supervised learner with such a list is a great boon , just as gazetteer features aid NER ( Smith and Osborne , 2006 ) -- perhaps even more so since suffixes and prefixes are generally closed-class ; hence these lists are likely to be comprehensive .", "metadata": {"citing_string": "Smith and Osborne , 2006", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1406", "citing_paper": "W11-0402"}}
{"text": "Virpioja et al. ( 2010 ) explored this idea for unsupervised segmentation .", "metadata": {"citing_string": "Virpioja et al. ( 2010 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2044", "citing_paper": "W07-0720"}}
{"text": "As in all exponential family models , the gradient of the log-likelihood takes the form of the difference between the observed and expected features counts ( Wainwright and Jordan , 2008 ) and can be computed efficiently with the semi-Markov extension of the forward-backward algorithm .", "metadata": {"citing_string": "Wainwright and Jordan , 2008", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-1137", "citing_paper": "W11-1406"}}
{"text": "The problem of joint CWS and POS tagging ( Ng and Low , 2004 ; Zhang and Clark , 2008 ) is related to LMS .", "metadata": {"citing_string": "Zhang and Clark , 2008", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2117", "citing_paper": "W05-1617"}}
{"text": "Lin et al. ( 2014 ) formulated the task as finding the nodes in the constituent parse that are Argument1 or Argument2 .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2219", "citing_paper": "W11-0120"}}
{"text": "Lin et al. ( 2014 ) tackled this problem by first using the connective list to identify the candidates and then using a combination of simple POS-based features and tree-based features , an approach which also achieved good performance .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W08-1137"}}
{"text": "They are the connective ( one or more words ) , the connectives POS , and the connective + its previous word ( Lin et al. , 2014 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2032", "citing_paper": "W08-2117"}}
{"text": "Ghosh et al. ( 2011 ) regarded argument extraction as a tokenlevel sequence labeling task , applying conditional random fields ( CRFs ) to label each token in a sentence .", "metadata": {"citing_string": "Ghosh et al. ( 2011 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1617", "citing_paper": "W10-2704"}}
{"text": "We apply token-level sequence labeling approach with the separate models for arguments of intra-sentential and inter-sentential explicit discourse relations ( Ghosh et al. 2011 ; Stepanov and Riccardi , 2012 ) .", "metadata": {"citing_string": "Ghosh et al. 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4163", "citing_paper": "W13-2815"}}
{"text": "Following on this work , Ghosh et al. ( 2012 ) designed many global features to help distinguish Argument1 and Argument2 within the same sentence .", "metadata": {"citing_string": "Ghosh et al. ( 2012 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3130", "citing_paper": "W09-0604"}}
{"text": "Rutherford and Xue ( 2014 ) exploit Brown cluster pairs to represent discourse relations in naturally occurring text .", "metadata": {"citing_string": "Rutherford and Xue ( 2014 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-0201", "citing_paper": "W10-4229"}}
{"text": "Prasad et al. ( 2008 ) show that arg1 may be located in various positions to the connective , such as within the same sentence ( SS ) , before ( PS ) , or after ( FS ) the sentence containing the connective .", "metadata": {"citing_string": "Prasad et al. ( 2008 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0409", "citing_paper": "W13-1715"}}
{"text": "of applying deep neural network technologies in natural language processing , we carried out an investigation of the use of recurrent neural network ( RNN ) for this difficult task ( Mesnil et al. , 2013 ; Raymond and Riccardi , 2007 ) .", "metadata": {"citing_string": "Mesnil et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0907", "citing_paper": "W13-2209"}}
{"text": "of applying deep neural network technologies in natural language processing , we carried out an investigation of the use of recurrent neural network ( RNN ) for this difficult task ( Mesnil et al. , 2013 ; Raymond and Riccardi , 2007 ) .", "metadata": {"citing_string": "Raymond and Riccardi , 2007", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-1111", "citing_paper": "W13-2203"}}
{"text": "And we apply Theano5 ( Bastien et al. , 2012 ; Bergstra et al. , 2010 ) for the RNNs .", "metadata": {"citing_string": "Bastien et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2032", "citing_paper": "W13-2203"}}
{"text": "And we apply Theano5 ( Bastien et al. , 2012 ; Bergstra et al. , 2010 ) for the RNNs .", "metadata": {"citing_string": "Bergstra et al. , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2148", "citing_paper": "W12-3152"}}
{"text": "Previous research relies on large amounts of labeled training data or leverages general semantic resources which are expensive to construct , e.g. FrameNet ( Baker et al. , 1998 ) .", "metadata": {"citing_string": "Baker et al. , 1998", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2219", "citing_paper": "W14-3304"}}
{"text": "Baroni et al. ( 2014 ) showed that this method outperforms count vector representations on a variety of tasks .", "metadata": {"citing_string": "Baroni et al. ( 2014 )", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1715", "citing_paper": "W10-1010"}}
{"text": "FrameNet has been shown to enable a correct role assignment for AGand PT-verbs ( Bethard et al. , 2004 ; Kim and Hovy , 2006 ) .", "metadata": {"citing_string": "Bethard et al. , 2004", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3411", "citing_paper": "W04-2415"}}
{"text": "mainly inspired by semantic role labeling ( Bethard et al. , 2004 ; Li et al. , 2012 ) .", "metadata": {"citing_string": "Bethard et al. , 2004", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0621", "citing_paper": "W08-2127"}}
{"text": "They have been previously found effective ( Choi et al. , 2005 ; Jakob and Gurevych , 2010 ; Wiegand and Klakow , 2012 ; Yang and Cardie , 2013 ) .", "metadata": {"citing_string": "Choi et al. , 2005", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4304", "citing_paper": "W11-2032"}}
{"text": "We use the k nearest neighbour classifier ( kNN ) ( Cover and Hart , 1967 ) as a simple method for propagating labels from seeds to other instances .", "metadata": {"citing_string": "Cover and Hart , 1967", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2415", "citing_paper": "W08-2231"}}
{"text": "Moreover , it has been effectively used for the related task of extending frames of unknown predicates in semantic parsing ( Das and Smith , 2011 ) .", "metadata": {"citing_string": "Das and Smith , 2011", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2121", "citing_paper": "W11-1406"}}
{"text": "We run the configuration that also assigns frame structures to unknown predicates ( Das and Smith , 2011 ) .", "metadata": {"citing_string": "Das and Smith , 2011", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0720", "citing_paper": "W08-2127"}}
{"text": "For semantic role labeling of FrameNetstructures ( srlframenet ) , we used Semafor ( Das et al. , 2010 ) with the argument identification based on dual decomposition ( Das et al. , 2012 ) .", "metadata": {"citing_string": "Das et al. , 2010", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4304", "citing_paper": "W10-4229"}}
{"text": "For semantic role labeling of FrameNetstructures ( srlframenet ) , we used Semafor ( Das et al. , 2010 ) with the argument identification based on dual decomposition ( Das et al. , 2012 ) .", "metadata": {"citing_string": "Das et al. , 2012", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1617", "citing_paper": "W11-2138"}}
{"text": "We do not annotate implicature-related information about effects ( Deng and Wiebe , 2014 ) but inherent sentiment ( the data release2 includes more details regarding the annotation process and our notion of holders and targets ) .", "metadata": {"citing_string": "Deng and Wiebe , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4304", "citing_paper": "W04-0825"}}
{"text": "Various relations within WordNet have been shown to be effective for polarity classification ( Esuli and Sebastiani , 2006 ; Rao and Ravichandran , 2009 ) .", "metadata": {"citing_string": "Esuli and Sebastiani , 2006", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W10-1010", "citing_paper": "W14-1614"}}
{"text": "This observation is consistent with Esuli and Sebastiani ( 2006 ) and Rao and Ravichandran ( 2009 ) .", "metadata": {"citing_string": "Esuli and Sebastiani ( 2006 )", "section_number": 6, "is_self_cite": false, "intent_label": "result", "cited_paper": "W04-2415", "citing_paper": "W14-3346"}}
{"text": "not only to noun categorization ( Riloff and Shepherd , 1997 ; Roark and Charniak , 1998 ) but also to different tasks in sentiment analysis , including polarity classification ( Hatzivassiloglou and McKeown , 1997 ) , the induction of patient polarity verbs ( Goyal et al. , 2010 ) and connotation learning ( Kang et al. , 2014 ) .", "metadata": {"citing_string": "Goyal et al. , 2010", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W01-0725", "citing_paper": "W00-1412"}}
{"text": "Instead of WordNet , we used GermaNet ( Hamp and Feldweg , 1997 ) .", "metadata": {"citing_string": "Hamp and Feldweg , 1997", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0419", "citing_paper": "W04-2304"}}
{"text": "not only to noun categorization ( Riloff and Shepherd , 1997 ; Roark and Charniak , 1998 ) but also to different tasks in sentiment analysis , including polarity classification ( Hatzivassiloglou and McKeown , 1997 ) , the induction of patient polarity verbs ( Goyal et al. , 2010 ) and connotation learning ( Kang et al. , 2014 ) .", "metadata": {"citing_string": "Hatzivassiloglou and McKeown , 1997", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-0135", "citing_paper": "W13-4068"}}
{"text": "They have been previously found effective ( Choi et al. , 2005 ; Jakob and Gurevych , 2010 ; Wiegand and Klakow , 2012 ; Yang and Cardie , 2013 ) .", "metadata": {"citing_string": "Jakob and Gurevych , 2010", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0853", "citing_paper": "W04-2304"}}
{"text": "As a classifier , we employ Support Vector Machines as implemented in SVMlight ( Joachims , 1999 ) .", "metadata": {"citing_string": "Joachims , 1999", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-1807", "citing_paper": "W09-3734"}}
{"text": "7The split-up of training and test set on the MPQA corpus follows the specification of Johansson and Moschitti ( 2013 ) .", "metadata": {"citing_string": "Johansson and Moschitti ( 2013 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-3815", "citing_paper": "W13-2806"}}
{"text": "Some work also employs information from existing semantic role labelers based on FrameNet ( Kim and Hovy , 2006 ) or PropBank ( Johansson and Moschitti , 2013 ; Wiegand and Klakow , 2012 ) .", "metadata": {"citing_string": "Johansson and Moschitti , 2013", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1412", "citing_paper": "W05-1302"}}
{"text": "not only to noun categorization ( Riloff and Shepherd , 1997 ; Roark and Charniak , 1998 ) but also to different tasks in sentiment analysis , including polarity classification ( Hatzivassiloglou and McKeown , 1997 ) , the induction of patient polarity verbs ( Goyal et al. , 2010 ) and connotation learning ( Kang et al. , 2014 ) .", "metadata": {"citing_string": "Kang et al. , 2014", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W03-0312"}}
{"text": "FrameNet has been shown to enable a correct role assignment for AGand PT-verbs ( Bethard et al. , 2004 ; Kim and Hovy , 2006 ) .", "metadata": {"citing_string": "Kim and Hovy , 2006", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0209", "citing_paper": "W03-2127"}}
{"text": "Some work also employs information from existing semantic role labelers based on FrameNet ( Kim and Hovy , 2006 ) or PropBank ( Johansson and Moschitti , 2013 ; Wiegand and Klakow , 2012 ) .", "metadata": {"citing_string": "Kim and Hovy , 2006", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2117", "citing_paper": "W06-0135"}}
{"text": "( gossip , speaker , holder ) & ( gossip , [ 1 ] , target ) 1By agent and patient , we mean constituents labeled as AO and Al in PropBank ( Kingsbury and Palmer , 2002 ) .", "metadata": {"citing_string": "Kingsbury and Palmer , 2002", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-2704", "citing_paper": "W08-0317"}}
{"text": "We use the dependency relation from Stanford parser ( Klein and Manning , 2003 ) to detect coordination ( 16 ) .", "metadata": {"citing_string": "Klein and Manning , 2003", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-3605", "citing_paper": "W04-2309"}}
{"text": "We use the PolArtsentiment lexicon ( Klenner et al. , 2009 ) ( 1416 entries ) .", "metadata": {"citing_string": "Klenner et al. , 2009", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1403", "citing_paper": "W13-2815"}}
{"text": "This agreement is mostly substantial ( Landis and Koch , 1977 ) .", "metadata": {"citing_string": "Landis and Koch , 1977", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2231", "citing_paper": "W13-2209"}}
{"text": "mainly inspired by semantic role labeling ( Bethard et al. , 2004 ; Li et al. , 2012 ) .", "metadata": {"citing_string": "Li et al. , 2012", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-0203", "citing_paper": "W06-1403"}}
{"text": "The metric proposed by Lin ( 1998 ) exploits the rich set of dependency-relation labels in the context of distributional similarity .", "metadata": {"citing_string": "Lin ( 1998 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1610", "citing_paper": "W12-2021"}}
{"text": "This concept is also known as expressive subjectivity ( Wiebe et al. , 2005 ) or speaker subjectivity ( Maks and Vossen , 2012 ) .", "metadata": {"citing_string": "Maks and Vossen , 2012", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1304", "citing_paper": "W05-1302"}}
{"text": "As an example of a competitive word embedding method , we induce vectors for our opinion verbs with Word2Vec ( Mikolov et al. , 2013 ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-0409", "citing_paper": "W04-0853"}}
{"text": "The metrics are based on WordNet 's graph structure ( Miller et al. , 1990 ) .", "metadata": {"citing_string": "Miller et al. , 1990", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-1710", "citing_paper": "W10-3815"}}
{"text": "al. , 2005 ) and WordNet ( Miller et al. , 1990 ) .", "metadata": {"citing_string": "Miller et al. , 1990", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-4068", "citing_paper": "W06-0102"}}
{"text": "We use WordNet : : Similarity ( Pedersen et al. , 2004 ) as an alternative source for similarity metrics .", "metadata": {"citing_string": "Pedersen et al. , 2004", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W11-2134"}}
{"text": "It focuses , however , on the extraction of topic-specific opinion terms ( Jijkoun et al. , 2010 ; Qiu et al. , 2011 ) rather than the variability of semantic roles for opinion holders and targets .", "metadata": {"citing_string": "Qiu et al. , 2011", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2219", "citing_paper": "W06-0102"}}
{"text": "Various relations within WordNet have been shown to be effective for polarity classification ( Esuli and Sebastiani , 2006 ; Rao and Ravichandran , 2009 ) .", "metadata": {"citing_string": "Rao and Ravichandran , 2009", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W13-2815", "citing_paper": "W00-1403"}}
{"text": "This observation is consistent with Esuli and Sebastiani ( 2006 ) and Rao and Ravichandran ( 2009 ) .", "metadata": {"citing_string": "Rao and Ravichandran ( 2009 )", "section_number": 6, "is_self_cite": false, "intent_label": "result", "cited_paper": "W10-4304", "citing_paper": "W04-0825"}}
{"text": "not only to noun categorization ( Riloff and Shepherd , 1997 ; Roark and Charniak , 1998 ) but also to different tasks in sentiment analysis , including polarity classification ( Hatzivassiloglou and McKeown , 1997 ) , the induction of patient polarity verbs ( Goyal et al. , 2010 ) and connotation learning ( Kang et al. , 2014 ) .", "metadata": {"citing_string": "Riloff and Shepherd , 1997", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2253", "citing_paper": "W05-0203"}}
{"text": "not only to noun categorization ( Riloff and Shepherd , 1997 ; Roark and Charniak , 1998 ) but also to different tasks in sentiment analysis , including polarity classification ( Hatzivassiloglou and McKeown , 1997 ) , the induction of patient polarity verbs ( Goyal et al. , 2010 ) and connotation learning ( Kang et al. , 2014 ) .", "metadata": {"citing_string": "Roark and Charniak , 1998", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-4068", "citing_paper": "W11-0120"}}
{"text": "This observation is in line with Ruppenhofer and Rehbein ( 2012 ) who claim that extensions to FrameNet are necessary to properly represent opinions evoked by verbal predicates .", "metadata": {"citing_string": "Ruppenhofer and Rehbein ( 2012 )", "section_number": 6, "is_self_cite": false, "intent_label": "result", "cited_paper": "W14-3329", "citing_paper": "W08-0907"}}
{"text": "Opinion holder and target extraction is a hard task ( Ruppenhofer et al. , 2008 ) .", "metadata": {"citing_string": "Ruppenhofer et al. , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0825", "citing_paper": "W04-0825"}}
{"text": "As a parser , we used ParZu ( Sennrich et al. , 2009 ) .", "metadata": {"citing_string": "Sennrich et al. , 2009", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0621", "citing_paper": "W10-1007"}}
{"text": "We then use the Adsorption label propagation algorithm from junto ( Talukdar et al. , 2008 ) in order propagate the labels from the seeds to the remaining verbs .", "metadata": {"citing_string": "Talukdar et al. , 2008", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0419", "citing_paper": "W10-3815"}}
{"text": "One major reason for this is that we do not annotate on the sense-level ( word-sense disambiguation ( Wiebe and Mihalcea , 2006 ) is still in its infancy ) but on the lemmalevel .", "metadata": {"citing_string": "Wiebe and Mihalcea , 2006", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-0111", "citing_paper": "W11-0317"}}
{"text": "Wiegand and Klakow ( 2012 ) proposed methods for extracting AGand PT-verbs .", "metadata": {"citing_string": "Wiegand and Klakow ( 2012 )", "section_number": 5, "is_self_cite": true, "intent_label": "result", "cited_paper": "W12-0111", "citing_paper": "W12-1511"}}
{"text": "For AG-verbs , we rely on the findings of Wiegand and Klakow ( 2012 ) who suggest that verbs predictive for opinion holders can be induced with the help of prototypical opinion holders .", "metadata": {"citing_string": "Wiegand and Klakow ( 2012 )", "section_number": 5, "is_self_cite": true, "intent_label": "result", "cited_paper": "W11-2139", "citing_paper": "W13-2209"}}
{"text": "As far as AGand PT-verbs are concerned , the entire lists of these initialization methods correspond to the original approach of Wiegand and Klakow ( 2012 ) .", "metadata": {"citing_string": "Wiegand and Klakow ( 2012 )", "section_number": 6, "is_self_cite": true, "intent_label": "result", "cited_paper": "W06-1403", "citing_paper": "W13-2253"}}
{"text": "They have been previously found effective ( Choi et al. , 2005 ; Jakob and Gurevych , 2010 ; Wiegand and Klakow , 2012 ; Yang and Cardie , 2013 ) .", "metadata": {"citing_string": "Wiegand and Klakow , 2012", "section_number": 6, "is_self_cite": true, "intent_label": "result", "cited_paper": "W13-1902", "citing_paper": "W04-1106"}}
{"text": "FICTION , introduced in Wiegand and Klakow ( 2012 ) , is a collection of summaries of classic literary works .", "metadata": {"citing_string": "Wiegand and Klakow ( 2012 )", "section_number": 6, "is_self_cite": true, "intent_label": "result", "cited_paper": "W10-1010", "citing_paper": "W00-1412"}}
{"text": "For classifiers , we consider convolution kernels CK from Wiegand and Klakow ( 2012 ) and the sequence labeler from Johansson and Moschitti ( 2013 ) MultiRel that incorporates relational features taking into account interactions between multiple opinion cues .", "metadata": {"citing_string": "Wiegand and Klakow ( 2012 )", "section_number": 6, "is_self_cite": true, "intent_label": "result", "cited_paper": "W12-2021", "citing_paper": "W07-0737"}}
{"text": "The new induction proposed in this paper also notably outperforms the induction from Wiegand and Klakow ( 2012 ) .", "metadata": {"citing_string": "Wiegand and Klakow ( 2012 )", "section_number": 6, "is_self_cite": true, "intent_label": "result", "cited_paper": "W08-1137", "citing_paper": "W04-0843"}}
{"text": "Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon ( Wilson et al. , 2005 ) .", "metadata": {"citing_string": "Wilson et al. , 2005", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-3938", "citing_paper": "W08-0621"}}
{"text": "For word embeddings ( Â§ 4.2.1 ) and WordNet : : Similarity ( Â§ 4.2.2 ) , we only report the performance of the best metric/configuration , i.e. for embeddings , the continuous bag-of-words model with 500 dimensions and for WordNet : : Similarity , the Wu & Palmer measure ( Wu and Palmer , 1994 ) .", "metadata": {"citing_string": "Wu and Palmer , 1994", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-2410", "citing_paper": "W11-2121"}}
{"text": "They have been previously found effective ( Choi et al. , 2005 ; Jakob and Gurevych , 2010 ; Wiegand and Klakow , 2012 ; Yang and Cardie , 2013 ) .", "metadata": {"citing_string": "Yang and Cardie , 2013", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0419", "citing_paper": "W00-1412"}}
{"text": "For the argument labeling subtask , ( Ghosh et al. , 2011 ) regarded it as a token-level sequence labeling task using conditional random fields ( CRFs ) .", "metadata": {"citing_string": "Ghosh et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0317", "citing_paper": "W05-1208"}}
{"text": "( 2 ) our proposed connective-related features : lowercased C string and C category ( the syntactic category of the connective : subordinating , coordinating , or discourse adverbial ) , where curr and prev indicate the current and previous clause respectively and the corresponding category for the connective C is obtained from the list provided in ( Knott , 1996 ) .", "metadata": {"citing_string": "Knott , 1996", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-1304", "citing_paper": "W04-0853"}}
{"text": "( Kong et al. , 2014 ) adopted a constituent-based approach to label arguments .", "metadata": {"citing_string": "Kong et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-1807", "citing_paper": "W04-0853"}}
{"text": "Most of features used in our parser are borrowed from previous work ( Kong et al. , 2014 ; Lin et al. , 2014 ; Pitler et al. , 2009 ; Pitler and Nenkova , 2009 ; Rutherford and Xue , 2014 ) .", "metadata": {"citing_string": "Kong et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2148", "citing_paper": "W13-2815"}}
{"text": "According to ( Kong et al. , 2014 ) , Kong 's constituent-based approach outperforms Lin 's tree subtraction algorithm for the Explicit arguments extraction .", "metadata": {"citing_string": "Kong et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2219", "citing_paper": "W00-1403"}}
{"text": "( Lan et al. , 2013 ) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance .", "metadata": {"citing_string": "Lan et al. , 2013", "section_number": 2, "is_self_cite": true, "intent_label": "background", "cited_paper": "W05-1208", "citing_paper": "W00-1412"}}
{"text": "As for Implicit sense classification , ( Pitler et al. , 2009 ) , ( Lin et al. , 2009 ) and ( Rutherford and Xue , 2014 ) performed the classification using several linguistically-informed features , such as verb classes , production rules and Brown cluster pair .", "metadata": {"citing_string": "Lin et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0853", "citing_paper": "W10-4163"}}
{"text": "As for Implicit sense classification , ( Pitler et al. , 2009 ) , ( Lin et al. , 2009 ) and ( Rutherford and Xue , 2014 ) performed the classification using several linguistically-informed features , such as verb classes , production rules and Brown cluster pair .", "metadata": {"citing_string": "Lin et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1733", "citing_paper": "W09-0419"}}
{"text": "The extraction of exact argument spans and Non-Explicit sense identification have been shown to be the main challenges of the discourse parsing ( Lin et al. , 2014 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-0312", "citing_paper": "W98-1419"}}
{"text": "( Lin et al. , 2014 ) constructed a full parser on the top of these subtasks , which contained multiple components joined in a sequential pipeline architecture including a connective classifier , argument labeler , explicit classifier , non-explicit classifier , and attribution span labeler .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2815", "citing_paper": "W04-2415"}}
{"text": "Most of features used in our parser are borrowed from previous work ( Kong et al. , 2014 ; Lin et al. , 2014 ; Pitler et al. , 2009 ; Pitler and Nenkova , 2009 ; Rutherford and Xue , 2014 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-1511", "citing_paper": "W04-0209"}}
{"text": "Therefore we borrow several attribution features from ( Lin et al. , 2014 ) in order to distinguish the attributionrelated span from others .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0317", "citing_paper": "W12-3152"}}
{"text": "To identify discourse connectives from non-discourse ones and to classify the Explicit relations , ( Pitler and Nenkova , 2009 ) extracted syntactic features of connectives from the constituent parses , and showed that syntactic features improved performance in both subtasks .", "metadata": {"citing_string": "Pitler and Nenkova , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3329", "citing_paper": "W04-2415"}}
{"text": "Most of features used in our parser are borrowed from previous work ( Kong et al. , 2014 ; Lin et al. , 2014 ; Pitler et al. , 2009 ; Pitler and Nenkova , 2009 ; Rutherford and Xue , 2014 ) .", "metadata": {"citing_string": "Pitler and Nenkova , 2009", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0308", "citing_paper": "W12-3152"}}
{"text": "As for Implicit sense classification , ( Pitler et al. , 2009 ) , ( Lin et al. , 2009 ) and ( Rutherford and Xue , 2014 ) performed the classification using several linguistically-informed features , such as verb classes , production rules and Brown cluster pair .", "metadata": {"citing_string": "Pitler et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-2127", "citing_paper": "W14-1807"}}
{"text": "Most of features used in our parser are borrowed from previous work ( Kong et al. , 2014 ; Lin et al. , 2014 ; Pitler et al. , 2009 ; Pitler and Nenkova , 2009 ; Rutherford and Xue , 2014 ) .", "metadata": {"citing_string": "Pitler et al. , 2009", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3304", "citing_paper": "W03-1710"}}
{"text": "As for Implicit sense classification , ( Pitler et al. , 2009 ) , ( Lin et al. , 2009 ) and ( Rutherford and Xue , 2014 ) performed the classification using several linguistically-informed features , such as verb classes , production rules and Brown cluster pair .", "metadata": {"citing_string": "Pitler et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-1218", "citing_paper": "W14-3411"}}
{"text": "Since the release of Penn Discourse Treebank ( PDTB ) ( Prasad et al. , 2008 ) , much research has been carried out on PDTB to perform the subtasks of a full end-to-end parser , such as identifying discourse connectives , labeling arguments and classifying Explicit or Implicit relations .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2231", "citing_paper": "W11-2040"}}
{"text": "We do not identify the case which Arg2 is located in some sentences following the sentence containing the connective ( FS ) , because the statistical distribution of ( Prasad et al. , 2008 ) shows that less than 0.1 % are FS for Explicit relations .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2139", "citing_paper": "W03-0312"}}
{"text": "As for Implicit sense classification , ( Pitler et al. , 2009 ) , ( Lin et al. , 2009 ) and ( Rutherford and Xue , 2014 ) performed the classification using several linguistically-informed features , such as verb classes , production rules and Brown cluster pair .", "metadata": {"citing_string": "Rutherford and Xue , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0621", "citing_paper": "W13-2203"}}
{"text": "Most of features used in our parser are borrowed from previous work ( Kong et al. , 2014 ; Lin et al. , 2014 ; Pitler et al. , 2009 ; Pitler and Nenkova , 2009 ; Rutherford and Xue , 2014 ) .", "metadata": {"citing_string": "Rutherford and Xue , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1733", "citing_paper": "W06-0135"}}
{"text": "As for Implicit sense classification , ( Pitler et al. , 2009 ) , ( Lin et al. , 2009 ) and ( Rutherford and Xue , 2014 ) performed the classification using several linguistically-informed features , such as verb classes , production rules and Brown cluster pair .", "metadata": {"citing_string": "Rutherford and Xue , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-4068", "citing_paper": "W11-0402"}}
{"text": "A crucial step in creating the Web of Data is the process of extracting structured data , or RDF ( Adida et al. , 2012 ) from unstructured text .", "metadata": {"citing_string": "Adida et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W07-0737", "citing_paper": "W04-2304"}}
{"text": "Three tasks play a part in extracting RDF from unstructured text : Named entity recognition ( NER ) , where strings representing named entities are extracted from the given text ; entity linking ( EL ) , where each named entity recognized from NER is mapped to a appropriate resource from a knowledge base ; and relation extraction ( Usbeck et al. , 2014 ) .", "metadata": {"citing_string": "Usbeck et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W11-2159"}}
{"text": "The current state-of-art entity linking system for Korean handles both named entity recognition and entity linking as two separate steps ( Kim et al. , 2014 ) .", "metadata": {"citing_string": "Kim et al. , 2014", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W12-2021", "citing_paper": "W13-4068"}}
{"text": "The current state-of-art entity linking system for Korean handles both named entity recognition and entity linking as two separate steps ( Kim et al. , 2014 ) .", "metadata": {"citing_string": "Kim et al. , 2014", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W08-0317", "citing_paper": "W08-0907"}}
{"text": "It employs a SVM model trained with character-based features to perform named entity recognition , and uses the TF * ICF ( Mendes et al. , 2011 ) and LDA metrics to disambiguate between entity resources during entity linking .", "metadata": {"citing_string": "Mendes et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1302", "citing_paper": "W13-2806"}}
{"text": "Such work typically concentrates on lexicosemantic aspects of adjective placement ( Cinque , 2010 ; Alexiadou , 2001 ) .", "metadata": {"citing_string": "Alexiadou , 2001", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2815", "citing_paper": "W05-1302"}}
{"text": "The interactions of several dependency factors are analysed using a logit mixed effect models ( Bates et al. , 2014 ) .", "metadata": {"citing_string": "Bates et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-4068", "citing_paper": "W10-1007"}}
{"text": "Cases of heavy NP-shift ( Stallings et al. , 1998 ) , dative alternation ( Bresnan et al. , 2007 ) and other alternation preferences among verbal dependents are traditionally evoked to argue in favour of the `` heaviness '' effect .", "metadata": {"citing_string": "Bresnan et al. , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1208", "citing_paper": "W04-0405"}}
{"text": "1We use the following languages and treebanks : English , Czech , Spanish , Chinese , Catalan , German , Italian ( HajiËc et al. , 2009 ) , Danish , Dutch , Portuguese , Swedish ( Buchholz and Marsi , 2006 ) , Latin , Ancient Greek ( Haug and JÃ¸hndal , 2008 ) , Hungarian ( Csendes et al. , 2005 ) , Polish ( Woli Â´ nski et al. , 2011 ) , Arabic ( Zeman et al. , 2012 ) , French ( McDonald et al. , 2013 ) .", "metadata": {"citing_string": "Buchholz and Marsi , 2006", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2107", "citing_paper": "W04-0825"}}
{"text": "al. , 2013 ) , and Portuguese ( Buchholz and Marsi , 2006 ) .", "metadata": {"citing_string": "Buchholz and Marsi , 2006", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2203", "citing_paper": "W11-2159"}}
{"text": "Such work typically concentrates on lexicosemantic aspects of adjective placement ( Cinque , 2010 ; Alexiadou , 2001 ) .", "metadata": {"citing_string": "Cinque , 2010", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2815", "citing_paper": "W14-3346"}}
{"text": "Closest to our paper is the theoretical work of Abeill Â´ e and Godard ( 2000 ) on the placement of adjective phrases in French and recent corpusbased work by Fox and Thuilier ( 2012 ) and Thuilier ( 2012 ) .", "metadata": {"citing_string": "Fox and Thuilier ( 2012 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-4068", "citing_paper": "W04-2415"}}
{"text": "The observation that human languages appear to minimise the distance between related words is well documented in sentence processing ( Gibson , 1998 ; Hawkins , 1994 ; Hawkins , 2004 ) , in corpus properties of treebanks ( Gildea and Temperley , 2007 ; Futrell et al. , 2015 ) , in diachronic language change ( Tily , 2010 ) .", "metadata": {"citing_string": "Gibson , 1998", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W07-0720", "citing_paper": "W00-1220"}}
{"text": "A more general explanation for the weight effect has been sought in a general tendency to minimise the length of the dependency between two related words , called Dependency Length Minimisation ( DLM , Temperley ( 2007 ) , Gildea and Temperley ( 2007 ) ) .", "metadata": {"citing_string": "Gildea and Temperley ( 2007 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1610", "citing_paper": "W14-3329"}}
{"text": "The observation that human languages appear to minimise the distance between related words is well documented in sentence processing ( Gibson , 1998 ; Hawkins , 1994 ; Hawkins , 2004 ) , in corpus properties of treebanks ( Gildea and Temperley , 2007 ; Futrell et al. , 2015 ) , in diachronic language change ( Tily , 2010 ) .", "metadata": {"citing_string": "Gildea and Temperley , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-0135", "citing_paper": "W10-1010"}}
{"text": "Gildea and Temperley ( 2007 ) demonstrated that DLM applies for the dependency annotated corpora in English and German .", "metadata": {"citing_string": "Gildea and Temperley ( 2007 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W98-1419", "citing_paper": "W12-1511"}}
{"text": "1We use the following languages and treebanks : English , Czech , Spanish , Chinese , Catalan , German , Italian ( HajiËc et al. , 2009 ) , Danish , Dutch , Portuguese , Swedish ( Buchholz and Marsi , 2006 ) , Latin , Ancient Greek ( Haug and JÃ¸hndal , 2008 ) , Hungarian ( Csendes et al. , 2005 ) , Polish ( Woli Â´ nski et al. , 2011 ) , Arabic ( Zeman et al. , 2012 ) , French ( McDonald et al. , 2013 ) .", "metadata": {"citing_string": "Haug and JÃ¸hndal , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0419", "citing_paper": "W04-0209"}}
{"text": "From a typological perspective , the formulation needs to be refined from a preference of end weight to a preference for all elements being closer to the governing head : languages with Verb-Object dominant order tend to put constituents in ` short before long ' order , while Object-Verb languages , like Japanese or Korean , do the reverse ( Hawkins , 1994 ; Wasow , 2002 ) .", "metadata": {"citing_string": "Hawkins , 1994", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1617", "citing_paper": "W08-1137"}}
{"text": "The observation that human languages appear to minimise the distance between related words is well documented in sentence processing ( Gibson , 1998 ; Hawkins , 1994 ; Hawkins , 2004 ) , in corpus properties of treebanks ( Gildea and Temperley , 2007 ; Futrell et al. , 2015 ) , in diachronic language change ( Tily , 2010 ) .", "metadata": {"citing_string": "Hawkins , 1994", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W11-1304"}}
{"text": "Hawkins ( 1994 ) , in his welldeveloped variant of DLM , postulates that minimisation occurs on the dependencies between the head and the edge of the dependent phrase .", "metadata": {"citing_string": "Hawkins ( 1994 )", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2806", "citing_paper": "W10-1902"}}
{"text": "The observation that human languages appear to minimise the distance between related words is well documented in sentence processing ( Gibson , 1998 ; Hawkins , 1994 ; Hawkins , 2004 ) , in corpus properties of treebanks ( Gildea and Temperley , 2007 ; Futrell et al. , 2015 ) , in diachronic language change ( Tily , 2010 ) .", "metadata": {"citing_string": "Hawkins , 2004", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1915", "citing_paper": "W06-3113"}}
{"text": "Their results also confirm the correspondence between the dependency annotation and the experimental data , something that has been reported previously ( Merlo , 1994 ; Roland and Jurafsky , 2002 ) .", "metadata": {"citing_string": "Merlo , 1994", "section_number": 6, "is_self_cite": true, "intent_label": "result", "cited_paper": "W11-1304", "citing_paper": "W13-1915"}}
{"text": "The extraction is based on the conversion to the universal part-of-speech tags ( Petrov et al. , 2012 ) .", "metadata": {"citing_string": "Petrov et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3346", "citing_paper": "W11-2107"}}
{"text": "Specifically , all treebanks are converted to coarse universal part-of-speech tags , using existing conventional mappings from the original tagset to the universal tagset ( Petrov et al. , 2012 ) .", "metadata": {"citing_string": "Petrov et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-0135", "citing_paper": "W04-2304"}}
{"text": "Their results also confirm the correspondence between the dependency annotation and the experimental data , something that has been reported previously ( Merlo , 1994 ; Roland and Jurafsky , 2002 ) .", "metadata": {"citing_string": "Roland and Jurafsky , 2002", "section_number": 6, "is_self_cite": false, "intent_label": "result", "cited_paper": "W06-1403", "citing_paper": "W07-0737"}}
{"text": "A more general explanation for the weight effect has been sought in a general tendency to minimise the length of the dependency between two related words , called Dependency Length Minimisation ( DLM , Temperley ( 2007 ) , Gildea and Temperley ( 2007 ) ) .", "metadata": {"citing_string": "Temperley ( 2007 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-1137", "citing_paper": "W14-1614"}}
{"text": "The observation that human languages appear to minimise the distance between related words is well documented in sentence processing ( Gibson , 1998 ; Hawkins , 1994 ; Hawkins , 2004 ) , in corpus properties of treebanks ( Gildea and Temperley , 2007 ; Futrell et al. , 2015 ) , in diachronic language change ( Tily , 2010 ) .", "metadata": {"citing_string": "Temperley , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2159", "citing_paper": "W10-1902"}}
{"text": "Gildea and Temperley ( 2007 ) demonstrated that DLM applies for the dependency annotated corpora in English and German .", "metadata": {"citing_string": "Temperley ( 2007 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W11-2134"}}
{"text": "Their observations have been recently confirmed in a corpus study by Thuilier ( 2012 ) .", "metadata": {"citing_string": "Thuilier ( 2012 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1403", "citing_paper": "W13-2806"}}
{"text": "Closest to our paper is the theoretical work of Abeill Â´ e and Godard ( 2000 ) on the placement of adjective phrases in French and recent corpusbased work by Fox and Thuilier ( 2012 ) and Thuilier ( 2012 ) .", "metadata": {"citing_string": "Thuilier ( 2012 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0419", "citing_paper": "W08-0621"}}
{"text": "The observation that human languages appear to minimise the distance between related words is well documented in sentence processing ( Gibson , 1998 ; Hawkins , 1994 ; Hawkins , 2004 ) , in corpus properties of treebanks ( Gildea and Temperley , 2007 ; Futrell et al. , 2015 ) , in diachronic language change ( Tily , 2010 ) .", "metadata": {"citing_string": "Tily , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1412", "citing_paper": "W08-1137"}}
{"text": "This tendency is also formulated as a Principle of End Weight , where phrases are presented in order of increasing weight ( Wasow , 2002 ) .", "metadata": {"citing_string": "Wasow , 2002", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0317", "citing_paper": "W10-4163"}}
{"text": "From a typological perspective , the formulation needs to be refined from a preference of end weight to a preference for all elements being closer to the governing head : languages with Verb-Object dominant order tend to put constituents in ` short before long ' order , while Object-Verb languages , like Japanese or Korean , do the reverse ( Hawkins , 1994 ; Wasow , 2002 ) .", "metadata": {"citing_string": "Wasow , 2002", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0209", "citing_paper": "W05-1208"}}
{"text": "Evaluation Metrics We compare all systems using three popular metrics for coreference resolution : MUC ( Vilain et al. , 1995 ) , B3 ( Bagga and Baldwin , 1998 ) , and Entity-based CEAF ( CEAFe ) ( Luo , 2005 ) .", "metadata": {"citing_string": "Bagga and Baldwin , 1998", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2032", "citing_paper": "W90-0112"}}
{"text": "In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( Bengtson and Roth , 2008 ) .", "metadata": {"citing_string": "Bengtson and Roth , 2008", "section_number": 2, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-2044", "citing_paper": "W12-0111"}}
{"text": "For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in Bengtson and Roth ( 2008 ) .", "metadata": {"citing_string": "Bengtson and Roth ( 2008 )", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W06-1403", "citing_paper": "W11-0402"}}
{"text": "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; Bengtson and Roth , 2008 ) .", "metadata": {"citing_string": "Bengtson and Roth , 2008", "section_number": 4, "is_self_cite": true, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W08-0621"}}
{"text": "Cardie , 2002 ; Bengtson and Roth , 2008 ; Soon et al. , 2001 ) .", "metadata": {"citing_string": "Bengtson and Roth , 2008", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W10-4304", "citing_paper": "W11-0402"}}
{"text": "Cardie and Pierce ( 1998 ) propose to select certain rules based on a given corpus , to identify base noun phrases .", "metadata": {"citing_string": "Cardie and Pierce ( 1998 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0621", "citing_paper": "W08-2127"}}
{"text": "Our work is inspired by the latent left-linking model in Chang et al. ( 2013 ) and the ILP formulation from Chang et al. ( 2011 ) .", "metadata": {"citing_string": "Chang et al. ( 2011 )", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W98-1419", "citing_paper": "W90-0112"}}
{"text": "The inference algorithm is inspired by the best-left-link approach ( Chang et al. , 2011 ) , where they solve the following ILP problem :", "metadata": {"citing_string": "Chang et al. , 2011", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W11-2107", "citing_paper": "W11-0120"}}
{"text": "The introduction of ILP methods has influenced the coreference area too ( Chang et al. , 2011 ; Denis and Baldridge , 2007 ) .", "metadata": {"citing_string": "Chang et al. , 2011", "section_number": 5, "is_self_cite": true, "intent_label": "method", "cited_paper": "W08-0308", "citing_paper": "W12-0111"}}
{"text": "Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( Chang et al. , 2013 ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .", "metadata": {"citing_string": "Chang et al. , 2013", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W07-0720", "citing_paper": "W03-0312"}}
{"text": "Our work is inspired by the latent left-linking model in Chang et al. ( 2013 ) and the ILP formulation from Chang et al. ( 2011 ) .", "metadata": {"citing_string": "Chang et al. ( 2013 )", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W98-1419", "citing_paper": "W07-0737"}}
{"text": "Our work is inspired by the latent left-linking model in Chang et al. ( 2013 ) and the ILP formulation from Chang et al. ( 2011 ) .", "metadata": {"citing_string": "Chang et al. ( 2013 )", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W05-1208", "citing_paper": "W11-0120"}}
{"text": "Developed Systems Our developed system is built on the work by Chang et al. ( 2013 ) , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .", "metadata": {"citing_string": "Chang et al. ( 2013 )", "section_number": 4, "is_self_cite": true, "intent_label": "method", "cited_paper": "W09-1111", "citing_paper": "W04-0209"}}
{"text": "In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in Chang et al. ( 2013 ) in our experiments .", "metadata": {"citing_string": "Chang et al. ( 2013 )", "section_number": 5, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-2203", "citing_paper": "W10-4229"}}
{"text": "The typical BIO representation was introduced in Ramshaw and Marcus ( 1995 ) ; OC representations were introduced in Church ( 1988 ) , while Finkel and Manning ( 2009 ) further study nested named entity recognition , which employs a tree structure as a representation of identifying named entities within other named entities .", "metadata": {"citing_string": "Church ( 1988 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3304", "citing_paper": "W05-1617"}}
{"text": "We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( Collins , 1999 ) to identify their heads .", "metadata": {"citing_string": "Collins , 1999", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0737", "citing_paper": "W03-0312"}}
{"text": "Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( Collins , 1999 ) with gold constituency parsing information and gold named entity information .", "metadata": {"citing_string": "Collins , 1999", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-1106", "citing_paper": "W14-3304"}}
{"text": "Machine learning approaches were introduced in many works ( Connolly et al. , 1997 ; Ng and", "metadata": {"citing_string": "Connolly et al. , 1997", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2253", "citing_paper": "W08-0317"}}
{"text": "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; Bengtson and Roth , 2008 ) .", "metadata": {"citing_string": "Culotta et al. , 2007", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2203", "citing_paper": "W14-3411"}}
{"text": "The introduction of ILP methods has influenced the coreference area too ( Chang et al. , 2011 ; Denis and Baldridge , 2007 ) .", "metadata": {"citing_string": "Denis and Baldridge , 2007", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1220", "citing_paper": "W09-0604"}}
{"text": "Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( Chang et al. , 2013 ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .", "metadata": {"citing_string": "Durrett and Klein , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2107", "citing_paper": "W05-1302"}}
{"text": "Coreference resolution has been extensively studied , with several state-of-the-art approaches addressing this task ( Lee et al. , 2011 ; Durrett and Klein , 2013 ; Bj Â¨ orkelund and Kuhn , 2014 ; Song et al. , 2012 ) .", "metadata": {"citing_string": "Durrett and Klein , 2013", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W01-1615", "citing_paper": "W14-3329"}}
{"text": "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( Durrett and Klein , 2014 ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .", "metadata": {"citing_string": "Durrett and Klein , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-0604", "citing_paper": "W11-2134"}}
{"text": "For Berkeley system , we use the reported results from Durrett and Klein ( 2014 ) .", "metadata": {"citing_string": "Durrett and Klein ( 2014 )", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0209", "citing_paper": "W10-4304"}}
{"text": "Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; Durrett and Klein ( 2014 ) consider joint coreference and entity-linking .", "metadata": {"citing_string": "Durrett and Klein ( 2014 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2121", "citing_paper": "W08-0317"}}
{"text": "The typical BIO representation was introduced in Ramshaw and Marcus ( 1995 ) ; OC representations were introduced in Church ( 1988 ) , while Finkel and Manning ( 2009 ) further study nested named entity recognition , which employs a tree structure as a representation of identifying named entities within other named entities .", "metadata": {"citing_string": "Finkel and Manning ( 2009 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2032", "citing_paper": "W10-4229"}}
{"text": "Many of the early rule-based systems like Hobbs ( 1978 ) and Lappin and Leass ( 1994 ) gained considerable popularity .", "metadata": {"citing_string": "Hobbs ( 1978 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4148", "citing_paper": "W12-1511"}}
{"text": "We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( Hovy et al. , 2006 ) .", "metadata": {"citing_string": "Hovy et al. , 2006", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1403", "citing_paper": "W05-0203"}}
{"text": "vector is sparse and we use sparse perceptron ( Jackson and Craven , 1996 ) for supervised training .", "metadata": {"citing_string": "Jackson and Craven , 1996", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-1916", "citing_paper": "W11-2032"}}
{"text": "Many of the early rule-based systems like Hobbs ( 1978 ) and Lappin and Leass ( 1994 ) gained considerable popularity .", "metadata": {"citing_string": "Lappin and Leass ( 1994 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W09-0604"}}
{"text": "The work closest to ours is that of Lassalle and Denis ( 2015 ) , which studies a joint anaphoricity detection and coreference resolution framework .", "metadata": {"citing_string": "Lassalle and Denis ( 2015 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W07-0720", "citing_paper": "W11-0317"}}
{"text": "Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( Chang et al. , 2013 ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .", "metadata": {"citing_string": "Lee et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1902", "citing_paper": "W08-1137"}}
{"text": "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( Durrett and Klein , 2014 ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .", "metadata": {"citing_string": "Lee et al. , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2309", "citing_paper": "W13-3605"}}
{"text": "Coreference resolution has been extensively studied , with several state-of-the-art approaches addressing this task ( Lee et al. , 2011 ; Durrett and Klein , 2013 ; Bj Â¨ orkelund and Kuhn , 2014 ; Song et al. , 2012 ) .", "metadata": {"citing_string": "Lee et al. , 2011", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1610", "citing_paper": "W13-1902"}}
{"text": "Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; Durrett and Klein ( 2014 ) consider joint coreference and entity-linking .", "metadata": {"citing_string": "Lee et al. ( 2012 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-1511", "citing_paper": "W10-4229"}}
{"text": "Evaluation Metrics We compare all systems using three popular metrics for coreference resolution : MUC ( Vilain et al. , 1995 ) , B3 ( Bagga and Baldwin , 1998 ) , and Entity-based CEAF ( CEAFe ) ( Luo , 2005 ) .", "metadata": {"citing_string": "Luo , 2005", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0737", "citing_paper": "W08-0621"}}
{"text": "The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( Pradhan et al. , 2012 ) , contains 3,145 annotated documents .", "metadata": {"citing_string": "Pradhan et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1218", "citing_paper": "W12-2021"}}
{"text": "We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( Collins , 1999 ) to identify their heads .", "metadata": {"citing_string": "Punyakanok and Roth , 2001", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-0120", "citing_paper": "W06-0135"}}
{"text": "Punyakanok and Roth ( 2001 ) thoroughly study phrase identification in sentences and propose three different general approaches .", "metadata": {"citing_string": "Punyakanok and Roth ( 2001 )", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-1304", "citing_paper": "W14-3329"}}
{"text": "NER can be regarded as a sequential labeling problem , which can be modeled by several proposed models , e.g. Hidden Markov Model ( Rabiner , 1989 ) or Conditional Random Fields ( Sarawagi and Cohen , 2004 ) .", "metadata": {"citing_string": "Rabiner , 1989", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1218", "citing_paper": "W04-0843"}}
{"text": "The typical BIO representation was introduced in Ramshaw and Marcus ( 1995 ) ; OC representations were introduced in Church ( 1988 ) , while Finkel and Manning ( 2009 ) further study nested named entity recognition , which employs a tree structure as a representation of identifying named entities within other named entities .", "metadata": {"citing_string": "Ramshaw and Marcus ( 1995 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-3938", "citing_paper": "W11-2139"}}
{"text": "Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in Ratinov and Roth ( 2009 ) .", "metadata": {"citing_string": "Ratinov and Roth ( 2009 )", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W05-1617"}}
{"text": "Ratinov and Roth ( 2009 ) present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .", "metadata": {"citing_string": "Ratinov and Roth ( 2009 )", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W08-2231", "citing_paper": "W10-4229"}}
{"text": "This paper develops a joint coreference resolution and mention head detection framework as an Integer Linear Program ( ILP ) following Roth and Yih ( 2004 ) .", "metadata": {"citing_string": "Roth and Yih ( 2004 )", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W12-1511", "citing_paper": "W11-0402"}}
{"text": "NER can be regarded as a sequential labeling problem , which can be modeled by several proposed models , e.g. Hidden Markov Model ( Rabiner , 1989 ) or Conditional Random Fields ( Sarawagi and Cohen , 2004 ) .", "metadata": {"citing_string": "Sarawagi and Cohen , 2004", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W03-2127"}}
{"text": "Coreference resolution has been extensively studied , with several state-of-the-art approaches addressing this task ( Lee et al. , 2011 ; Durrett and Klein , 2013 ; Bj Â¨ orkelund and Kuhn , 2014 ; Song et al. , 2012 ) .", "metadata": {"citing_string": "Song et al. , 2012", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2304", "citing_paper": "W09-3734"}}
{"text": "Cardie , 2002 ; Bengtson and Roth , 2008 ; Soon et al. , 2001 ) .", "metadata": {"citing_string": "Soon et al. , 2001", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-1511", "citing_paper": "W00-1220"}}
{"text": "Evaluation Metrics We compare all systems using three popular metrics for coreference resolution : MUC ( Vilain et al. , 1995 ) , B3 ( Bagga and Baldwin , 1998 ) , and Entity-based CEAF ( CEAFe ) ( Luo , 2005 ) .", "metadata": {"citing_string": "Vilain et al. , 1995", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3329", "citing_paper": "W02-1104"}}
{"text": "A potential solution may lie in rich and up-to-date structured knowledge resources such as Wikidata ( VrandeËci Â´ c , 2012 ) , DBPedia ( Auer et al. , 2007 ) , and Yago ( Suchanek et al. , 2007 ) .", "metadata": {"citing_string": "Auer et al. , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W90-0112", "citing_paper": "W11-0317"}}
{"text": "DBPedia ( Auer et al. , 2007 ) contains structured information from Wikipedia : info boxes , redirections , disambiguation links , etc. .", "metadata": {"citing_string": "Auer et al. , 2007", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-0111", "citing_paper": "W11-2032"}}
{"text": "Two additional datasets were created using WordNet ( Baroni and Lenci , 2011 ; Baroni et al. , 2012 ) , whose definition of R can be trivially captured by a resource-based approach using WordNet .", "metadata": {"citing_string": "Baroni and Lenci , 2011", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2159", "citing_paper": "W13-2253"}}
{"text": "While earlier methods were mostly unsupervised , recent trends introduced supervised methods for the task ( Baroni et al. , 2012 ; Turney and Mohammad , 2015 ; Roller et al. , 2014 ) .", "metadata": {"citing_string": "Baroni et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3130", "citing_paper": "W02-1610"}}
{"text": "Two additional datasets were created using WordNet ( Baroni and Lenci , 2011 ; Baroni et al. , 2012 ) , whose definition of R can be trivially captured by a resource-based approach using WordNet .", "metadata": {"citing_string": "Baroni et al. , 2012", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-2021", "citing_paper": "W04-0853"}}
{"text": "Lexical inference has been thoroughly explored in distributional semantics , with recent supervised methods ( Baroni et al. , 2012 ; Turney and Mohammad , 2015 ) showing promising results .", "metadata": {"citing_string": "Baroni et al. , 2012", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0308", "citing_paper": "W04-0209"}}
{"text": "Using these resources for various NLP tasks has become exceedingly popular ( Wu and Weld , 2010 ; Rahman and Ng , 2011 ; Unger et al. , 2012 ; Berant et al. , 2013 ) .", "metadata": {"citing_string": "Berant et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1610", "citing_paper": "W08-2117"}}
{"text": "ods to construct a feature vector for each termpair : concatenation x Â® y ( Baroni et al. , 2012 ) , difference y â x ( Roller et al. , 2014 ; Fu et al. , 2014 ; Weeds et al. , 2014 ) , and similarity x Â· y .", "metadata": {"citing_string": "Fu et al. , 2014", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-3815", "citing_paper": "W08-2117"}}
{"text": "One approach looks for chains of these predefined relations ( Harabagiu and Moldovan , 1998 ) , e.g. dog -- * mammal using a chain of hypernyms : dog -- * canine -- * carnivore -- * placental mammal -- * mammal .", "metadata": {"citing_string": "Harabagiu and Moldovan , 1998", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0907", "citing_paper": "W04-0405"}}
{"text": "Corpus-based methods are often employed to recognize lexical inferences , based on either cooccurrence patterns ( Hearst , 1992 ; Turney , 2006 ) or distributional representations ( Weeds and Weir , 2003 ; Kotlerman et al. , 2010 ) .", "metadata": {"citing_string": "Hearst , 1992", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2159", "citing_paper": "W04-0405"}}
{"text": "Corpus-based methods are often employed to recognize lexical inferences , based on either cooccurrence patterns ( Hearst , 1992 ; Turney , 2006 ) or distributional representations ( Weeds and Weir , 2003 ; Kotlerman et al. , 2010 ) .", "metadata": {"citing_string": "Kotlerman et al. , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W02-1104"}}
{"text": "kotlerman2010 ( Kotlerman et al. , 2010 ) is a manually annotated lexical entailment dataset of distributionally similar nouns .", "metadata": {"citing_string": "Kotlerman et al. , 2010", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-1710", "citing_paper": "W00-1412"}}
{"text": "levy2014 ( Levy et al. , 2014 ) was generated from manually annotated entailment graphs of subject-verb-object tuples .", "metadata": {"citing_string": "Levy et al. , 2014", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-2704", "citing_paper": "W04-0209"}}
{"text": "10Note that the corpus-based method benefits from lexical memorization ( Levy et al. , 2015 ) , overfitting for the lexical terms in the training set , while our resource-based method does not .", "metadata": {"citing_string": "Levy et al. , 2015", "section_number": 8, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-2021", "citing_paper": "W01-1615"}}
{"text": "10Note that the corpus-based method benefits from lexical memorization ( Levy et al. , 2015 ) , overfitting for the lexical terms in the training set , while our resource-based method does not .", "metadata": {"citing_string": "Levy et al. , 2015", "section_number": 8, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2138", "citing_paper": "W10-4304"}}
{"text": "To represent term-pairs with distributional features , we downloaded the pre-trained word2vec embeddings .9 These vectors were trained over a huge corpus ( 100 billion words ) using a stateof-the-art embedding algorithm ( Mikolov et al. , 2013 ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 7, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2806", "citing_paper": "W12-3152"}}
{"text": "Another approach is via WordNet Similarity ( Pedersen et al. , 2004 ) , which takes two synsets and returns a numeric value that represents their similarity based on WordNet 's hierarchical hypernymy structure .", "metadata": {"citing_string": "Pedersen et al. , 2004", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2117", "citing_paper": "W10-1010"}}
{"text": "Using these resources for various NLP tasks has become exceedingly popular ( Wu and Weld , 2010 ; Rahman and Ng , 2011 ; Unger et al. , 2012 ; Berant et al. , 2013 ) .", "metadata": {"citing_string": "Rahman and Ng , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3130", "citing_paper": "W05-1617"}}
{"text": "While earlier methods were mostly unsupervised , recent trends introduced supervised methods for the task ( Baroni et al. , 2012 ; Turney and Mohammad , 2015 ; Roller et al. , 2014 ) .", "metadata": {"citing_string": "Roller et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2309", "citing_paper": "W04-1218"}}
{"text": "ods to construct a feature vector for each termpair : concatenation x Â® y ( Baroni et al. , 2012 ) , difference y â x ( Roller et al. , 2014 ; Fu et al. , 2014 ; Weeds et al. , 2014 ) , and similarity x Â· y .", "metadata": {"citing_string": "Roller et al. , 2014", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2127", "citing_paper": "W08-0308"}}
{"text": "We trained both variants with back-propagation ( Rumelhart et al. , 1986 ) and gradient ascent .", "metadata": {"citing_string": "Rumelhart et al. , 1986", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2806", "citing_paper": "W11-0317"}}
{"text": "We thus limited the maximum path length to E = 8 and employed bidirectional search ( Russell and Norvig , 2009 , Ch .3 ) to find the shortest paths .", "metadata": {"citing_string": "Russell and Norvig , 2009", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-0135", "citing_paper": "W13-2806"}}
{"text": "We tried several local search algorithms , and found that genetic search ( Russell and Norvig , 2009 , Ch .4 ) performed well .", "metadata": {"citing_string": "Russell and Norvig , 2009", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2203", "citing_paper": "W04-2415"}}
{"text": "Little attention , however , was given to leveraging them for identifying lexical inference ; the exception being Shnarch et al. ( 2009 ) , who used structured data from Wikipedia for this purpose .", "metadata": {"citing_string": "Shnarch et al. ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-1807", "citing_paper": "W14-3411"}}
{"text": "A potential solution may lie in rich and up-to-date structured knowledge resources such as Wikidata ( VrandeËci Â´ c , 2012 ) , DBPedia ( Auer et al. , 2007 ) , and Yago ( Suchanek et al. , 2007 ) .", "metadata": {"citing_string": "Suchanek et al. , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0809", "citing_paper": "W08-1137"}}
{"text": "Yago ( Suchanek et al. , 2007 ) is a semantic knowledge base derived from Wikipedia , WordNet , and GeoNames .2 Table 2 compares the scale of the resources we used .", "metadata": {"citing_string": "Suchanek et al. , 2007", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-2704", "citing_paper": "W04-2415"}}
{"text": "While earlier methods were mostly unsupervised , recent trends introduced supervised methods for the task ( Baroni et al. , 2012 ; Turney and Mohammad , 2015 ; Roller et al. , 2014 ) .", "metadata": {"citing_string": "Turney and Mohammad , 2015", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1403", "citing_paper": "W10-1916"}}
{"text": "turney2014 ( Turney and Mohammad , 2015 ) is based on a crowdsourced dataset of semantic relations , from which we removed non-nouns and lemmatized plurals .", "metadata": {"citing_string": "Turney and Mohammad , 2015", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-3815", "citing_paper": "W10-1010"}}
{"text": "Lexical inference has been thoroughly explored in distributional semantics , with recent supervised methods ( Baroni et al. , 2012 ; Turney and Mohammad , 2015 ) showing promising results .", "metadata": {"citing_string": "Turney and Mohammad , 2015", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W07-0720", "citing_paper": "W02-1610"}}
{"text": "Corpus-based methods are often employed to recognize lexical inferences , based on either cooccurrence patterns ( Hearst , 1992 ; Turney , 2006 ) or distributional representations ( Weeds and Weir , 2003 ; Kotlerman et al. , 2010 ) .", "metadata": {"citing_string": "Turney , 2006", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2127", "citing_paper": "W11-2040"}}
{"text": "Corpus-based methods are often employed to recognize lexical inferences , based on either cooccurrence patterns ( Hearst , 1992 ; Turney , 2006 ) or distributional representations ( Weeds and Weir , 2003 ; Kotlerman et al. , 2010 ) .", "metadata": {"citing_string": "Weeds and Weir , 2003", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2107", "citing_paper": "W13-3605"}}
{"text": "ods to construct a feature vector for each termpair : concatenation x Â® y ( Baroni et al. , 2012 ) , difference y â x ( Roller et al. , 2014 ; Fu et al. , 2014 ; Weeds et al. , 2014 ) , and similarity x Â· y .", "metadata": {"citing_string": "Weeds et al. , 2014", "section_number": 7, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0409", "citing_paper": "W06-1403"}}
{"text": "Using these resources for various NLP tasks has become exceedingly popular ( Wu and Weld , 2010 ; Rahman and Ng , 2011 ; Unger et al. , 2012 ; Berant et al. , 2013 ) .", "metadata": {"citing_string": "Wu and Weld , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-0102", "citing_paper": "W12-3149"}}
{"text": "In general , genetic search is claimed to be a preferred strategy for subset selection ( Yang and Honavar , 1998 ) .", "metadata": {"citing_string": "Yang and Honavar , 1998", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2209", "citing_paper": "W00-1412"}}
{"text": "In addition to the features used by Lin et al. ( 2014 ) , we also incorporate Brown cluster ( Brown et al. , 1992 ) features generated by Liang ( 2005 ) .", "metadata": {"citing_string": "Brown et al. , 1992", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3304", "citing_paper": "W13-1715"}}
{"text": "To implement this idea we designed a constrained conditional model ( CCM ) ( Chang et al. , 2012 ) in LBJava .", "metadata": {"citing_string": "Chang et al. , 2012", "section_number": 6, "is_self_cite": true, "intent_label": "method", "cited_paper": "W11-0809", "citing_paper": "W13-2209"}}
{"text": "All the classifiers are built based on LibLinear ( Fan et al. , 2008 ) via the interface of Learning Based Java ( LBJava ) ( Rizzolo and Roth , 2010 ) .", "metadata": {"citing_string": "Fan et al. , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4229", "citing_paper": "W10-3815"}}
{"text": "The preprocessing stage identifies tokens , sentences , part-of-speech ( Roth and Zelenko , 1998 ) , shallow parse chunks ( Punyakanok and Roth , 2001 ) , lemmas2 , syntactic constituency parse ( Klein and Manning , 2003 ) , and dependency parse ( de Marneffe et al. , 2006 ) .", "metadata": {"citing_string": "Klein and Manning , 2003", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-3734", "citing_paper": "W11-0317"}}
{"text": "In addition to the features used by Lin et al. ( 2014 ) , we also incorporate Brown cluster ( Brown et al. , 1992 ) features generated by Liang ( 2005 ) .", "metadata": {"citing_string": "Liang ( 2005 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-1614", "citing_paper": "W04-2309"}}
{"text": "For our starting point , we implemented a pipeline architecture based on the description in Lin et al. ( 2014 ) , then investigated features and inference approaches to improve the system .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0402", "citing_paper": "W14-3329"}}
{"text": "We then build a classifier following Lin et al. ( 2014 ) to decide whether the candidate is a valid attribution or not .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0402", "citing_paper": "W11-2134"}}
{"text": "We follow Lin et al. ( 2014 ) to generate the product rules of both constituent parse and dependency parse features , and to generate the word pair features to enumerate all the word pairs in each pair of sentences .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-3149", "citing_paper": "W01-1615"}}
{"text": "The first classifier is similar to that developed by Lin et al. ( 2014 ) .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-4068", "citing_paper": "W11-2044"}}
{"text": "We also use another rich source of information the polarity of context , which has been previously shown to be useful for coreference problems ( Peng et al. , 2015 ) .", "metadata": {"citing_string": "Peng et al. , 2015", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W90-0112", "citing_paper": "W10-4304"}}
{"text": "To recognize explicit connectives , we construct a list of existing connectives labeled in the Penn Discourse Treebank ( Prasad et al. , 2008a ) .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2806", "citing_paper": "W09-3734"}}
{"text": "The data we used is provided through the CoNLL2015 shared task ( Xue et al. , 2015 ) , which is a modification of Penn Discourse Treebank ( PDTB ) ( Prasad et al. , 2008b ) sections 2 through 21 .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2107", "citing_paper": "W11-0809"}}
{"text": "The preprocessing stage identifies tokens , sentences , part-of-speech ( Roth and Zelenko , 1998 ) , shallow parse chunks ( Punyakanok and Roth , 2001 ) , lemmas2 , syntactic constituency parse ( Klein and Manning , 2003 ) , and dependency parse ( de Marneffe et al. , 2006 ) .", "metadata": {"citing_string": "Punyakanok and Roth , 2001", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W14-0201", "citing_paper": "W13-2209"}}
{"text": "The tagger is designed following the features used in Illinois Chunker ( Punyakanok and Roth , 2001 ) .", "metadata": {"citing_string": "Punyakanok and Roth , 2001", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-1915", "citing_paper": "W04-0853"}}
{"text": "All the classifiers are built based on LibLinear ( Fan et al. , 2008 ) via the interface of Learning Based Java ( LBJava ) ( Rizzolo and Roth , 2010 ) .", "metadata": {"citing_string": "Rizzolo and Roth , 2010", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W11-2139", "citing_paper": "W11-2107"}}
{"text": "The preprocessing stage identifies tokens , sentences , part-of-speech ( Roth and Zelenko , 1998 ) , shallow parse chunks ( Punyakanok and Roth , 2001 ) , lemmas2 , syntactic constituency parse ( Klein and Manning , 2003 ) , and dependency parse ( de Marneffe et al. , 2006 ) .", "metadata": {"citing_string": "Roth and Zelenko , 1998", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W06-0135"}}
{"text": "In addition , similar to Rutherford and Xue ( 2014 ) , we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes ( length of 4 ) in the way described in Section 2.4 .", "metadata": {"citing_string": "Rutherford and Xue ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-2127", "citing_paper": "W10-4163"}}
{"text": "We use the patterns proposed by Skadhauge and Hardt ( 2005 ) to enumerate all the candidate attribution spans .", "metadata": {"citing_string": "Skadhauge and Hardt ( 2005 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1617", "citing_paper": "W13-1902"}}
{"text": "We extract polarity information using the data provided by Wilson et al. ( 2005 ) given the predicates of two discourse arguments .", "metadata": {"citing_string": "Wilson et al. ( 2005 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3938", "citing_paper": "W04-0825"}}
{"text": "In this section , we present the data we used and results of the evaluation based on both crossvalidation on the training data ( computed within our software ) and using the official CoNLL 2015 Shared Task evaluation framework of Xue et al. ( 2015 ) .", "metadata": {"citing_string": "Xue et al. ( 2015 )", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0419", "citing_paper": "W14-2410"}}
{"text": "We design our shallow discourse parser as a sequential pipeline to mimic the annotation procedure as the Penn Discourse Treebank ( we will use PDTB instead in the rest of this paper ) annotator ( Lin et al. , 2014 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2148", "citing_paper": "W11-2040"}}
{"text": "Seven features are considered ( Pitler and Nenkova , 2009 ) :", "metadata": {"citing_string": "Pitler and Nenkova , 2009", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-2410", "citing_paper": "W04-0853"}}
{"text": "There are 100 explicit connectives in PDTB annotation ( Prasad et al. , 2008 ) .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0907", "citing_paper": "W11-0809"}}
{"text": "Maximum Entropy classifier has shown good performance in various previous works ( Wang et al. , 2014 ; Jia et al. , 2013 ; Zhao and Kit , 2008 ) .", "metadata": {"citing_string": "Wang et al. , 2014", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W13-2203", "citing_paper": "W14-3304"}}
{"text": "Maximum Entropy classifier has shown good performance in various previous works ( Wang et al. , 2014 ; Jia et al. , 2013 ; Zhao and Kit , 2008 ) .", "metadata": {"citing_string": "Zhao and Kit , 2008", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W04-2415", "citing_paper": "W05-1617"}}
{"text": "We extract features from constituent parser tree ( Zhao and Kit , 2008 ; Zhao et al. , 2009 ) .", "metadata": {"citing_string": "Zhao and Kit , 2008", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W04-0405", "citing_paper": "W13-1733"}}
{"text": "We extract features from constituent parser tree ( Zhao and Kit , 2008 ; Zhao et al. , 2009 ) .", "metadata": {"citing_string": "Zhao et al. , 2009", "section_number": 3, "is_self_cite": true, "intent_label": "method", "cited_paper": "W13-2209", "citing_paper": "W02-1104"}}
{"text": "This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring ( Webber et al. , 2012 ) or hierarchial structuring ( Asher and Lascarides , 2003 ) .", "metadata": {"citing_string": "Asher and Lascarides , 2003", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-0111", "citing_paper": "W13-1715"}}
{"text": "Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications , such as coherence modeling ( Barzilay and Lapata , 2005 ; Lin et al. , 2011 ) , text summarization ( Lin et al. , 2012 ) , and statistical machine translation ( Meyer and Webber , 2013 ) .", "metadata": {"citing_string": "Barzilay and Lapata , 2005", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-0201", "citing_paper": "W04-1218"}}
{"text": "Different from another famous discourse corpus , the Rhetorical Structure Theory ( RST ) Treebank corpus ( Carlson et al. , 2001 ) , the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency .", "metadata": {"citing_string": "Carlson et al. , 2001", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3304", "citing_paper": "W11-2138"}}
{"text": "As the largest discourse corpus , the Penn Discourse TreeBank ( PDTB ) corpus ( Prasad et al. , 2008 ) adds a layer of discourse annotations on the top of the Penn TreeBank ( PTB ) corpus ( Marcus et al. , 1993 ) and has been attracting more and more attention recently ( Elwell and Baldridge , 2008 ; Pitler and Nenkova , 2009 ; Prasad et al. , 2010 ; Ghosh et al. , 2011 ; Kong et al. , 2014 ; Lin et al. , 2014 ) .", "metadata": {"citing_string": "Elwell and Baldridge , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2815", "citing_paper": "W11-2148"}}
{"text": "As the largest discourse corpus , the Penn Discourse TreeBank ( PDTB ) corpus ( Prasad et al. , 2008 ) adds a layer of discourse annotations on the top of the Penn TreeBank ( PTB ) corpus ( Marcus et al. , 1993 ) and has been attracting more and more attention recently ( Elwell and Baldridge , 2008 ; Pitler and Nenkova , 2009 ; Prasad et al. , 2010 ; Ghosh et al. , 2011 ; Kong et al. , 2014 ; Lin et al. , 2014 ) .", "metadata": {"citing_string": "Ghosh et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0308", "citing_paper": "W11-2148"}}
{"text": "Besides that , the number of verb pairs which have the same highest VerbNet verb class ( Kipper et al. , 2006 ) is included as a feature .", "metadata": {"citing_string": "Kipper et al. , 2006", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2253", "citing_paper": "W09-1111"}}
{"text": "Production rules : According to Lin et al. ( 2009 ) , the syntactic structure of one argument may constrain the relation type and the syntactic structure of the other argument .", "metadata": {"citing_string": "Lin et al. ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0317", "citing_paper": "W13-1915"}}
{"text": "Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications , such as coherence modeling ( Barzilay and Lapata , 2005 ; Lin et al. , 2011 ) , text summarization ( Lin et al. , 2012 ) , and statistical machine translation ( Meyer and Webber , 2013 ) .", "metadata": {"citing_string": "Lin et al. , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2203", "citing_paper": "W14-0201"}}
{"text": "Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications , such as coherence modeling ( Barzilay and Lapata , 2005 ; Lin et al. , 2011 ) , text summarization ( Lin et al. , 2012 ) , and statistical machine translation ( Meyer and Webber , 2013 ) .", "metadata": {"citing_string": "Lin et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0312", "citing_paper": "W14-3411"}}
{"text": "As the largest discourse corpus , the Penn Discourse TreeBank ( PDTB ) corpus ( Prasad et al. , 2008 ) adds a layer of discourse annotations on the top of the Penn TreeBank ( PTB ) corpus ( Marcus et al. , 1993 ) and has been attracting more and more attention recently ( Elwell and Baldridge , 2008 ; Pitler and Nenkova , 2009 ; Prasad et al. , 2010 ; Ghosh et al. , 2011 ; Kong et al. , 2014 ; Lin et al. , 2014 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-1710", "citing_paper": "W13-3605"}}
{"text": "Different from the traditional approach ( i.e. , Lin et al. ( 2014 ) ) , considering the interaction between argument labeler and explicit sense classifier , co-occurrence relation between explicit and non-explicit discourse relations in a text , our system does not employ a complete sequential pipeline framework .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-0201", "citing_paper": "W04-0853"}}
{"text": "Following the work of Lin et al. ( 2014 ) , we introduce three features to train a sense classifier : the connective itself , its POS and the previous word of the connective .", "metadata": {"citing_string": "Lin et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3304", "citing_paper": "W00-1403"}}
{"text": "As the largest discourse corpus , the Penn Discourse TreeBank ( PDTB ) corpus ( Prasad et al. , 2008 ) adds a layer of discourse annotations on the top of the Penn TreeBank ( PTB ) corpus ( Marcus et al. , 1993 ) and has been attracting more and more attention recently ( Elwell and Baldridge , 2008 ; Pitler and Nenkova , 2009 ; Prasad et al. , 2010 ; Ghosh et al. , 2011 ; Kong et al. , 2014 ; Lin et al. , 2014 ) .", "metadata": {"citing_string": "Marcus et al. , 1993", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2138", "citing_paper": "W09-0604"}}
{"text": "Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications , such as coherence modeling ( Barzilay and Lapata , 2005 ; Lin et al. , 2011 ) , text summarization ( Lin et al. , 2012 ) , and statistical machine translation ( Meyer and Webber , 2013 ) .", "metadata": {"citing_string": "Meyer and Webber , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1412", "citing_paper": "W11-2138"}}
{"text": "As the largest discourse corpus , the Penn Discourse TreeBank ( PDTB ) corpus ( Prasad et al. , 2008 ) adds a layer of discourse annotations on the top of the Penn TreeBank ( PTB ) corpus ( Marcus et al. , 1993 ) and has been attracting more and more attention recently ( Elwell and Baldridge , 2008 ; Pitler and Nenkova , 2009 ; Prasad et al. , 2010 ; Ghosh et al. , 2011 ; Kong et al. , 2014 ; Lin et al. , 2014 ) .", "metadata": {"citing_string": "Pitler and Nenkova , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W07-0737", "citing_paper": "W11-0402"}}
{"text": "Pitler and Nenkova ( 2009 ) showed that syntactic features extracted from constituent parse trees are very useful in disambiguating discourse connectives .", "metadata": {"citing_string": "Pitler and Nenkova ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1106", "citing_paper": "W10-4163"}}
{"text": "Although the same connective may carry different semantics under different contexts , only a few connectives are ambiguous ( Pitler and Nenkova , 2009 ) .", "metadata": {"citing_string": "Pitler and Nenkova , 2009", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W11-2148"}}
{"text": "As the largest discourse corpus , the Penn Discourse TreeBank ( PDTB ) corpus ( Prasad et al. , 2008 ) adds a layer of discourse annotations on the top of the Penn TreeBank ( PTB ) corpus ( Marcus et al. , 1993 ) and has been attracting more and more attention recently ( Elwell and Baldridge , 2008 ; Pitler and Nenkova , 2009 ; Prasad et al. , 2010 ; Ghosh et al. , 2011 ; Kong et al. , 2014 ; Lin et al. , 2014 ) .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2815", "citing_paper": "W14-3411"}}
{"text": "As the largest discourse corpus , the Penn Discourse TreeBank ( PDTB ) corpus ( Prasad et al. , 2008 ) adds a layer of discourse annotations on the top of the Penn TreeBank ( PTB ) corpus ( Marcus et al. , 1993 ) and has been attracting more and more attention recently ( Elwell and Baldridge , 2008 ; Pitler and Nenkova , 2009 ; Prasad et al. , 2010 ; Ghosh et al. , 2011 ; Kong et al. , 2014 ; Lin et al. , 2014 ) .", "metadata": {"citing_string": "Prasad et al. , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1715", "citing_paper": "W12-1511"}}
{"text": "This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring ( Webber et al. , 2012 ) or hierarchial structuring ( Asher and Lascarides , 2003 ) .", "metadata": {"citing_string": "Webber et al. , 2012", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3411", "citing_paper": "W11-2138"}}
{"text": "The polarity of every word in arguments is derived from Multi-perspective Question Answering Opinion Corpus ( MPQA ) ( Wilson et al. , 2005 ) .", "metadata": {"citing_string": "Wilson et al. , 2005", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0312", "citing_paper": "W06-0102"}}
{"text": "The CoNLL 2015 shared task ( Xue et al. , 2015 ) evaluates end-to-end shallow discourse parsing systems for determining and classifying both explicit and non-explicit discourse relations .", "metadata": {"citing_string": "Xue et al. , 2015", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2040", "citing_paper": "W10-1007"}}
{"text": "Aggregation over such word-pairs was described by Biran and McKeown ( 2013 ) , while Park and Cardie ( 2012 ) optimized feature sets through feature selection , preprocessing and special binning techniques .", "metadata": {"citing_string": "Biran and McKeown ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2309", "citing_paper": "W09-3938"}}
{"text": "Biran and McKeown ( 2013 ) address this issue in closer detail by replacing the sparse lexical word-pair features by more dense , aggregated score features .", "metadata": {"citing_string": "Biran and McKeown ( 2013 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-3938", "citing_paper": "W13-2219"}}
{"text": "Closely related to our observation are earlier findings that using even a small stop word list has adverse effects on performance , which seems implausible at first sight ( Blair-Goldensohn et al. , 2007 ) .", "metadata": {"citing_string": "Blair-Goldensohn et al. , 2007", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W08-2127", "citing_paper": "W13-2253"}}
{"text": "6In all our experiments , we made use of the JAVA implementation of libsvm ( Chang and Lin , 2011 ) with linear kernel and default parameters .", "metadata": {"citing_string": "Chang and Lin , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2253", "citing_paper": "W14-3346"}}
{"text": "At the moment , few full-fledged end-to-end discourse parsers exist , but they use different theories of discourse , e.g. , PDTB ( Lin et al. , 2010 ) , or RST ( duVerle and Prendinger , 2009 ; Feng and Hirst , 2012 ) .", "metadata": {"citing_string": "duVerle and Prendinger , 2009", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W02-1610", "citing_paper": "W11-2121"}}
{"text": "At the moment , few full-fledged end-to-end discourse parsers exist , but they use different theories of discourse , e.g. , PDTB ( Lin et al. , 2010 ) , or RST ( duVerle and Prendinger , 2009 ; Feng and Hirst , 2012 ) .", "metadata": {"citing_string": "Feng and Hirst , 2012", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W01-0725", "citing_paper": "W09-3938"}}
{"text": "Most of the literature on automated discourse analysis has focused on specialized subtasks : Argument identification is approached by , e.g. , Ghosh et al. ( 2012 ) on the word and intersentential level , using a CRF-based approach including local and global features .", "metadata": {"citing_string": "Ghosh et al. ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-1304", "citing_paper": "W13-1902"}}
{"text": "propositions , cfXXX Lascarides and Asher ( 1993 ) or Kintsch ( 1998 ) , resp .", "metadata": {"citing_string": "Kintsch ( 1998 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3304", "citing_paper": "W03-1710"}}
{"text": "Kong et al. ( 2014 ) tackle argument span detection on the constituent-level with features for subtrees and special constraints .", "metadata": {"citing_string": "Kong et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1412", "citing_paper": "W14-0201"}}
{"text": "Lin et al. ( 2009 ) refine their work by introducing contextual and dependency information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications .", "metadata": {"citing_string": "Lin et al. ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W07-0720", "citing_paper": "W03-1710"}}
{"text": "class baseline ( 25.4 % , Expansion.Conjunction ) .11 Applying lower-case normalization to the input tends to improve classifier performance , but using a frequency threshold on the minimum number of occurrences of a feature does not : This is an interesting observation and not in line with the previous literature on implicit sense classification ; Lin et al. ( 2009 ) , for example , use a frequency cutoff of 5 for feature selection .", "metadata": {"citing_string": "Lin et al. ( 2009 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4229", "citing_paper": "W14-2410"}}
{"text": "While some previous approaches ( most notably Lin et al. , 2009 ) incorporate cutoffs in their experiments , others do not .", "metadata": {"citing_string": "Lin et al. , 2009", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1403", "citing_paper": "W13-1733"}}
{"text": "At the moment , few full-fledged end-to-end discourse parsers exist , but they use different theories of discourse , e.g. , PDTB ( Lin et al. , 2010 ) , or RST ( duVerle and Prendinger , 2009 ; Feng and Hirst , 2012 ) .", "metadata": {"citing_string": "Lin et al. , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2121", "citing_paper": "W07-0737"}}
{"text": "Different formalisms have been proposed to model these assumptions in frameworks of coherence relations and discourse structure ( Mann and Thompson , 1988 ; Lascarides and Asher , 1993 ; Webber , 2004 ) .", "metadata": {"citing_string": "Mann and Thompson , 1988", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3149", "citing_paper": "W10-1902"}}
{"text": "Implicit relation classification In the early attempt by Marcu and Echihabi ( 2002 ) , implicit relation classification was grounded on synthetic training data ( relation patterns with explicit cue phrases removed ) and a Naive Bayes model trained on word-pair features .", "metadata": {"citing_string": "Marcu and Echihabi ( 2002 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0907", "citing_paper": "W09-3938"}}
{"text": "and NomBank ( Palmer et al. , 2005 ; Meyers et al. , 2004 ) .", "metadata": {"citing_string": "Meyers et al. , 2004", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2117", "citing_paper": "W10-1010"}}
{"text": "These were obtained by summing over all skip-gram neural word embeddings ( Mikolov et al. , 2013 ) present in each argument weighted by the respective number of elements ( embeddings ) found in each argument .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3346", "citing_paper": "W04-0405"}}
{"text": "and NomBank ( Palmer et al. , 2005 ; Meyers et al. , 2004 ) .", "metadata": {"citing_string": "Palmer et al. , 2005", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-1511", "citing_paper": "W12-1511"}}
{"text": "Aggregation over such word-pairs was described by Biran and McKeown ( 2013 ) , while Park and Cardie ( 2012 ) optimized feature sets through feature selection , preprocessing and special binning techniques .", "metadata": {"citing_string": "Park and Cardie ( 2012 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W04-2415"}}
{"text": "â¢ either employ additional metrics permitting partial matches , e.g. , using sliding-window metrics such as Pevzner and Hearst ( 2002 ) , â¢ or to ground argument definitions in psycholinguistically more plausible models of", "metadata": {"citing_string": "Pevzner and Hearst ( 2002 )", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3346", "citing_paper": "W13-2806"}}
{"text": "Pitler and Nenkova ( 2009 ) introduce a refinement using syntactic features to disambiguate explicit connectives which increases performance close to a human baseline .", "metadata": {"citing_string": "Pitler and Nenkova ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2253", "citing_paper": "W14-3411"}}
{"text": "This is still below state-of-the art ( 94 % in Pitler and Nenkova , 2009 ) 9 -- yet satisfying for our lightweight system with its original emphasis on implicit relations .", "metadata": {"citing_string": "Pitler and Nenkova , 2009", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0317", "citing_paper": "W14-1807"}}
{"text": "considerable interest : Pitler et al. ( 2009 ) present an extensive evaluation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment .", "metadata": {"citing_string": "Pitler et al. ( 2009 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1406", "citing_paper": "W06-0135"}}
{"text": "but using word stems ( Porter , 1980 ) instead .", "metadata": {"citing_string": "Porter , 1980", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-2021", "citing_paper": "W10-1916"}}
{"text": "With the release of the Penn Discourse Treebank ( Prasad et al. , 2008 , PDTB ) , annotated training data for SDP has become available and , as a consequence , the field has considerably attracted researchers from the NLP and IR community .", "metadata": {"citing_string": "Prasad et al. , 2008", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1902", "citing_paper": "W10-4229"}}
{"text": "Our approach is most similar to the one by Rutherford and Xue ( 2014 ) , who successfully integrate distributional representations to substitute word-pair features .", "metadata": {"citing_string": "Rutherford and Xue ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0825", "citing_paper": "W12-2021"}}
{"text": "but using a Brown cluster 3200 representation ( Turian et al. , 2010 ) for each word form if it exists .", "metadata": {"citing_string": "Turian et al. , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2107", "citing_paper": "W08-0308"}}
{"text": "Different formalisms have been proposed to model these assumptions in frameworks of coherence relations and discourse structure ( Mann and Thompson , 1988 ; Lascarides and Asher , 1993 ; Webber , 2004 ) .", "metadata": {"citing_string": "Webber , 2004", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-1614", "citing_paper": "W14-3304"}}
{"text": "Moreover , Zhou et al. ( 2010 ) use a language model to `` predict '' explicit connectives from implicit relations .", "metadata": {"citing_string": "Zhou et al. ( 2010 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-3815", "citing_paper": "W05-1208"}}
{"text": "It is noted that we tried indicator function ( alternatively called discrete-valued vector , bucket function ( Bansal et al. , 2014 ) , binarization of embeddings ( Guo et al. , 2014 ) ) for embeddings which are converted from real-valued vector .", "metadata": {"citing_string": "Bansal et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1420", "citing_paper": "W08-0317"}}
{"text": "Additionally we use word2vec ( Mikolov et al. , 2013b ) and Theano ( Bastien et al. , 2012 ) 11 in the pipeline .", "metadata": {"citing_string": "Bastien et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0419", "citing_paper": "W10-2704"}}
{"text": "Followed by Bordes et al. ( Bordes et al. , 2011 ; Bordes et al. , 2013 ) , we minimized a marginbased ranking criterion over the pair of embeddings :", "metadata": {"citing_string": "Bordes et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2415", "citing_paper": "W08-1137"}}
{"text": "Arg1 , Arg2 ( and Connectives ) , we used the relational phrase embedding from these triples ( or pairs ) based on their phrase embeddings ( Bordes et al. , 2013 ) .", "metadata": {"citing_string": "Bordes et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2117", "citing_paper": "W05-1617"}}
{"text": "The first type of embedding we used in this paper is a combination of paragraph vector ( Le and Mikolov , 2014 ) and translational embeddings ( Bordes et al. , 2013 ) .", "metadata": {"citing_string": "Bordes et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-3130", "citing_paper": "W90-0112"}}
{"text": "( Rutherford and Xue , 2015 ) further developed this method combined with Brown clustering ( Brown et al. , 1992 ) .", "metadata": {"citing_string": "Brown et al. , 1992", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3938", "citing_paper": "W10-3815"}}
{"text": "We use external tools such as libSVM ( Chang and Lin , 2011 ) , liblinear ( Fan et al. , 2008 ) , wapiti ( Lavergne et al. , 2010 ) , and maximum entropy model10 for a classification task described as Section 2 .", "metadata": {"citing_string": "Chang and Lin , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2209", "citing_paper": "W08-0907"}}
{"text": "This is by nature a multiple-instance learning setting ( Dietterich et al. , 1997 ) , which receives a set of instances which", "metadata": {"citing_string": "Dietterich et al. , 1997", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2040", "citing_paper": "W10-1010"}}
{"text": "We use external tools such as libSVM ( Chang and Lin , 2011 ) , liblinear ( Fan et al. , 2008 ) , wapiti ( Lavergne et al. , 2010 ) , and maximum entropy model10 for a classification task described as Section 2 .", "metadata": {"citing_string": "Fan et al. , 2008", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2139", "citing_paper": "W11-0402"}}
{"text": "It is noted that we tried indicator function ( alternatively called discrete-valued vector , bucket function ( Bansal et al. , 2014 ) , binarization of embeddings ( Guo et al. , 2014 ) ) for embeddings which are converted from real-valued vector .", "metadata": {"citing_string": "Guo et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-3734", "citing_paper": "W08-1137"}}
{"text": "We used a paragraph vector model to obtain these phrase embeddings ( Le and Mikolov , 2014 ) .", "metadata": {"citing_string": "Mikolov , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0825", "citing_paper": "W02-1104"}}
{"text": "We use a paragraph vector model to obtain the feature for Arg1 and Arg2 ( Le and Mikolov , 2014 ) .", "metadata": {"citing_string": "Mikolov , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2219", "citing_paper": "W04-2309"}}
{"text": "A paragraph vector is proven useful for the sentiment analysis-typed task ( Le and Mikolov , 2014 ) .", "metadata": {"citing_string": "Mikolov , 2014", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3130", "citing_paper": "W03-0409"}}
{"text": "If we plug in a RNN-LSTM model ( Le and Zuidema , 2015 ) , the model considers the effect of uni-gram within a sentence as a tree .", "metadata": {"citing_string": "Zuidema , 2015", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-1710", "citing_paper": "W11-1902"}}
{"text": "The first phase of the identification of connective and arguments are described in ( Wang et al. , 2015 ) , which bases on the framework of ( Lin et al. , 2009 ) and is also presented in this shared task as a different paper .", "metadata": {"citing_string": "Lin et al. , 2009", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2139", "citing_paper": "W13-1915"}}
{"text": "Production features are proposed in ( Lin et al. , 2014 ) and word-pair features are reported in ( Lin et al. , 2014 ; Rutherford and Xue , 2015 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3938", "citing_paper": "W08-1137"}}
{"text": "( Lin et al. , 2014 ) describes the method using the production features based on the parsing results .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4304", "citing_paper": "W08-1137"}}
{"text": "This feature is used in ( Lin et al. , 2014 ; Rutherford and Xue , 2015 ) .", "metadata": {"citing_string": "Lin et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3304", "citing_paper": "W13-1733"}}
{"text": "The paragraph vector model is an idea to obtain a real-valued vector in the similar construction with the word vector model ( or word2vec ) ( Mikolov et al. , 2013b ) where the detailed explanation can be obtained .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4148", "citing_paper": "W13-1733"}}
{"text": "For the dataset , we used the CoNLL 2015 Shared task data set , i.e. LDC2015E21 ( Xue et al. , 2015 ) and Skip-gram neural word embeddings ( Mikolov et al. , 2013a ) 8 ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W10-2704"}}
{"text": "Additionally we use word2vec ( Mikolov et al. , 2013b ) and Theano ( Bastien et al. , 2012 ) 11 in the pipeline .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3304", "citing_paper": "W06-1420"}}
{"text": "First , ( Mikolov et al. , 2013b ) observed a linear relation on two word embeddings .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2159", "citing_paper": "W06-3113"}}
{"text": "The paragraph vector model is an idea to obtain a real-valued vector in the similar construction with the word vector model ( or word2vec ) ( Mikolov et al. , 2013b ) where the detailed explanation can be obtained .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W07-0720", "citing_paper": "W11-1902"}}
{"text": "For the dataset , we used the CoNLL 2015 Shared task data set , i.e. LDC2015E21 ( Xue et al. , 2015 ) and Skip-gram neural word embeddings ( Mikolov et al. , 2013a ) 8 ) .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-3938", "citing_paper": "W13-1902"}}
{"text": "Additionally we use word2vec ( Mikolov et al. , 2013b ) and Theano ( Bastien et al. , 2012 ) 11 in the pipeline .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2304", "citing_paper": "W10-4148"}}
{"text": "First , ( Mikolov et al. , 2013b ) observed a linear relation on two word embeddings .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-1807", "citing_paper": "W08-0621"}}
{"text": "Followed by Pitler et al. ( Pitler et al. , 2008 ) we use the 100 frequent wordpairs in training set for each category of relation .", "metadata": {"citing_string": "Pitler et al. , 2008", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0604", "citing_paper": "W07-0737"}}
{"text": "Production features are proposed in ( Lin et al. , 2014 ) and word-pair features are reported in ( Lin et al. , 2014 ; Rutherford and Xue , 2015 ) .", "metadata": {"citing_string": "Rutherford and Xue , 2015", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3938", "citing_paper": "W03-2127"}}
{"text": "This feature is used in ( Lin et al. , 2014 ; Rutherford and Xue , 2015 ) .", "metadata": {"citing_string": "Rutherford and Xue , 2015", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W03-2127", "citing_paper": "W13-1915"}}
{"text": "The first phase of the identification of connective and arguments are described in ( Wang et al. , 2015 ) , which bases on the framework of ( Lin et al. , 2009 ) and is also presented in this shared task as a different paper .", "metadata": {"citing_string": "Wang et al. , 2015", "section_number": 2, "is_self_cite": true, "intent_label": "method", "cited_paper": "W14-3346", "citing_paper": "W07-0720"}}
{"text": "This paper describes the discourse parsing system developed at Dublin City University for participation in the CoNLL 2015 shared task ( Xue et al. , 2015 ) .", "metadata": {"citing_string": "Xue et al. , 2015", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2309", "citing_paper": "W11-2139"}}
{"text": "For the dataset , we used the CoNLL 2015 Shared task data set , i.e. LDC2015E21 ( Xue et al. , 2015 ) and Skip-gram neural word embeddings ( Mikolov et al. , 2013a ) 8 ) .", "metadata": {"citing_string": "Xue et al. , 2015", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2219", "citing_paper": "W11-2044"}}
{"text": "Nivre ( 2003 ) and Nivre ( 2004 ) establish the basis for arc-eager algorithm and arc-standard parsing algorithms , which are central to most recent transitionbased parsers ( Zhang and Clark , 2011b ; Zhang and Nivre , 2011 ; Bohnet and Nivre , 2012 ) .", "metadata": {"citing_string": "Bohnet and Nivre , 2012", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1617", "citing_paper": "W12-3152"}}
{"text": "In our case , we follow the representations proposed by Carreras et al. ( 2008 ) , which we call spinal trees .", "metadata": {"citing_string": "Carreras et al. ( 2008 )", "section_number": 2, "is_self_cite": true, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W03-2127"}}
{"text": "In this section we describe the spinal trees used by Carreras et al. ( 2008 ) .", "metadata": {"citing_string": "Carreras et al. ( 2008 )", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W10-4229", "citing_paper": "W11-2159"}}
{"text": "In practice , it is straightforward to obtain spinal trees from a treebank of constituent trees with head-child annotations in each constituent ( Carreras et al. , 2008 ) : starting from a token , its spine consists of the non-terminal labels of the constituents whose head is the token ; the parent node of the top of the spine gives information about the lexical head ( by following the head children of the parent ) and the position where the spine attaches to .", "metadata": {"citing_string": "Carreras et al. , 2008", "section_number": 3, "is_self_cite": true, "intent_label": "background", "cited_paper": "W12-0111", "citing_paper": "W14-3329"}}
{"text": "In the literature , these triplets have been shown to provide very rich parameterizations of statistical models for parsing ( Collins , 1996 ; Collins , 1999 ; Carreras et al. , 2008 ) .", "metadata": {"citing_string": "Carreras et al. , 2008", "section_number": 4, "is_self_cite": true, "intent_label": "background", "cited_paper": "W08-0317", "citing_paper": "W06-1420"}}
{"text": "5We use the same setting as in ( Carreras et al. , 2008 ) by training over a treebank with predicted part-of-speech tags with mxpost ( Ratnaparkhi , 1996 ) ( accuracy : 96.5 ) and we test on the development set and test set with predicted partof-speech tags of Collins ( 1997 ) ( accuracy : 96.8 ) .", "metadata": {"citing_string": "Carreras et al. , 2008", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W14-3329"}}
{"text": "However , unlike ( Carreras et al. , 2008 ) , the arc-eager parser does not substantially benefit of using the triplets during training .", "metadata": {"citing_string": "Carreras et al. , 2008", "section_number": 5, "is_self_cite": true, "intent_label": "background", "cited_paper": "W10-4304", "citing_paper": "W08-0907"}}
{"text": "Carreras et al. ( 2008 ) was the first to use spinal representations to define an arc-factored dependency parsing model based on the Eisner algorithm , that parses in cubic time .", "metadata": {"citing_string": "Carreras et al. ( 2008 )", "section_number": 6, "is_self_cite": true, "intent_label": "background", "cited_paper": "W12-3149", "citing_paper": "W04-0853"}}
{"text": "This approach was first explored by Collins ( 1996 ) , who defined a dependency-based probabilistic model that associates a triple of constituents with each dependency .", "metadata": {"citing_string": "Collins ( 1996 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W10-1916"}}
{"text": "Collins ( 1996 ) defined a statistical model for dependency parsing based on using constituent triplets in the labels , which forms the basis of our arc-eager model .", "metadata": {"citing_string": "Collins ( 1996 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2806", "citing_paper": "W10-4148"}}
{"text": "We follow Collins ( 1996 ) and define a labeling for dependencies based on constituent triplets .", "metadata": {"citing_string": "Collins ( 1996 )", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0409", "citing_paper": "W13-2806"}}
{"text": "In the literature , these triplets have been shown to provide very rich parameterizations of statistical models for parsing ( Collins , 1996 ; Collins , 1999 ; Carreras et al. , 2008 ) .", "metadata": {"citing_string": "Collins , 1996", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3329", "citing_paper": "W04-0843"}}
{"text": "Collins ( 1996 ) defined a statistical model for dependency parsing based on using constituent triplets in the labels , which forms the basis of our arc-eager model .", "metadata": {"citing_string": "Collins ( 1996 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-1111", "citing_paper": "W13-1902"}}
{"text": "5We use the same setting as in ( Carreras et al. , 2008 ) by training over a treebank with predicted part-of-speech tags with mxpost ( Ratnaparkhi , 1996 ) ( accuracy : 96.5 ) and we test on the development set and test set with predicted partof-speech tags of Collins ( 1997 ) ( accuracy : 96.8 ) .", "metadata": {"citing_string": "Collins ( 1997 )", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-3113", "citing_paper": "W11-1902"}}
{"text": "The recent literature has shown that such lexical features greatly favor the accuracy of statistical models for parsing ( Collins , 1999 ; Nivre , 2003 ; McDonald et al. , 2005 ) .", "metadata": {"citing_string": "Collins , 1999", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-1010", "citing_paper": "W11-0317"}}
{"text": "In the literature , these triplets have been shown to provide very rich parameterizations of statistical models for parsing ( Collins , 1996 ; Collins , 1999 ; Carreras et al. , 2008 ) .", "metadata": {"citing_string": "Collins , 1999", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W98-1419", "citing_paper": "W12-0111"}}
{"text": "Statistical models that use both representations jointly were pioneered by Collins ( 1999 ) , who used constituent trees annotated with head-child information in order to define lexicalized PCFG models , i.e. extensions of classic constituent-based PCFG that make a central use of lexical dependencies .", "metadata": {"citing_string": "Collins ( 1999 )", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2139", "citing_paper": "W03-0409"}}
{"text": "In the literature , stacking is a common technique to improve accuracies by combining dependency and constituent information , in both ways ( Wang and Zong , 2011 ; Farkas and Bohnet , 2012 ) .", "metadata": {"citing_string": "Farkas and Bohnet , 2012", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0317", "citing_paper": "W04-1106"}}
{"text": "In a similar style to our method Hall et al. ( 2007 ) , Hall and Nivre ( 2008 ) and Hall ( 2008 ) introduced an approach for parsing Swedish and German , in which MaltParser ( Nivre et al. , 2007 ) is used to predict dependency trees , whose dependency labels are enriched with constituency labels .", "metadata": {"citing_string": "Hall and Nivre ( 2008 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-3734", "citing_paper": "W11-1406"}}
{"text": "In a similar style to our method Hall et al. ( 2007 ) , Hall and Nivre ( 2008 ) and Hall ( 2008 ) introduced an approach for parsing Swedish and German , in which MaltParser ( Nivre et al. , 2007 ) is used to predict dependency trees , whose dependency labels are enriched with constituency labels .", "metadata": {"citing_string": "Hall et al. ( 2007 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2044", "citing_paper": "W13-2219"}}
{"text": "In a similar style to our method Hall et al. ( 2007 ) , Hall and Nivre ( 2008 ) and Hall ( 2008 ) introduced an approach for parsing Swedish and German , in which MaltParser ( Nivre et al. , 2007 ) is used to predict dependency trees , whose dependency labels are enriched with constituency labels .", "metadata": {"citing_string": "Hall ( 2008 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0120", "citing_paper": "W11-0809"}}
{"text": "Recently Kong et al. ( 2015 ) proposed a structured prediction model for mapping dependency trees to constituent trees , using the CKY algorithm .", "metadata": {"citing_string": "Kong et al. ( 2015 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W00-1403", "citing_paper": "W11-0402"}}
{"text": "The recent literature has shown that such lexical features greatly favor the accuracy of statistical models for parsing ( Collins , 1999 ; Nivre , 2003 ; McDonald et al. , 2005 ) .", "metadata": {"citing_string": "McDonald et al. , 2005", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3152", "citing_paper": "W11-2107"}}
{"text": "In a similar style to our method Hall et al. ( 2007 ) , Hall and Nivre ( 2008 ) and Hall ( 2008 ) introduced an approach for parsing Swedish and German , in which MaltParser ( Nivre et al. , 2007 ) is used to predict dependency trees , whose dependency labels are enriched with constituency labels .", "metadata": {"citing_string": "Nivre et al. , 2007", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1617", "citing_paper": "W04-2415"}}
{"text": "Each action can have constraints ( Nivre et al. , 2014 ) , Figure 2 and Section 3.2 describe the constraints of the spinal parser .", "metadata": {"citing_string": "Nivre et al. , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0907", "citing_paper": "W10-1902"}}
{"text": "The recent literature has shown that such lexical features greatly favor the accuracy of statistical models for parsing ( Collins , 1999 ; Nivre , 2003 ; McDonald et al. , 2005 ) .", "metadata": {"citing_string": "Nivre , 2003", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3130", "citing_paper": "W13-1715"}}
{"text": "In this paper we propose a transition-based parser for spinal parsing , based on the arc-eager strategy by Nivre ( 2003 ) .", "metadata": {"citing_string": "Nivre ( 2003 )", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2209", "citing_paper": "W06-1403"}}
{"text": "The arc-eager transition-based parser ( Nivre , 2003 ) parses a sentence from left to right in linear time .", "metadata": {"citing_string": "Nivre , 2003", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2148", "citing_paper": "W11-2032"}}
{"text": "The constraints that are not labeled are standard constraints of the arc-eager parsing algorithm ( Nivre , 2003 ) .", "metadata": {"citing_string": "Nivre , 2003", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2117", "citing_paper": "W10-4148"}}
{"text": "Nivre ( 2003 ) and Nivre ( 2004 ) establish the basis for arc-eager algorithm and arc-standard parsing algorithms , which are central to most recent transitionbased parsers ( Zhang and Clark , 2011b ; Zhang and Nivre , 2011 ; Bohnet and Nivre , 2012 ) .", "metadata": {"citing_string": "Nivre ( 2003 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0120", "citing_paper": "W11-2139"}}
{"text": "Nivre ( 2003 ) and Nivre ( 2004 ) establish the basis for arc-eager algorithm and arc-standard parsing algorithms , which are central to most recent transitionbased parsers ( Zhang and Clark , 2011b ; Zhang and Nivre , 2011 ; Bohnet and Nivre , 2012 ) .", "metadata": {"citing_string": "Nivre ( 2004 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4148", "citing_paper": "W06-1420"}}
{"text": "5We use the same setting as in ( Carreras et al. , 2008 ) by training over a treebank with predicted part-of-speech tags with mxpost ( Ratnaparkhi , 1996 ) ( accuracy : 96.5 ) and we test on the development set and test set with predicted partof-speech tags of Collins ( 1997 ) ( accuracy : 96.8 ) .", "metadata": {"citing_string": "Ratnaparkhi , 1996", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-0317", "citing_paper": "W01-1615"}}
{"text": "In terms of parsing spinal structures , Rush et al. ( 2010 ) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure .", "metadata": {"citing_string": "Rush et al. ( 2010 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1915", "citing_paper": "W14-2410"}}
{"text": "For shift-reduce constituent parsing , Sagae and Lavie ( 2005 ; 2006 ) presented a shift-reduce phrase structure parser .", "metadata": {"citing_string": "Sagae and Lavie ( 2005", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-3130", "citing_paper": "W12-3130"}}
{"text": "In the literature , stacking is a common technique to improve accuracies by combining dependency and constituent information , in both ways ( Wang and Zong , 2011 ; Farkas and Bohnet , 2012 ) .", "metadata": {"citing_string": "Wang and Zong , 2011", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0720", "citing_paper": "W11-2139"}}
{"text": "We use the WSJ portion of the Penn Treebank4 , augmented with head-dependant information using the rules of Yamada and Matsumoto ( 2003 ) .", "metadata": {"citing_string": "Yamada and Matsumoto ( 2003 )", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W01-0725", "citing_paper": "W06-3113"}}
{"text": "As we can see , the parser also provides a good trade-off between parsing speed and accuracy .6 In order to test whether the number of dependency labels is an issue for the parser , we also trained a model on dependency trees labeled with Yamada and Matsumoto ( 2003 ) rules , and the results are comparable to ours .", "metadata": {"citing_string": "Yamada and Matsumoto ( 2003 )", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W12-2021", "citing_paper": "W98-1419"}}
{"text": "In this paper , we took the already existent implementation of arc-eager from ZPar1 ( Zhang and Clark , 2009 ) which is a beam-search parser implemented in C++ focused on efficiency .", "metadata": {"citing_string": "Zhang and Clark , 2009", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1403", "citing_paper": "W12-3130"}}
{"text": "More recently , Zhang and Clark ( 2009 ) and the subsequent work of Zhu et al. ( 2013 ) described a beam-search shift-reduce parsers obtaining very high results .", "metadata": {"citing_string": "Zhang and Clark ( 2009 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-1902", "citing_paper": "W13-2253"}}
{"text": "More recently , Zhang and Clark ( 2009 ) and the subsequent work of Zhu et al. ( 2013 ) described a beam-search shift-reduce parsers obtaining very high results .", "metadata": {"citing_string": "Zhang and Clark ( 2009 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-1304", "citing_paper": "W08-0317"}}
{"text": "To control the trade-off , we use beam search for transition-based parsing , which has been shown to be successful ( Zhang and Clark , 2011b ) .", "metadata": {"citing_string": "Zhang and Clark , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-2231", "citing_paper": "W06-0102"}}
{"text": "Nivre ( 2003 ) and Nivre ( 2004 ) establish the basis for arc-eager algorithm and arc-standard parsing algorithms , which are central to most recent transitionbased parsers ( Zhang and Clark , 2011b ; Zhang and Nivre , 2011 ; Bohnet and Nivre , 2012 ) .", "metadata": {"citing_string": "Zhang and Clark , 2011", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2138", "citing_paper": "W11-2040"}}
{"text": "To control the trade-off , we use beam search for transition-based parsing , which has been shown to be successful ( Zhang and Clark , 2011b ) .", "metadata": {"citing_string": "Zhang and Clark , 2011", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4304", "citing_paper": "W13-1733"}}
{"text": "Nivre ( 2003 ) and Nivre ( 2004 ) establish the basis for arc-eager algorithm and arc-standard parsing algorithms , which are central to most recent transitionbased parsers ( Zhang and Clark , 2011b ; Zhang and Nivre , 2011 ; Bohnet and Nivre , 2012 ) .", "metadata": {"citing_string": "Zhang and Clark , 2011", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W05-1302", "citing_paper": "W13-2806"}}
{"text": "We used the exact same features as Zhang and Nivre ( 2011 ) , which extract a rich set of features that encode higherorder interactions betwen the current action and elements of the stack .", "metadata": {"citing_string": "Zhang and Nivre ( 2011 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0120", "citing_paper": "W09-0419"}}
{"text": "We used the exact same features as Zhang and Nivre ( 2011 ) , which extract a rich set of features that encode higherorder interactions betwen the current action and elements of the stack .", "metadata": {"citing_string": "Zhang and Nivre ( 2011 )", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-4304", "citing_paper": "W14-1807"}}
{"text": "Nivre ( 2003 ) and Nivre ( 2004 ) establish the basis for arc-eager algorithm and arc-standard parsing algorithms , which are central to most recent transitionbased parsers ( Zhang and Clark , 2011b ; Zhang and Nivre , 2011 ; Bohnet and Nivre , 2012 ) .", "metadata": {"citing_string": "Zhang and Nivre , 2011", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W98-1419", "citing_paper": "W11-2139"}}
{"text": "More recently , Zhang and Clark ( 2009 ) and the subsequent work of Zhu et al. ( 2013 ) described a beam-search shift-reduce parsers obtaining very high results .", "metadata": {"citing_string": "Zhu et al. ( 2013 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-0111", "citing_paper": "W06-0135"}}
{"text": "More recently , Zhang and Clark ( 2009 ) and the subsequent work of Zhu et al. ( 2013 ) described a beam-search shift-reduce parsers obtaining very high results .", "metadata": {"citing_string": "Zhu et al. ( 2013 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0853", "citing_paper": "W10-1007"}}
{"text": "A number of domain adaptation techniques are based on learning common feature representation ( Pan and Yang , 2010b ; Blitzer et al. , 2006 ; Ji et al. , 2011 ; Daum Â´ e III , 2009 ) for text classification .", "metadata": {"citing_string": "Blitzer et al. , 2006", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-1420", "citing_paper": "W14-0201"}}
{"text": "One of the widely used approaches is Structural Correspondence Learning ( SCL ) ( Blitzer et al. , 2006 ) which aims to learn the co-occurrence between features expressing similar meaning in different domains .", "metadata": {"citing_string": "Blitzer et al. , 2006", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2044", "citing_paper": "W08-0317"}}
{"text": "Among them , Structural Correspondence Learning ( SCL ) ( Blitzer et al. , 2007 ) is the most representative one , explained later .", "metadata": {"citing_string": "Blitzer et al. , 2007", "section_number": 3, "is_self_cite": false, "intent_label": "result", "cited_paper": "W06-3113", "citing_paper": "W10-1007"}}
{"text": "There have been several works towards feature-representation-transfer approach such as ( Blitzer et al. , 2007 ; Ji et al. , 2011 ) which derives a transformation matrix Q that gives a shared representation between the source and target domains .", "metadata": {"citing_string": "Blitzer et al. , 2007", "section_number": 4, "is_self_cite": false, "intent_label": "result", "cited_paper": "W10-4148", "citing_paper": "W06-1420"}}
{"text": "Domain Similarity-based Aggregation : Performance of domain adaptation is often constrained by the dissimilarity between the source and target domains ( Luo et al. , 2012 ; Rosenstein et al. , 2005 ; Chin , 2013 ; Blitzer et al. , 2007 ) .", "metadata": {"citing_string": "Blitzer et al. , 2007", "section_number": 4, "is_self_cite": false, "intent_label": "result", "cited_paper": "W13-1733", "citing_paper": "W08-1137"}}
{"text": "The efficacy of the proposed algorithm is evaluated on different datasets for cross-domain text classification ( Blitzer et al. , 2007 ) , ( Dai et al. , 2007 ) .", "metadata": {"citing_string": "Blitzer et al. , 2007", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W11-2032", "citing_paper": "W03-2127"}}
{"text": "As compared to other existing domain adaptation approaches like SCL ( Blitzer et al. , 2007 ) and CoCC ( Dai et al. , 2007 ) , the proposed algorithm outperforms by at least 4 % and 1.9 % respectively .", "metadata": {"citing_string": "Blitzer et al. , 2007", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W11-0809", "citing_paper": "W02-1610"}}
{"text": "Bollegala et al. ( 2011 ) used sentiment sensitive thesaurus to expand features for cross-domain sentiment classification .", "metadata": {"citing_string": "Bollegala et al. ( 2011 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-3605", "citing_paper": "W00-1403"}}
{"text": "In a comprehensive evaluation study , it was observed that their approach tends to increase the adaptation performance when multiple source domains were used ( Bollegala et al. , 2013 ) .", "metadata": {"citing_string": "Bollegala et al. , 2013", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-1511", "citing_paper": "W10-1902"}}
{"text": "1We also compared our performance with sentiment sensitive thesaurus ( SST ) proposed by ( Bollegala et al. , 2013 ) and our algorithm outperformed on our protocol .", "metadata": {"citing_string": "Bollegala et al. , 2013", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2107", "citing_paper": "W08-0907"}}
{"text": "Domain adaptation based on iterative learning has been explored by Chen et al. ( 2011 ) and Garcia-Fernandez et al. ( 2014 ) and are similar to the philosophy of the proposed approach in appending pseudo-labeled test data to the training set .", "metadata": {"citing_string": "Chen et al. ( 2011 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-2021", "citing_paper": "W07-0720"}}
{"text": "A similar approach , based on co-clustering ( Dhillon et al. , 2003 ) , was proposed in Dai et al. ( 2007 ) to leverage common words as bridge between two domains .", "metadata": {"citing_string": "Dai et al. ( 2007 )", "section_number": 3, "is_self_cite": false, "intent_label": "result", "cited_paper": "W04-1106", "citing_paper": "W08-2117"}}
{"text": "The efficacy of the proposed algorithm is evaluated on different datasets for cross-domain text classification ( Blitzer et al. , 2007 ) , ( Dai et al. , 2007 ) .", "metadata": {"citing_string": "Dai et al. , 2007", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W04-0843", "citing_paper": "W10-1007"}}
{"text": "As compared to other existing domain adaptation approaches like SCL ( Blitzer et al. , 2007 ) and CoCC ( Dai et al. , 2007 ) , the proposed algorithm outperforms by at least 4 % and 1.9 % respectively .", "metadata": {"citing_string": "Dai et al. , 2007", "section_number": 5, "is_self_cite": false, "intent_label": "result", "cited_paper": "W11-2134", "citing_paper": "W13-1902"}}
{"text": "A similar approach , based on co-clustering ( Dhillon et al. , 2003 ) , was proposed in Dai et al. ( 2007 ) to leverage common words as bridge between two domains .", "metadata": {"citing_string": "Dhillon et al. , 2003", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0604", "citing_paper": "W14-3411"}}
{"text": "Domain adaptation based on iterative learning has been explored by Chen et al. ( 2011 ) and Garcia-Fernandez et al. ( 2014 ) and are similar to the philosophy of the proposed approach in appending pseudo-labeled test data to the training set .", "metadata": {"citing_string": "Garcia-Fernandez et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W07-0720", "citing_paper": "W13-3605"}}
{"text": "Confidence of prediction , Î±z for ith instance , is measured as the distance from the decision boundary ( Hsu et al. , 2003 ) which is computed as shown in Eq .", "metadata": {"citing_string": "Hsu et al. , 2003", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-0135", "citing_paper": "W02-1610"}}
{"text": "01 and 02 are set empirically on a held-out set , with values ranging from zero to distance of farthest classified instance from the SVM hyperplane ( Hsu et al. , 2003 ) .", "metadata": {"citing_string": "Hsu et al. , 2003", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-2410", "citing_paper": "W98-1419"}}
{"text": "A number of domain adaptation techniques are based on learning common feature representation ( Pan and Yang , 2010b ; Blitzer et al. , 2006 ; Ji et al. , 2011 ; Daum Â´ e III , 2009 ) for text classification .", "metadata": {"citing_string": "Ji et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0402", "citing_paper": "W98-1419"}}
{"text": "There have been several works towards feature-representation-transfer approach such as ( Blitzer et al. , 2007 ; Ji et al. , 2011 ) which derives a transformation matrix Q that gives a shared representation between the source and target domains .", "metadata": {"citing_string": "Ji et al. , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W90-0112", "citing_paper": "W08-1137"}}
{"text": "Apart from using the SCL representation , the accuracy is compared with the proposed algorithm using two other representations , 1 ) common features between the two domains ( `` common '' ) and 2 ) multiview principal component analysis based representation ( `` MVPCA '' ) ( Ji et al. , 2011 ) as they are previously used for cross-domain sentiment classification on the same dataset .", "metadata": {"citing_string": "Ji et al. , 2011", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-3149", "citing_paper": "W11-2107"}}
{"text": "Jiang and Zhai ( 2007 ) proposed instance weighing scheme for domain adaptation in NLP tasks which exploit independence between feature mapping and instance weighing approaches .", "metadata": {"citing_string": "Jiang and Zhai ( 2007 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2139", "citing_paper": "W10-1916"}}
{"text": "The second dataset is the 20 Newsgroups dataset ( Lang , 1995 ) which is a text collection of approximately 20 , 000 documents evenly partitioned across 20 newsgroups .", "metadata": {"citing_string": "Lang , 1995", "section_number": 5, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2209", "citing_paper": "W14-1807"}}
{"text": "Towards this direction , Liao et al. ( 2005 ) learned mismatch between two domains and used active learning to select instances from the source domain to enhance adaptability of the classifier .", "metadata": {"citing_string": "Liao et al. ( 2005 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3346", "citing_paper": "W12-3130"}}
{"text": "Domain Similarity-based Aggregation : Performance of domain adaptation is often constrained by the dissimilarity between the source and target domains ( Luo et al. , 2012 ; Rosenstein et al. , 2005 ; Chin , 2013 ; Blitzer et al. , 2007 ) .", "metadata": {"citing_string": "Luo et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1010", "citing_paper": "W12-3130"}}
{"text": "While transfer learning has generally proved useful in reducing the labelled data requirement , brute force techniques suffer from the problem of negative transfer ( Pan and Yang , 2010a ) .", "metadata": {"citing_string": "Pan and Yang , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W04-2309"}}
{"text": "Transfer learning in text analysis ( domain adaptation ) has shown promising results in recent years ( Pan and Yang , 2010a ) .", "metadata": {"citing_string": "Pan and Yang , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W06-3113", "citing_paper": "W09-0604"}}
{"text": "While transfer learning has generally proved useful in reducing the labelled data requirement , brute force techniques suffer from the problem of negative transfer ( Pan and Yang , 2010a ) .", "metadata": {"citing_string": "Pan and Yang , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0317", "citing_paper": "W13-1902"}}
{"text": "Transfer learning in text analysis ( domain adaptation ) has shown promising results in recent years ( Pan and Yang , 2010a ) .", "metadata": {"citing_string": "Pan and Yang , 2010", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-2704", "citing_paper": "W10-1010"}}
{"text": "Pan et al. ( 2008 ) proposed a dimensionality reduction method Maximum Mean Discrepancy Embedding to identify a latent space .", "metadata": {"citing_string": "Pan et al. ( 2008 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-1106", "citing_paper": "W12-3130"}}
{"text": "Subsequently , Pan et al. ( 2010 ) proposed to map domain specific words into unified clusters using spectral clustering algorithm .", "metadata": {"citing_string": "Pan et al. ( 2010 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3304", "citing_paper": "W13-2253"}}
{"text": "1 ) Effect of learning target specific features : Results in Figure 3 show that iteratively learning target specific feature representation ( slow transfer as opposed to one-shot transfer ) yields better performance across different cross-domain classification tasks as compared to SCL , SFA ( Pan et al. , 2010 ) 4 and the baseline .", "metadata": {"citing_string": "Pan et al. , 2010", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2148", "citing_paper": "W11-2107"}}
{"text": "In another follow up work , Pan et al. ( 2011 ) proposed a novel feature representation to perform domain adaptation via Reproducing Kernel Hilbert Space using Maximum Mean Discrepancy .", "metadata": {"citing_string": "Pan et al. ( 2011 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-2209", "citing_paper": "W13-1715"}}
{"text": "Domain Similarity-based Aggregation : Performance of domain adaptation is often constrained by the dissimilarity between the source and target domains ( Luo et al. , 2012 ; Rosenstein et al. , 2005 ; Chin , 2013 ; Blitzer et al. , 2007 ) .", "metadata": {"citing_string": "Rosenstein et al. , 2005", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0621", "citing_paper": "W10-3815"}}
{"text": "Xia et al. ( 2013 ) proposed a hybrid method for sentiment classification task that also addresses the challenge of mutually opposite orientation words .", "metadata": {"citing_string": "Xia et al. ( 2013 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1208", "citing_paper": "W11-2139"}}
{"text": "While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable ( Turian et al. , 2010 ) , more recent studies have focused on how to tune and enhance word embeddings for specific tasks ( Bansal et al. , 2014 ; Boros et al. , 2014 ; Chen et al. , 2014 ; Guo et al. , 2014 ; Nguyen and Grishman , 2014 ) and we continue this line of research for the task of relation classification .", "metadata": {"citing_string": "Bansal et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-2127", "citing_paper": "W04-1106"}}
{"text": "For dependency parsing , Bansal et al. ( 2014 ) and Chen et al. ( 2014 ) found ways to improve performance by integrating dependencybased context information into their embeddings .", "metadata": {"citing_string": "Bansal et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-0843", "citing_paper": "W03-0312"}}
{"text": "As discussed in Bansal et al. ( 2014 ) , the small context size captures the syntactic similarity between words rather than the topical similarity .", "metadata": {"citing_string": "Bansal et al. ( 2014 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-2134", "citing_paper": "W12-3152"}}
{"text": "This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag ( Baroni and Zamparelli , 2010 ; Grefenstette and Sadrzadeh , 2011 ; Hashimoto et al. , 2013 ; Hashimoto et al. , 2014 ; Kartsaklis and Sadrzadeh , 2013 ) .", "metadata": {"citing_string": "Baroni and Zamparelli , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2304", "citing_paper": "W06-1403"}}
{"text": "For syntactic parse trees , the paths between the target entities on constituency and dependency trees have been demonstrated to be useful ( Bunescu and Mooney , 2005 ; Zhang et al. , 2006 ) .", "metadata": {"citing_string": "Bunescu and Mooney , 2005", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-2309", "citing_paper": "W11-1902"}}
{"text": "Among them , we use dependency path features ( Bunescu and Mooney , 2005 ) based on the untyped binary dependencies of the Stanford parser to find the shortest path between target nouns .", "metadata": {"citing_string": "Bunescu and Mooney , 2005", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3346", "citing_paper": "W11-0402"}}
{"text": "While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable ( Turian et al. , 2010 ) , more recent studies have focused on how to tune and enhance word embeddings for specific tasks ( Bansal et al. , 2014 ; Boros et al. , 2014 ; Chen et al. , 2014 ; Guo et al. , 2014 ; Nguyen and Grishman , 2014 ) and we continue this line of research for the task of relation classification .", "metadata": {"citing_string": "Chen et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1106", "citing_paper": "W10-1007"}}
{"text": "For dependency parsing , Bansal et al. ( 2014 ) and Chen et al. ( 2014 ) found ways to improve performance by integrating dependencybased context information into their embeddings .", "metadata": {"citing_string": "Chen et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W03-0312", "citing_paper": "W11-2134"}}
{"text": "The features come from NE tags and WordNet hypernym tags of target nouns provided by a sense tagger ( Ciaramita and Altun , 2006 ) .", "metadata": {"citing_string": "Ciaramita and Altun , 2006", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W08-2231", "citing_paper": "W08-2231"}}
{"text": "Recently , word embeddings have become popular as an alternative to hand-crafted features ( Collobert et al. , 2011 ) .", "metadata": {"citing_string": "Collobert et al. , 2011", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1218", "citing_paper": "W14-3346"}}
{"text": "More recently , dos Santos et al. ( 2015 ) have introduced CR-CNN by extending the CNN model and achieved the best result to date .", "metadata": {"citing_string": "Santos et al. ( 2015 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W00-1220", "citing_paper": "W11-1902"}}
{"text": "dos Santos et al. ( 2015 ) also boosted the score of CR-CNNOther by omitting the noisy class `` Other '' by a rankingbased classifier , and achieved the best score ( CRCNNBeA ) .", "metadata": {"citing_string": "Santos et al. ( 2015 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0308", "citing_paper": "W11-0809"}}
{"text": "Î¸ = ( N , W , ËW , S , s ) is the set of parameters and Jlabeled is maximized using AdaGrad ( Duchi et al. , 2011 ) .", "metadata": {"citing_string": "Duchi et al. , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3411", "citing_paper": "W04-0843"}}
{"text": "Subsequently , Ebrahimi and Dou ( 2015 ) and Hashimoto et al. ( 2013 ) proposed RNN models to better handle the relations .", "metadata": {"citing_string": "Ebrahimi and Dou ( 2015 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-2415", "citing_paper": "W02-1610"}}
{"text": "We evaluated the learned embeddings using a word-level semantic evaluation task called WordSim-353 ( Finkelstein et al. , 2001 ) .", "metadata": {"citing_string": "Finkelstein et al. , 2001", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2148", "citing_paper": "W12-3152"}}
{"text": "Automatic classification of semantic relations has a variety of applications , such as information extraction and the construction of semantic networks ( Girju et al. , 2007 ; Hendrickx et al. , 2010 ) .", "metadata": {"citing_string": "Girju et al. , 2007", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-2704", "citing_paper": "W01-0725"}}
{"text": "This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag ( Baroni and Zamparelli , 2010 ; Grefenstette and Sadrzadeh , 2011 ; Hashimoto et al. , 2013 ; Hashimoto et al. , 2014 ; Kartsaklis and Sadrzadeh , 2013 ) .", "metadata": {"citing_string": "Grefenstette and Sadrzadeh , 2011", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W11-0809", "citing_paper": "W04-2309"}}
{"text": "While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable ( Turian et al. , 2010 ) , more recent studies have focused on how to tune and enhance word embeddings for specific tasks ( Bansal et al. , 2014 ; Boros et al. , 2014 ; Chen et al. , 2014 ; Guo et al. , 2014 ; Nguyen and Grishman , 2014 ) and we continue this line of research for the task of relation classification .", "metadata": {"citing_string": "Guo et al. , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W04-1106", "citing_paper": "W03-1710"}}
{"text": "This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag ( Baroni and Zamparelli , 2010 ; Grefenstette and Sadrzadeh , 2011 ; Hashimoto et al. , 2013 ; Hashimoto et al. , 2014 ; Kartsaklis and Sadrzadeh , 2013 ) .", "metadata": {"citing_string": "Hashimoto et al. , 2013", "section_number": 4, "is_self_cite": true, "intent_label": "background", "cited_paper": "W08-0317", "citing_paper": "W05-1617"}}
{"text": "Subsequently , Ebrahimi and Dou ( 2015 ) and Hashimoto et al. ( 2013 ) proposed RNN models to better handle the relations .", "metadata": {"citing_string": "Hashimoto et al. ( 2013 )", "section_number": 6, "is_self_cite": true, "intent_label": "background", "cited_paper": "W09-1111", "citing_paper": "W09-0604"}}
{"text": "This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag ( Baroni and Zamparelli , 2010 ; Grefenstette and Sadrzadeh , 2011 ; Hashimoto et al. , 2013 ; Hashimoto et al. , 2014 ; Kartsaklis and Sadrzadeh , 2013 ) .", "metadata": {"citing_string": "Hashimoto et al. , 2013", "section_number": 4, "is_self_cite": true, "intent_label": "background", "cited_paper": "W10-1007", "citing_paper": "W06-0135"}}
{"text": "This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag ( Baroni and Zamparelli , 2010 ; Grefenstette and Sadrzadeh , 2011 ; Hashimoto et al. , 2013 ; Hashimoto et al. , 2014 ; Kartsaklis and Sadrzadeh , 2013 ) .", "metadata": {"citing_string": "Hashimoto et al. , 2014", "section_number": 4, "is_self_cite": true, "intent_label": "background", "cited_paper": "W10-1010", "citing_paper": "W13-2219"}}
{"text": "Recently dropout has been applied to deep neural network models for natural language processing tasks and proven effective ( Irsoy and Cardie , 2014 ; Paulus et al. , 2014 ) .", "metadata": {"citing_string": "Irsoy and Cardie , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2040", "citing_paper": "W10-3815"}}
{"text": "This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag ( Baroni and Zamparelli , 2010 ; Grefenstette and Sadrzadeh , 2011 ; Hashimoto et al. , 2013 ; Hashimoto et al. , 2014 ; Kartsaklis and Sadrzadeh , 2013 ) .", "metadata": {"citing_string": "Kartsaklis and Sadrzadeh , 2013", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W14-3346", "citing_paper": "W03-0409"}}
{"text": "Le and Mikolov ( 2014 ) integrated paragraph information into a word2vec-based model , which allowed them to capture paragraph-level information .", "metadata": {"citing_string": "Mikolov ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0317", "citing_paper": "W12-1511"}}
{"text": "Although word2vec has successfully been used to learn word embeddings , these kinds of word embeddings capture only co-occurrence relationships between words ( Levy and Goldberg , 2014 ) .", "metadata": {"citing_string": "Levy and Goldberg , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1916", "citing_paper": "W13-2209"}}
{"text": "However , one of the limitations is that word embeddings are usually learned by predicting a target word in its context , leading to only local co-occurrence information being captured ( Levy and Goldberg , 2014 ) .", "metadata": {"citing_string": "Levy and Goldberg , 2014", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W09-3734", "citing_paper": "W13-1733"}}
{"text": "For example , word2vec ' ( Mikolov et al. , 2013b ) is a well-established tool for learning word embeddings .", "metadata": {"citing_string": "Mikolov et al. , 2013", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W10-4148", "citing_paper": "W11-1406"}}
{"text": "When training we employ several procedures introduced by Mikolov et al. ( 2013b ) , namely , negative sampling , a modified unigram noise distribution and subsampling .", "metadata": {"citing_string": "Mikolov et al. ( 2013", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2159", "citing_paper": "W12-3152"}}
{"text": "The learning rate was set to Î± and linearly decreased to 0 during training , as described in Mikolov et al. ( 2013a ) .", "metadata": {"citing_string": "Mikolov et al. ( 2013", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2040", "citing_paper": "W13-3605"}}
{"text": "we extracted 80 million sentences from the original Wikipedia file , and then used Enju3 ( Miyao and Tsujii , 2008 ) to automatically assign part-ofspeech ( POS ) tags .", "metadata": {"citing_string": "Miyao and Tsujii , 2008", "section_number": 5, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-1511", "citing_paper": "W12-1511"}}
{"text": "Mnih and Kavukcuoglu ( 2013 ) have demonstrated that using embeddings like those in W is useful in representing the words .", "metadata": {"citing_string": "Mnih and Kavukcuoglu ( 2013 )", "section_number": 4, "is_self_cite": false, "intent_label": "background", "cited_paper": "W13-4068", "citing_paper": "W11-2159"}}
{"text": "While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable ( Turian et al. , 2010 ) , more recent studies have focused on how to tune and enhance word embeddings for specific tasks ( Bansal et al. , 2014 ; Boros et al. , 2014 ; Chen et al. , 2014 ; Guo et al. , 2014 ; Nguyen and Grishman , 2014 ) and we continue this line of research for the task of relation classification .", "metadata": {"citing_string": "Nguyen and Grishman , 2014", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W04-0405", "citing_paper": "W07-0720"}}
{"text": "For information extraction , Boros et al. ( 2014 ) trained word embeddings relevant for event role extraction , and Nguyen and Grishman ( 2014 ) employed word embeddings for domain adaptation of relation extraction .", "metadata": {"citing_string": "Nguyen and Grishman ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-0402", "citing_paper": "W14-1614"}}
{"text": "We calculated a confidence interval ( 82.0 , 84.9 ) ( p < 0.05 ) using bootstrap resampling ( Noreen , 1989 ) .", "metadata": {"citing_string": "Noreen , 1989", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W05-1302", "citing_paper": "W11-2121"}}
{"text": "Recently dropout has been applied to deep neural network models for natural language processing tasks and proven effective ( Irsoy and Cardie , 2014 ; Paulus et al. , 2014 ) .", "metadata": {"citing_string": "Paulus et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2806", "citing_paper": "W11-2032"}}
{"text": "Carefully crafted features derived from lexical , syntactic , and semantic resources play a significant role in achieving high accuracy for semantic relation classification ( Rink and Harabagiu , 2010 ) .", "metadata": {"citing_string": "Rink and Harabagiu , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "method", "cited_paper": "W14-3329", "citing_paper": "W00-1403"}}
{"text": "On the shared task introduced by Hendrickx et al. ( 2010 ) , Rink and Harabagiu ( 2010 ) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine ( SVM ) .", "metadata": {"citing_string": "Rink and Harabagiu ( 2010 )", "section_number": 3, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-0102", "citing_paper": "W00-1412"}}
{"text": "As shown by Rink and Harabagiu ( 2010 ) , the words between the noun pairs are the most effective among these features .", "metadata": {"citing_string": "Rink and Harabagiu ( 2010 )", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-1902", "citing_paper": "W02-1104"}}
{"text": "The dependency path features are computed by averaging word embeddings from W on the shortest path , and are then concatenated to the feature vector e. Furthermore , we directly incorporate semantic information using word-level semantic features from Named Entity ( NE ) tags and WordNet hypernyms , as used in previous work ( Rink and Harabagiu , 2010 ; Socher et al. , 2012 ; Yu et al. , 2014 ) .", "metadata": {"citing_string": "Rink and Harabagiu , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W11-2148", "citing_paper": "W13-2253"}}
{"text": "The dependency path features are computed by averaging word embeddings from W on the shortest path , and are then concatenated to the feature vector e. Furthermore , we directly incorporate semantic information using word-level semantic features from Named Entity ( NE ) tags and WordNet hypernyms , as used in previous work ( Rink and Harabagiu , 2010 ; Socher et al. , 2012 ; Yu et al. , 2014 ) .", "metadata": {"citing_string": "Rink and Harabagiu , 2010", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W09-0419", "citing_paper": "W08-0308"}}
{"text": "RelEmb also outperforms the SVM system of Rink and Harabagiu ( 2010 ) , which demonstrates the effectiveness of our task-specific word embeddings , despite our only requirement being a large unannotated corpus and a POS tagger .", "metadata": {"citing_string": "Rink and Harabagiu ( 2010 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W90-0112", "citing_paper": "W10-4304"}}
{"text": "The dependency path features are computed by averaging word embeddings from W on the shortest path , and are then concatenated to the feature vector e. Furthermore , we directly incorporate semantic information using word-level semantic features from Named Entity ( NE ) tags and WordNet hypernyms , as used in previous work ( Rink and Harabagiu , 2010 ; Socher et al. , 2012 ; Yu et al. , 2014 ) .", "metadata": {"citing_string": "Socher et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-2253", "citing_paper": "W11-2040"}}
{"text": "Socher et al. ( 2012 ) used Recursive Neural Network ( RNN ) models to classify the relations .", "metadata": {"citing_string": "Socher et al. ( 2012 )", "section_number": 6, "is_self_cite": false, "intent_label": "method", "cited_paper": "W06-0102", "citing_paper": "W14-1807"}}
{"text": "The dependency path features are computed by averaging word embeddings from W on the shortest path , and are then concatenated to the feature vector e. Furthermore , we directly incorporate semantic information using word-level semantic features from Named Entity ( NE ) tags and WordNet hypernyms , as used in previous work ( Rink and Harabagiu , 2010 ; Socher et al. , 2012 ; Yu et al. , 2014 ) .", "metadata": {"citing_string": "Socher et al. , 2012", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W12-3152", "citing_paper": "W05-1302"}}
{"text": "Another kind of task-specific word embeddings was proposed by Tang et al. ( 2014 ) , which used sentiment labels on tweets to adapt word embeddings for a sentiment analysis tasks .", "metadata": {"citing_string": "Tang et al. ( 2014 )", "section_number": 3, "is_self_cite": false, "intent_label": "background", "cited_paper": "W10-1916", "citing_paper": "W10-4229"}}
{"text": "While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable ( Turian et al. , 2010 ) , more recent studies have focused on how to tune and enhance word embeddings for specific tasks ( Bansal et al. , 2014 ; Boros et al. , 2014 ; Chen et al. , 2014 ; Guo et al. , 2014 ; Nguyen and Grishman , 2014 ) and we continue this line of research for the task of relation classification .", "metadata": {"citing_string": "Turian et al. , 2010", "section_number": 2, "is_self_cite": false, "intent_label": "background", "cited_paper": "W08-0317", "citing_paper": "W10-4148"}}
{"text": "As was noted by Wang and Manning ( 2012 ) , we should carefully implement strong baselines and see whether complex models can outperform these baselines .", "metadata": {"citing_string": "Wang and Manning ( 2012 )", "section_number": 6, "is_self_cite": false, "intent_label": "background", "cited_paper": "W12-2021", "citing_paper": "W14-3411"}}
{"text": "The dependency path features are computed by averaging word embeddings from W on the shortest path , and are then concatenated to the feature vector e. Furthermore , we directly incorporate semantic information using word-level semantic features from Named Entity ( NE ) tags and WordNet hypernyms , as used in previous work ( Rink and Harabagiu , 2010 ; Socher et al. , 2012 ; Yu et al. , 2014 ) .", "metadata": {"citing_string": "Yu et al. , 2014", "section_number": 4, "is_self_cite": false, "intent_label": "method", "cited_paper": "W13-4068", "citing_paper": "W03-0409"}}
