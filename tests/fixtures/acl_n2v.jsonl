{"paper_id": "N12-1048", "abstract": "In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. In this work, we address the problem of incremental speech-to-speech translation (S2S) that enables cross-lingual communication between two remote participants over a telephone. We investigate the problem in a novel real-time Session Initiation Protocol (SIP) based S2S framework. The speech translation is performed incrementally based on generation of partial hypotheses from speech recognition. We describe the statistical models comprising the S2S system and the SIP architecture for enabling real-time two-way cross-lingual dialog. We present dialog experiments performed in this framework and study the tradeoff in accuracy versus latency in incremental speech translation. Experimental results demonstrate that high quality translations can be generated with the incremental approach with approximately half the latency associated with nonincremental approach.", "title": "Real-time Incremental Speech-to-Speech Translation of Dialogs", "venue": "N", "graph_vector": [0.125098, 0.480843, 0.229536, 0.222365, 0.653658, -1.19315, 0.0124634, 0.15576, -1.33231, 0.246068, -0.123579, -0.590741, 0.460251, 0.397713, -0.0796341, -0.281036, 0.258627, -0.727439, 0.218483, 0.0603461, -0.383203, 0.636246, 0.390166, 0.270394, -0.511859, -0.251107, -0.00431533, 0.117586, -0.417964, -0.540696, -0.24143, 0.270124, -0.284199, -0.0340927, -0.208311, 0.273013, -0.278786, 0.749513, 0.217222, -0.417844, -0.0403301, -0.574581, -0.492015, 0.387932, 1.4002, -0.925445, -0.233266, 0.203208, -0.201505, -0.0213002, -0.0495792, -0.105481, 0.812265, 0.841613, -0.44453, 0.202133, 0.23349, 0.390498, 0.105252, -0.0167633, 0.00310336, -0.317088, 0.331828, -0.320586, 0.182974, 0.156197, -0.0599738, -0.0961462, -0.0589716, -0.0875468, 0.322352, 0.0842272, -0.0964686, 0.215097, 0.381328, 0.302268, -0.664523, -0.447111, -0.257419, -0.401835, -0.105269, 0.0705273, -0.377407, -0.418956, -0.0791064, -0.311179, 0.213812, -0.324002, 0.198423, 0.14494, 0.0912152, -0.0969431, -0.78948, -0.287532, -0.0496351, 0.241951, -0.27647, -0.0500544, -0.208937, 0.0470909, -0.173611, 0.0447822, -0.0329984, 0.344312, 0.114495, 0.0920097, 0.173714, 0.036945, 0.4675, 0.104321, 0.392125, 0.357749, 0.246987, 0.107861, -0.338435, 0.0597549, 0.0980967, 0.574062, 0.147811, -0.106757, 0.397994, -0.126438, 0.128953, -0.163617, 0.476223, 0.467374, -0.121263, 0.0711929], "internal": 1}
{"paper_id": "N13-3005", "abstract": "This paper presents a web application and a web service for the diagnostic evaluation of Machine Translation (MT). These web-based tools are built on top of DELiC4MT, an opensource software package that assesses the performance of MT systems over user-defined linguistic phenomena (lexical, morphological, syntactic and semantic). The advantage of the web-based scenario is clear; compared to the standalone tool, the user does not need to carry out any installation, configuration or maintenance of the tool.", "title": "A Web Application for the Diagnostic Evaluation of Machine Translation over Specific Linguistic Phenomena", "venue": "N", "graph_vector": [0.333472, 0.120822, -0.0748983, 0.036097, 0.605674, -0.589111, 0.00255181, -0.00368569, -1.65007, 0.370181, 0.336711, -0.679126, 0.899343, -0.109746, 0.259376, -0.209642, 0.419882, -0.518214, -0.0228003, 0.498603, -0.336177, 0.514375, 0.513284, 0.136573, -0.442842, -0.281873, -0.194004, 0.320289, -0.00100586, -0.369399, 0.0721651, -0.0949236, 0.12012, -0.0453385, -0.464259, 0.179968, -0.396534, 1.02953, -0.0589181, 0.0459338, -0.318328, -0.285813, -0.672862, 0.140921, 0.714476, -0.601083, -0.0594363, 0.358885, 0.0984207, -0.0335563, 0.212031, 0.204323, 0.605581, 0.61638, -0.170378, 0.598051, 0.528155, 0.221439, -0.0032318, -0.34613, -0.290706, 0.0902969, 0.145762, 0.283484, 0.0842984, 0.322738, 0.381082, -0.701912, 0.169577, -0.0539376, 0.0892959, 0.124191, 0.376012, 0.114421, 0.532383, 0.225403, -0.501733, -0.231997, 0.0812583, 0.00922223, 0.0824587, 0.0942523, 0.0705091, -0.0403879, 0.131924, -0.322876, 0.12077, -0.225977, 0.0609625, 0.033336, 0.010254, -0.35859, -1.15068, 0.0377639, -0.113692, -0.100836, -0.171992, 0.0219737, -0.0419629, -0.0900012, -0.496437, 0.163804, -0.132974, 0.368423, -0.280484, 0.00123196, -0.202521, 0.167808, 0.108667, -0.270983, 0.505835, 0.0477493, -0.212245, -0.0207654, -0.15161, -0.26703, 0.226507, 0.259411, 0.156391, 0.276205, 0.153282, -0.222267, -0.147468, 0.339656, 0.440037, 0.275035, -0.0407365, 0.36333], "internal": 1}
{"paper_id": "N13-3001", "abstract": "Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings. These ambiguities can still be found when language is restricted to a particular domain, such as biomedicine. Word Sense Disambiguation (WSD) systems attempt to resolve these ambiguities but are often only able to identify the meanings for a small set of ambiguous terms. DALE (Disambiguation using Automatically Labeled Examples) is a supervised WSD system that can disambiguate a wide range of ambiguities found in biomedical documents. DALE uses the UMLS Metathesaurus as both a sense inventory and as a source of information for automatically generating labeled training examples. DALE is able to disambiguate biomedical documents with the coverage of unsupervised approaches and accuracy of supervised methods.", "title": "DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples", "venue": "N", "graph_vector": [0.0305873, 0.329426, 0.450815, -0.0901532, 0.622076, -0.60545, 0.178767, -0.323464, -1.8557, 0.389639, -0.241218, -0.153732, 0.748446, 0.377286, 0.0191177, -0.246435, 0.901923, -0.806926, 0.293941, 0.256523, -0.129928, 0.205642, -0.277478, 0.000638746, -0.11803, 0.288164, -0.214139, 0.0196118, 0.0232115, 0.125302, 0.106988, 0.278318, -0.22119, -0.161157, -0.365731, -0.00382491, -0.456402, 0.722091, 0.22101, -0.206679, -0.262471, -0.259473, -0.25085, -0.058131, 1.06838, -0.784823, 0.601505, 0.0970251, -0.11433, -0.188072, 0.652541, 0.156953, 0.958969, 0.731058, -0.19773, 0.111297, 0.107563, 0.470793, -0.332844, 0.310308, -0.216853, -0.0703449, 0.135596, 0.129376, 0.0916219, -0.461139, 0.111264, -0.114357, -0.140484, -0.20589, 0.313725, 0.04551, 0.0274969, 0.0564127, -0.0145161, -0.0905536, -0.833155, -0.188146, 0.163708, -0.114639, -0.277959, -0.050256, 0.0935285, 0.36478, 0.078457, -0.504321, 0.208024, -0.131845, 0.210167, 0.100658, 0.444669, 0.0365569, -0.778038, -0.0688073, -0.122099, -0.079435, 0.0253824, -0.0469866, -0.140332, 0.409292, -0.46445, 0.121393, 0.183999, 0.212383, -0.126512, -0.34201, -0.413164, 0.0100552, -0.220856, -0.950241, -0.0533633, 0.177655, -0.501414, 0.161194, -0.262436, -0.116859, 0.436587, 0.299072, 0.205589, 0.211556, 0.0903932, -0.147762, -0.132485, -0.107423, 0.347574, -0.187472, -0.0325946, 0.0947153], "internal": 1}
{"paper_id": "N13-3004", "abstract": "Anafora is a newly-developed open source web-based text annotation tool built to be lightweight, flexible, easy to use and capable of annotating with a variety of schemas, simple and complex. Anafora allows secure web-based annotation of any plaintext file with both spanned (e.g. named entity or markable) and relation annotations, as well as adjudication for both types of annotation. Anafora offers automatic set assignment and progress-tracking, centralized and humaneditable XML annotation schemas, and filebased storage and organization of data in a human-readable single-file XML format.", "title": "Anafora: A Web-based General Purpose Annotation Tool", "venue": "N", "graph_vector": [-0.161939, 0.556682, 0.4048, 0.30189, 0.639702, -0.693487, -0.244341, 0.533594, -1.59263, 0.405136, -0.232866, -0.358289, 0.86727, -0.243028, -0.0426217, 0.0791103, 0.594098, -0.922589, 0.21223, 0.468591, -0.384293, 0.825632, 0.433429, 0.0113032, -0.350605, 0.0517608, 0.0921099, -0.0242704, -0.198057, -0.0137691, 0.145636, 0.195094, -0.0340751, -0.352841, -0.0435399, -0.091032, 0.00195727, 0.467248, -0.178696, -0.0157147, 0.190716, -0.253654, -0.415277, -0.0593514, 0.929047, -1.06012, 0.213073, 0.22552, 0.189169, 0.176255, 0.377887, 0.0140598, 0.711896, 0.802647, -0.0415096, 0.178036, 0.00917409, 0.106547, -0.0362512, -0.00488296, -0.385235, 0.137929, -0.0140593, -0.156071, 0.212482, -0.153314, 0.128084, -0.0466429, 0.425804, -0.143659, 0.292127, 0.126341, 0.14451, 0.242647, -0.176568, -0.171556, -0.605398, -0.422596, -0.240873, 0.423539, -0.209123, 0.0525821, 0.281391, -0.172124, 0.0526081, -0.281473, 0.157791, -0.207228, 0.260996, 0.0381286, -0.136838, 0.212391, -0.767901, 0.091663, -0.0198788, -0.252608, -0.0606041, -0.0842115, -0.234753, -0.0682312, -0.310578, -0.401445, 0.168465, 0.122636, -0.0648129, 0.0802416, 0.00317776, -0.0547608, -0.0271057, -0.551219, 0.0310062, 0.222299, 0.164132, -0.215792, -0.610901, -0.0982422, 0.618728, 0.224311, 0.130477, 0.297421, 0.509445, -0.081673, 0.0840036, -0.346074, 0.259275, 0.184515, -0.20257, -0.0647893], "internal": 1}
{"paper_id": "N13-2001", "abstract": "In this paper we revisit the task of quantitative evaluation of coreference resolution systems. We review the most commonly used metrics (MUC, B3, CEAF and BLANC) on the basis of their evaluation of coreference resolution in five texts from the OntoNotes corpus. We examine both the correlation between the metrics and the degree to which our human judgement of coreference resolution agrees with the metrics. In conclusion we claim that loss of information value is an essential factor, insufficiently adressed in current metrics, in human perception of the degree of success or failure of coreference resolution. We thus conjecture that including a layer of mention information weight could improve both the coreference resolution and its evaluation.", "title": "Critical Reflections on Evaluation Practices in Coreference Resolution", "venue": "N", "graph_vector": [0.0318512, 0.13185, -0.198625, 0.402401, 0.696802, -0.397147, 0.136625, -0.153502, -1.74734, 0.489159, 0.0443007, -0.167246, 0.696649, 0.344089, 0.412315, -0.140184, 0.386229, -0.759374, 0.163688, 0.404254, -0.855201, 0.337156, 0.058745, -0.168978, -0.38885, -0.0283194, 0.371026, 0.571194, -0.244277, -0.168792, 0.0116987, -0.0187564, 0.188589, -0.572933, -0.222566, 0.34412, -0.112999, 0.519111, -0.147323, -0.126693, 0.288731, -0.178874, -0.375292, 0.214569, 0.859226, -0.909979, 0.326473, 0.0246392, -0.214819, -0.0844252, -0.00838534, 0.00405335, 0.817087, 0.740573, 0.050798, 0.13921, -0.464992, 0.0901702, -0.218621, 0.151332, -0.237351, -0.344695, 0.0100587, -0.0997552, 0.00827883, -0.255619, 0.305641, -0.173203, 0.00510831, 0.0149295, 0.121061, 0.409935, -0.00444137, -0.097127, 0.12111, -0.146525, -0.39224, -0.143921, -0.297511, -0.247778, 0.169017, 0.107646, 0.0384998, -0.276049, 0.0235926, -0.315274, 0.130566, 0.0251412, 0.057661, 0.115561, 0.458813, 0.0271141, -0.576938, -0.117054, -0.239529, -0.184906, 0.146746, 0.0215783, -0.0302145, -0.0376009, 0.0254632, -0.294498, -0.0864786, 0.226454, -0.49606, 0.157469, -0.25055, 0.0998574, 0.063881, -0.246853, -0.12279, 0.0920977, -0.0932872, -0.0926803, 0.0109605, -0.335971, 0.465587, 0.0758049, 0.0365742, 0.364311, -0.0441247, 0.124196, -0.0332721, -0.241658, 0.395478, 0.27429, 0.119759, -0.23049], "internal": 1}
{"paper_id": "N09-2014", "abstract": "We investigate natural language understanding of partial speech recognition results to equip a dialogue system with incremental language processing capabilities for more realistic human-computer conversations. We show that relatively high accuracy can be achieved in understanding of spontaneous utterances before utterances are completed.", "title": "Towards Natural Language Understanding of Partial Speech Recognition Results in Dialogue Systems", "venue": "N", "graph_vector": [0.437112, -0.0659429, -0.349185, 0.0664161, 0.360241, -0.967163, 0.509795, -0.267708, -1.83792, 0.27359, -0.0901591, 0.0358707, 0.578567, -0.0429482, 0.119617, -0.141548, 0.568366, -0.239381, 0.0381973, 0.263526, -0.369495, 0.653238, 0.175874, -0.113096, -0.512407, 0.045633, -0.121349, -0.0989701, -0.217287, 0.243926, -0.393607, 0.0646336, -0.0822479, -0.9709, -0.0700689, 0.0163934, -0.271109, 0.722492, 0.599547, 0.424503, -0.0961988, -0.000633052, -0.478759, 0.130155, 0.907123, -1.09009, -0.0324114, -0.351038, 0.0203396, -0.328539, 0.325799, -0.405715, 0.536402, 0.783296, -0.272868, 0.220232, 0.357273, 0.217354, 0.32918, 0.0643624, 0.0332071, 0.29397, -0.049657, 0.152334, -0.412643, 0.187765, 0.191783, 0.0774011, -0.100006, 0.0635196, 0.271562, 0.393344, 0.273463, 0.278035, 0.0309718, -0.074612, -0.530536, -0.276203, -0.20096, 0.341728, -0.0227707, -0.400377, 0.12734, -0.0584057, 0.236215, 0.0225392, 0.0777701, 0.0190901, 0.186146, -0.17172, 0.561949, 0.0607868, -0.491887, -0.224884, -0.24159, 0.117683, -0.164726, 0.0923298, 0.459506, -0.177568, -0.159642, -0.432812, 0.369847, 0.411834, -0.320885, -0.0472919, 0.21981, -0.152384, 0.164834, -0.41597, -0.430555, -0.110091, -0.216273, 0.002456, 0.00182676, -0.227557, 0.190471, 0.0307567, -0.106537, -0.144216, 0.419738, 0.0951945, -0.210194, -0.241877, 0.328367, 0.0753045, -0.465716, -0.0711644], "internal": 1}
{"paper_id": "N03-1030", "abstract": "We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.", "title": "Sentence Level Discourse Parsing using Syntactic and Lexical Information", "venue": "N", "graph_vector": [-0.0261667, 0.246105, 0.164945, 0.0880282, 0.2968, -0.678668, -0.0472365, -0.41394, -1.71016, 0.21315, -0.397468, -0.421572, 0.0623369, -0.0631454, -0.00248879, 0.151459, 0.311899, -0.589878, 0.374607, 0.312256, -0.239775, 0.595612, 0.0786003, -0.244345, -0.136571, 0.216195, -0.157366, -0.0280036, 0.141517, -0.37545, 0.0802257, -0.11688, 0.155411, -0.382336, -0.161117, 0.0191217, -0.477183, 0.835845, 0.340032, -0.0569027, -0.0581612, -0.210864, -0.162974, -0.197232, 0.887099, -0.398619, -0.155026, -0.124039, 0.166447, -0.0583709, 0.185042, 0.0453087, 1.06669, 0.655023, -0.200443, 0.215325, 0.213954, 0.207845, -0.00858189, 0.0127024, 0.120186, -0.0561144, -0.145313, -0.112114, -0.0927159, 0.105847, 0.162344, -0.0703262, 0.230059, -0.204848, 0.108534, -0.204576, 0.101123, -0.0864381, -0.00652624, -0.202432, -0.0823429, 0.140261, -0.129709, 0.155454, -0.216329, 0.211777, 0.0953691, 0.0845275, 0.00132185, -0.029076, -0.0250675, 0.139131, 0.243991, -0.0954888, 0.018359, 0.078034, -0.671342, -0.056641, -0.0529545, 0.10108, -0.332806, 0.21724, -0.057571, -0.154467, -0.210523, -0.276558, 0.00613879, 0.0713728, -0.110587, -0.240346, -0.178027, 0.0329299, -0.0916883, -0.216027, 0.139651, 0.196438, 0.0588096, -0.200479, 0.235606, 0.27621, 0.470317, -0.000829274, 0.097815, 0.117356, -0.152879, 0.0333427, -0.021364, -0.10261, -0.0624005, 0.159091, 0.120919, -0.737189], "internal": 1}
{"paper_id": "N03-1002", "abstract": "Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering. A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking. However, there are some cases where segmentation granularity contradicts the results of morphological analysis and the building units of NEs, so that extraction of some NEs are inherently impossible in this setting. To cope with the unit problem, we propose a character-based chunking method. Firstly, the input sentence is analyzed redundantly by a statistical morphological analyzer to produce multiple (n-best) answers. Then, each character is annotated with its character types and its possible POS tags of the top n-best answers. Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs. This method introduces richer information to the chunker than previous methods that base on a single morphological analysis result. We apply our method to IREX NE extraction task. The cross validation result of the F-measure being 87.2 shows the superiority and effectiveness of the method.", "title": "Japanese Named Entity Extraction with Redundant Morphological Analysis", "venue": "N", "graph_vector": [0.072419, -0.344789, 0.293087, 0.469448, 0.289753, -0.684766, -0.244513, 0.148427, -1.67906, 0.238574, 0.148479, -0.037085, 0.399851, 0.100953, -0.0814023, -0.152655, 0.136348, -0.566448, 0.428361, 0.40098, -0.405074, 0.681208, 0.421614, 0.331375, -0.691756, 0.135318, 0.108111, -0.0937147, -0.364998, -0.310181, 0.310586, 0.0451639, 0.390773, -0.374646, 0.246482, -0.135012, -0.410784, 0.710799, 0.308258, 0.0192965, -0.0628261, -0.360311, -0.293397, 0.653065, 1.29096, -0.527708, -0.172025, 0.274614, 0.0550162, 0.0996121, 0.685335, -0.366724, 0.474465, 0.663211, -0.396185, 0.251064, 0.0667461, 0.453518, -0.0473293, 0.0535186, -0.632297, -0.19575, 0.141304, 0.0196617, -0.362308, -0.305144, 0.202482, 0.0839392, 0.211627, 0.0690873, -0.0996831, 0.374762, -0.308401, -0.36955, 0.28394, -0.228815, -0.417481, -0.227879, 0.0721907, -0.00466215, 0.306982, -0.120226, 0.766912, 0.378783, 0.203636, -0.514315, 0.440955, -0.25599, -0.0027087, 0.185204, -0.0335465, -0.31488, -0.777931, -0.1041, -0.19599, -0.0163317, -0.350024, 0.0134766, -0.0591079, -0.237664, 0.0157489, -0.368192, 0.138288, -0.441746, 0.238335, -0.202369, 0.308285, 0.0430267, 0.145511, -0.255321, -0.274988, 0.487357, -0.180924, -0.0211248, 0.0631278, -0.32445, 0.630862, -0.050458, -0.118117, 0.327774, 0.037932, -0.113446, -0.143392, 0.0769867, 0.296583, -0.0270856, -0.00366342, 0.257247], "internal": 1}
{"paper_id": "N03-1006", "abstract": "We compute the coverage and accuracy of the system considering two estimation methods: type-based (all nouns are considered equally important and get a weight of one) and token-based (each prediction is weighted by the number of occurrences of the target noun in the corpus). For most languages, we had at least a main-POS annotated corpus, so that we could consider only the nominal instances of the words when computing the token coverage and accuracy. When such information is not available, one can approximate the token coverage and accuracy by considering all instances of the word forms from the noun list in the corpus, regardless their true POS.", "title": "", "venue": "N", "graph_vector": [-0.0290364, 0.385941, -0.434211, 0.320631, 0.602778, -0.733401, 0.107948, 0.206993, -1.32615, 0.100288, -0.385905, 0.0183233, 0.706224, 0.0618191, -0.101842, -0.1947, 0.164734, -0.378293, 0.0864714, 0.151939, -0.599048, 0.732037, -0.0339705, -0.314813, -0.49388, 0.272766, 0.119638, 0.261111, -0.416366, -0.37152, -0.141697, -0.250461, -0.0822258, -0.566836, 0.0119498, 0.362875, -0.0078455, 0.950415, 0.163104, -0.308921, 0.0870347, -0.584018, -0.23816, 0.468325, 0.924332, -0.941181, 0.274373, 0.130956, -0.292756, 0.181254, 0.264991, 0.367232, 0.897485, 0.619005, -0.145371, 0.0265107, -0.202125, -0.081452, -0.18333, 0.284745, -0.358326, -0.477522, 0.30206, -0.159977, -0.016725, -0.203224, 0.414655, 0.100575, 0.187414, 0.0248612, -0.235168, -0.311155, 0.134824, 0.251416, -0.0931015, 0.102719, 0.198983, -0.516633, 0.186091, 0.266741, 0.0463419, 0.040343, -0.0877806, -0.191742, 0.0648325, 0.047349, 0.198628, -0.0467004, 0.0347502, -0.104143, 0.0253047, -0.135042, -0.321911, 0.0174977, -0.0188313, -0.201706, 0.231329, -0.24178, 0.155124, -0.0643822, -0.118436, -0.156609, 0.19672, -0.217127, 0.200926, 0.0368307, 0.38306, -0.265325, -0.0240221, -0.0758778, -0.477504, 0.372655, -0.00366266, -0.18269, 0.171747, 0.121432, 0.274738, 0.244959, -0.198343, 0.341426, -0.338619, -0.487646, -0.201072, 0.323189, 0.784803, 0.498837, 0.0539732, 0.263967], "internal": 1}
{"paper_id": "N03-1011", "abstract": "The discovery of semantic relations from text becomes increasingly important for applications such as Question Answering, Information Extraction, Text Summarization, Text Understanding, and others. The semantic relations are detected by checking selectional constraints. This paper presents a method and its results for learning semantic constraints to detect part-whole relations. Twenty constraints were found. Their validity was tested on a 10,000 sentence corpus, and the targeted partwhole relations were detected with an accuracy of 83%.", "title": "the Automatic Discovery of Part-Whole Relations Main Papers , pp. 1-8 Edmonton, May-June 2003", "venue": "N", "graph_vector": [0.112611, 0.409342, -0.0489564, 0.0412289, 0.337918, -0.442521, -0.0448157, 0.126719, -1.86075, 0.348109, 0.0688109, -0.373311, 0.754926, -0.280345, -0.171636, -0.350332, 0.230709, -0.438896, 0.12795, 0.10491, -0.299532, 0.313236, 0.136234, 0.213665, -0.242018, 0.276847, 0.0718442, 0.645071, -0.193412, 0.130202, 0.358862, -0.0675415, -0.151955, -0.384911, 0.0403879, 0.334387, -0.389689, 0.543534, 0.185606, 0.101903, 0.141657, -0.357265, -0.0229337, 0.495747, 0.827977, -0.780636, 0.0694195, 0.295356, -0.141723, 0.118571, 0.124717, 0.210689, 0.502728, 0.59435, 0.0894693, 0.0745862, 0.0641465, 0.396187, -0.335115, 0.225423, -0.157297, -0.13429, -0.361522, 0.126608, -0.302572, -0.248359, 0.344162, -0.0585927, -0.0555501, -0.0669774, -0.065215, 0.0171181, 0.0374954, 0.227896, 0.00373191, -0.510165, -0.331576, -0.383894, 0.088587, 0.0341386, 0.216357, 0.12033, 0.411468, 0.0515909, -0.122197, 0.217955, 0.0140992, 0.521668, 0.215993, -0.0837544, 0.0864064, 0.347662, -0.729071, 0.0401412, -0.0560499, -0.0610962, -0.444549, 0.449904, 0.237179, -0.246195, -0.327263, -0.351214, 0.15621, 0.0104744, 0.4534, 0.0403487, -0.190396, 0.114495, -0.259653, -0.0958775, -0.415196, -0.0327708, -0.179263, 0.266991, 0.0623633, 0.0833985, 0.549831, 0.26936, 0.0358072, 0.0576323, 0.242989, -0.231383, -0.308394, 0.0223673, 0.129873, 0.36735, 0.0143792, 0.28649], "internal": 1}
{"paper_id": "N06-1024", "abstract": "We present a two stage parser that recovers Penn Treebank style syntactic analyses of new sentences including skeletal syntactic structure, and, for the first time, both function tags and empty categories. The accuracy of the first-stage parser on the standard Parseval metric matches that of the (Collins, 2003) parser on which it is based, despite the data fragmentation caused by the greatly enriched space of possible node labels. This first stage simultaneously achieves near state-of-theart performance on recovering function tags with minimal modifications to the underlying parser, modifying less than ten lines of code. The second stage achieves state-of-the-art performance on the recovery of empty categories by combining a linguistically-informed architecture and a rich feature set with the power of modern machine learning methods.", "title": "Fully Parsing the Penn Treebank", "venue": "N", "graph_vector": [-0.218606, 0.0650576, -0.0807978, 0.292605, 0.273839, -0.782178, -0.107649, -0.0698429, -1.46035, 0.460201, 0.0362123, -0.597557, 0.54927, 0.188058, -0.0423659, 0.00655739, 0.655081, -0.798132, -0.0974638, 0.595025, -0.272619, 0.548585, 0.268117, -0.00129414, -0.280042, 0.621449, 0.304518, -0.140506, -0.173265, -0.573459, -0.050928, 0.0650418, -0.121218, -0.647952, -0.0529597, 0.0846736, -0.0189549, 0.664946, 0.517358, -0.320902, 0.136573, 0.0863686, -0.560584, 0.182346, 0.793208, -0.721623, 0.244729, 0.140817, 0.239174, 0.25025, 0.116316, 0.02569, 0.58048, 0.372317, -0.399788, 0.359844, 0.144684, 0.221977, 0.0243561, 0.132153, -0.212012, -0.57591, -0.0791794, -0.0229158, -0.402461, 0.0107212, -0.172293, 0.0738248, 0.124763, 0.121006, 0.0548963, -0.0401261, 0.194447, -0.0725427, 0.0196309, -0.148631, -0.361243, -0.389867, -0.0118097, -0.215454, -0.231009, 0.106492, 0.0391813, 0.0469957, 0.0197272, -0.444601, 0.128929, -0.102452, 0.352761, 0.11572, 0.152789, -0.128138, -0.294382, -0.171913, -0.107766, 0.174625, -0.435394, 0.363054, 0.136848, -0.0994896, 0.366051, -0.382121, -0.0926936, 0.148853, -0.14547, 0.282354, -0.232789, -0.0575805, -0.113422, -0.136326, -0.0717367, -0.159557, 0.510818, -0.0351581, 0.155262, -0.296603, 0.514398, 0.209533, 0.477155, -0.0151321, -0.0523975, 0.0843795, -0.297014, -0.210267, 0.453688, 0.375419, -0.0786096, -0.229728], "internal": 1}
{"paper_id": "N07-2018", "abstract": "This paper presents an alternative algorithm based on the singular value decomposition (SVD) that creates vector representation for linguistic units with reduced dimensionality. The work was motivated by an application aimed to represent text segments for further processing in a multi-document summarization system. The algorithm tries to compensate for SVD’s bias towards dominant-topic documents. Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the baseline algorithms - the SVD and the vector space model.", "title": "Clustered Sub-matrix Singular Value Decomposition", "venue": "N", "graph_vector": [0.57635, 0.498627, 0.0511136, 0.324814, 0.0419539, -0.353128, -0.0937154, 0.352982, -2.3387, 0.239293, -0.157761, 0.0703445, 1.19878, 0.103787, -0.0642609, -0.315379, 0.525655, -1.03073, 0.204339, 0.374066, -0.267332, 0.470355, 0.392148, -0.339845, -0.707825, 0.288043, 0.255533, 0.226971, -0.188619, -0.307958, 0.030288, -0.240726, -0.0258099, -0.477366, -0.336068, 0.520272, 0.0183379, 0.805101, 0.561251, -0.486956, -0.232125, -0.643816, -0.330098, 0.622407, 1.02443, -0.659399, -0.188333, 0.152269, 0.108303, -0.0252861, 0.391114, -0.49552, 1.00162, 0.969643, -0.692361, -0.025056, 0.408485, 0.28911, 0.102107, 0.225707, -0.247805, 0.223035, -0.419556, -0.0211754, -0.212575, 0.238013, 0.0668987, 0.476523, -0.293748, 0.132818, -0.10242, 0.213756, 0.241156, -0.0729755, 0.102259, -0.539009, -0.61331, -0.288099, 0.0680617, 0.124989, 0.199411, 0.249557, 0.39302, -0.0230343, 0.0432684, -0.28582, 0.149838, -0.30169, 0.0746763, -0.253363, -0.109819, 0.228192, -0.951523, -0.149908, -0.276328, 0.613004, -0.38011, -0.141567, -0.154586, -0.0354536, -0.187335, -0.258682, 0.0777007, -0.030079, -0.141844, -0.120742, -0.12307, -0.16187, 0.233389, -0.387209, -0.196004, -0.0597877, -0.0879739, 0.209397, -0.0597101, -0.132112, 0.330729, 0.224274, -0.174977, 0.445106, -0.404986, -0.0335828, -0.116375, -0.217526, 0.539229, 0.336458, -0.106311, 0.0639979], "internal": 1}
{"paper_id": "N07-2036", "abstract": "Texts exhibit subtle yet identifiable modality about writers’ estimation of how true each statement is (e.g., definitely true or somewhat true). This study is an analysis of such explicit certainty and doubt markers in epistemically modalized statements for a written news discourse. The study systematically accounts for five levels of writer’s certainty (ABSOLUTE, HIGH, MODERATE, LOW CERTAINTY and UNCERTAINTY) in three news pragmatic contexts: perspective, focus, and time. The study concludes that independent coders’ perceptions of the boundaries between shades of certainty in epistemically modalized statements are highly subjective and present difficulties for manual annotation and consequent automation for opinion extraction and sentiment analysis. While stricter annotation instructions and longer coder training can improve intercoder agreement results, it is not entirely clear that a five-level distinction of certainty is preferable to a simplistic distinction between statements with certainty and statements with doubt.", "title": "Stating with Certainty or Stating with Doubt: Intercoder Reliability Results for Manual Annotation of Epistemically Modalized Statements", "venue": "N", "graph_vector": [-0.676643, 0.329093, 0.426547, -0.118903, 0.646016, -0.510731, -0.120172, -0.0966249, -1.81416, 0.10537, -0.191085, -0.473271, 0.404872, 0.227907, -0.0478012, 0.0886127, 0.611901, -0.401855, -0.133627, 0.484328, -0.336774, 0.556069, 0.241599, 0.314298, -0.501143, -0.184218, 0.274175, 0.100029, -1.00723, -0.318336, -0.135538, 0.081391, 0.10571, -0.441403, -0.253878, 0.0436776, -0.0630145, 0.824893, 0.054267, 0.352517, -0.664886, -0.178087, -0.41622, 0.0692858, 1.055, -0.805408, 0.110147, 0.27989, 0.0109849, 0.241486, 0.511709, -0.0976216, 0.574342, 0.755803, -0.0888764, 0.122063, 0.258382, 0.396637, 0.225154, 0.0532899, -0.0808903, -0.0179296, -0.0757947, -0.0379803, -0.0628594, -0.110016, 0.383846, 0.224173, -0.0850566, 0.313871, -0.0548168, -0.197106, 0.290453, 0.347289, 0.20226, -0.188158, -0.524868, -0.519332, -0.261487, 0.0740264, -0.0798566, -0.0685069, -0.0093415, -0.477466, -0.109032, -0.174594, 0.176744, 0.269301, -0.0524419, 0.162019, 0.107873, -0.313999, -0.591707, -0.224224, 0.112107, 0.0316288, 0.143429, -0.057237, 0.167642, 0.195502, -0.291442, -0.339212, -0.426906, -0.454651, -0.195316, -0.249998, 0.0016381, -0.240167, 0.0191001, -0.341719, -0.320471, 0.0935543, -0.419857, 0.138652, -0.418271, -0.150367, 0.797555, 0.268481, 0.305799, 0.385727, 0.221712, -0.337529, -0.0267332, 0.3221, 0.454542, -0.101333, 0.0114586, -0.428998], "internal": 1}
{"paper_id": "N07-4013", "abstract": "Traditional information extraction systems have focused on satisfying precise, narrow, pre-specified requests from small, homogeneous corpora. In contrast, the TEXTRUNNER system demonstrates a new kind of information extraction, called Open Information Extraction (OIE), in which the system makes a single, data-driven pass over the entire corpus and extracts a large set of relational tuples, without requiring any human input. (Banko et al., 2007) TEXTRUNNER is a fullyimplemented, highly scalable example of OIE. TEXTRUNNER's extractions are indexed, allowing a fast query mechanism. Our first public demonstration of the TEXTRUNNER system shows the results of performing OIE on a set of 117 million web pages. It demonstrates the power of TEXTRUNNER in terms of the raw number of facts it has extracted, as well as its precision using our novel assessment mechanism. And it shows the ability to automatically determine synonymous relations and objects using large sets of extractions. We have built a fast user interface for querying the results.", "title": "TEXTRUNNER: Open Information Extraction on the Web", "venue": "N", "graph_vector": [-0.131141, 0.123631, 0.10497, 0.0939536, 0.539964, -0.537692, 0.0833992, -0.0413201, -1.54496, 0.529159, 0.0729701, -0.620591, 0.699235, 0.402163, 0.0202359, 0.139326, 0.267295, -0.205683, 0.0732087, 0.329476, -0.0567319, 0.483108, 0.267883, 0.0438294, -0.409906, 0.200978, -0.356972, 0.218039, -0.0323815, -0.275271, 0.305066, -0.0215304, 0.123669, -0.944526, -0.121324, 0.246332, -0.596067, 0.478914, 0.100028, -0.144068, 0.0355757, -0.858416, -0.152556, 0.14522, 1.04272, -0.66076, -0.191474, 0.0642724, -0.0673316, -0.244804, 0.579491, -0.077497, 0.558029, 0.68007, 0.129602, -0.078934, -0.117692, 0.185533, -0.326207, 0.214395, -0.216939, -0.438762, -0.13285, 0.0802802, 0.0408808, -0.104196, 0.010243, -0.324847, 0.287209, 0.187506, 0.021601, -0.0183511, -0.188908, 0.104866, 0.666732, -0.229254, -0.308874, -0.255652, 0.033722, -0.0138926, -0.0432353, -0.118548, 0.14875, -0.151541, -0.0794046, -0.480362, -0.115392, 0.149727, 0.0851824, 0.263132, 0.052065, 0.143112, -0.901264, -0.0741174, -0.0774395, 0.0793744, -0.036974, -0.185877, -0.186994, -0.305204, -0.0828976, -0.155897, -0.156446, 0.333233, 0.0253385, 0.10616, -0.0659748, -0.0296185, 0.422558, -0.103637, -0.0308706, 0.0190192, 0.0919962, 0.276701, -0.138293, -0.0863205, 0.305984, 0.0977622, -0.307331, -0.115753, -0.376932, -0.154777, -0.123077, -0.120794, 0.365239, 0.0468971, 0.113087, 0.263266], "internal": 1}
{"paper_id": "E91-1032", "abstract": "This paper describes a computational model of human sentence processing based on the principles and parameters paradigm of current linguistic theory. The syntactic processing model posits four modules, recovering phrase structure, long-distance dependencies, coreference, and thematic structure. These four modules are implemented as meta-interpreters over their relevant components of the grammar, permitting variation in the deductive strategies employed by each module. These four interpreters are also `coroutined' via the freeze directive of constraint logic programming to achieve incremental interpretation across the modules.", "title": "Multiple Interpreters in a Principle-Based Model of Sentence Processing", "venue": "E", "graph_vector": [], "internal": 1}
{"paper_id": "E12-1029", "abstract": "Most event extraction systems are trained with supervised learning and rely on a collection of annotated documents. Due to the domain-specificity of this task, event extraction systems must be retrained with new annotated data for each domain. In this paper, we propose a bootstrapping solution for event role filler extraction that requires minimal human supervision. We aim to rapidly train a state-of-the-art event extraction system using a small set of “seed nouns” for each event role, a collection of relevant (in-domain) and irrelevant (outof-domain) texts, and a semantic dictionary. The experimental results show that the bootstrapped system outperforms previous weakly supervised event extraction systems on the MUC-4 data set, and achieves performance levels comparable to supervised training with 700 manually annotated documents.", "title": "Bootstrapped Training of Event Extraction Classifiers", "venue": "E", "graph_vector": [0.232707, 0.298702, 0.11862, 0.0291672, 0.435212, -0.54812, -0.159885, 0.0847165, -1.77364, 0.443863, 0.0239665, -0.470902, 0.405409, 0.360082, 0.0958626, -0.0643006, 0.35617, -0.265807, 0.459762, 0.292515, -0.179876, 0.411703, 0.352567, -0.181834, -0.230382, 0.150885, 0.437252, 0.314961, -0.220386, -0.156921, 0.422078, -0.237857, -0.297515, -0.663997, 0.172944, 0.195347, -0.139648, 0.432638, 0.092392, -0.224852, -0.0162385, -0.693393, -0.296512, 0.362465, 0.856137, -0.586762, -0.161971, 0.33288, -0.27162, 0.154135, 0.258496, 0.0644007, 0.782722, 0.905741, -0.012294, 0.0153684, 0.105721, 0.176462, 0.0241779, -0.055915, -0.312923, -0.118271, 0.322202, -0.422017, -0.154367, 0.0509573, 0.03225, -0.438523, 0.282934, 0.169437, -0.028621, 0.39323, 0.173683, -0.162565, 0.0270965, -0.238598, -0.104719, -0.385153, -0.195031, -0.324845, 0.110532, -0.0739055, -0.0862868, 0.0197804, -0.197505, -0.0993578, -0.0436036, -0.0542241, -0.221163, 0.146848, -0.272419, 0.0962677, -1.01692, 0.0217126, 0.0905811, -0.0285278, 0.0822467, -0.31996, -0.344176, -0.235003, -0.080971, 0.101655, -0.0182893, 0.0676254, 0.266306, 0.0765455, 0.0610698, -0.277932, 0.379708, 0.0402404, -0.0341982, -0.220862, -0.0338867, 0.190616, -0.0888802, -0.0551437, 0.665323, 0.031597, 0.195632, 0.117367, 0.0141408, -0.0986232, -0.256184, 0.139645, 0.18467, 0.302113, -0.0946263, 0.00123949], "internal": 1}
{"paper_id": "E14-1035", "abstract": "Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation benchmark. We further provide an analysis of the domain-specific data and show additive gains of our model in combination with other types of topic-adapted features.", "title": "Dynamic Topic Adaptation for Phrase-based MT", "venue": "E", "graph_vector": [-0.290495, 0.215648, 0.351102, 0.115619, 0.199688, -0.635628, 0.214745, -0.0256513, -1.46687, -0.0334496, -0.0724921, -0.547222, 0.659249, 0.0720483, 0.022919, -0.230887, 0.450704, -0.703933, 0.0120566, 0.338427, 0.0404582, 0.570783, 0.359434, 0.103782, -0.365225, 0.204333, 0.126293, -0.123462, -0.0105766, -0.137732, 0.0792917, 0.149624, 0.108054, -0.151946, -0.160433, 0.338833, -0.150528, 0.635257, 0.147909, -0.350721, -0.354387, -0.379584, -0.579667, 0.575255, 1.17341, -0.931177, 0.318968, -0.0831681, -0.250063, -0.0363014, 0.407982, 0.361378, 0.966785, 0.690489, -0.417705, 0.369558, 0.0259649, 0.201409, 0.240366, 0.260919, -0.00118078, -0.0517378, 0.0234584, 0.0696581, -0.114451, 0.0479677, -0.147683, -0.103758, -0.0373057, 0.103143, 0.128014, -0.128487, 0.168155, -0.515057, 0.618458, 0.185626, -0.889881, -0.0946084, -0.0193951, -0.00146581, -0.435527, 0.0230095, 0.132034, -0.515874, -0.0853794, 0.106748, 0.154247, 0.0932215, 0.0514371, -0.281362, 0.182891, -0.0329247, -0.348252, 0.168609, -0.0732086, 0.23627, 0.200095, -0.197072, -0.0182918, 0.135953, 0.0751516, -0.43426, 0.0520126, 0.406823, -0.359809, 0.019221, 0.0940472, -0.025986, 0.108667, 0.0947659, -0.100335, -0.00292564, 0.0131828, -0.024774, -0.445925, 0.333878, 0.348465, 0.0891761, 0.182626, -0.0993568, -0.171213, 0.0405424, -0.250241, 0.0281722, 0.339626, 0.175037, -0.189475, 0.0153355], "internal": 1}
{"paper_id": "E14-2004", "abstract": "Within the context of globalization, multilinguality and cross-linguality for information access have emerged as issues of major interest. In order to achieve the goal that users from all countries have access to the same information, there is an impending need for systems that can help in overcoming language barriers by facilitating multilingual and cross-lingual access to data. In this paper, we demonstrate such a toolkit, which supports both service-oriented and user-oriented interfaces for semantically annotating, analyzing and comparing multilingual texts across the boundaries of languages. We conducted an extensive user study that shows that our toolkit allows users to solve cross-lingual entity tracking and article matching tasks more efficiently and with higher accuracy compared to the baseline approach.", "title": "Semantic Annotation, Analysis and Comparison: A Multilingual and Cross-lingual Text Analytics Toolkit", "venue": "E", "graph_vector": [0.24943, 0.24077, 0.119095, -0.284227, 0.197324, -0.459721, 0.306206, -0.0411604, -1.37196, 0.205255, -0.3364, -0.465081, 0.713231, 0.150861, -0.0787664, 0.191087, 0.7151, -0.527614, -0.190685, 0.200989, -0.236913, 0.795136, 0.151886, 0.0943041, -0.268187, -0.0401597, -0.430364, 0.250846, -0.0264324, 0.024404, 0.192765, 0.205454, -0.0426243, -0.172089, -0.398411, 0.0195432, -0.478126, 0.391255, 0.282203, -0.192438, -0.00266108, -0.366099, -0.138941, 0.438543, 0.794543, -0.762382, 0.15119, 0.281515, -0.192429, -0.109279, 0.427859, 0.268433, 1.25322, 1.05612, -0.170432, 0.167389, -0.197635, 0.260021, 0.0315379, -0.34778, -0.13858, -0.361973, 0.0488196, -0.0435528, 0.183199, 0.178959, 0.297588, 0.389908, 0.0149567, 0.215202, 0.226489, 0.0682982, -0.313536, -0.191315, 0.106834, -0.274127, -0.732899, -0.200923, 0.0184002, -0.200927, -0.349678, -0.100807, 0.24101, -0.144928, 0.191661, -0.460822, -0.0718464, -0.0573804, -0.104058, -0.213379, -0.0325883, -0.178091, -0.999228, -0.214299, -0.420553, 0.1592, 0.018591, 0.0685822, 0.150912, -0.102529, 0.243335, 0.0330807, -0.508036, 0.154413, -0.257062, 0.212155, -0.103763, 0.0752351, -0.0306554, 0.0302028, -0.491115, -0.258716, -0.29871, -0.151855, 0.15587, -0.125881, 0.347232, 0.12286, -0.0600333, 0.100625, 0.0390665, 0.0190872, -0.2048, 0.0866302, 0.368539, 0.218119, 0.304138, 0.122691], "internal": 1}
{"paper_id": "E14-4037", "abstract": "Professional human translators usually do not employ the concept of word alignments, producing translations ‘sense-forsense’ instead of ‘word-for-word’. This suggests that unalignable words may be prevalent in the parallel text used for machine translation (MT). We analyze this phenomenon in-depth for Chinese-English translation. We further propose a simple and effective method to improve automatic word alignment by pre-removing unalignable words, and show improvements on hierarchical MT systems in both translation directions.", "title": "Analysis and Prediction of Unalignable Words in Parallel Text", "venue": "E", "graph_vector": [-0.118197, 0.520819, 0.382401, 0.265865, 0.265573, -0.815337, 0.310517, -0.29448, -1.63125, 0.111627, 0.179326, -0.0957494, 0.529357, 0.0499471, 0.145009, -0.182008, 0.270462, -0.619994, 0.179715, 0.073405, -0.175132, 0.245926, 0.57847, -0.117615, -0.435261, 0.00372938, 0.325111, -0.302443, -0.237009, 0.0592049, -0.16575, 0.31777, 0.233776, -0.384715, -0.0300676, 0.186819, -0.26847, 0.629586, 0.406363, 0.000851509, -0.0773461, -0.0862016, -0.71218, 0.44269, 1.36046, -0.892559, 0.408297, 0.0885177, -0.0727963, -0.0873761, 0.0793455, -0.35133, 0.300265, 0.697972, -0.460563, 0.0953403, -0.106037, 0.0899476, 0.112486, -0.225278, -0.092461, -0.143615, 0.123046, -0.0754659, -0.395852, -0.0961968, 0.0940437, -0.149462, 0.230775, -0.0647749, 0.0996187, -0.0967757, 0.312906, -0.212464, 0.182385, -0.111596, -0.434697, -0.0664152, 0.0471217, -0.0343281, -0.25842, 0.0805903, 0.169312, 0.273257, 0.16487, -0.383309, -0.0937959, 0.0340592, -0.120312, 0.0424517, -0.0823889, -0.107109, -0.449695, -0.0550054, -0.209206, -0.212103, -0.346809, -0.0121029, 0.0907775, -0.169137, -0.162157, -0.116697, -0.115786, 0.50059, -0.0704168, -0.216864, -0.0685465, -0.200501, 0.276698, 0.0696413, 0.150975, -0.0251828, -0.217133, -0.0157596, 0.0324097, -0.150027, 0.00105319, 0.0411066, 0.0805632, 0.376973, 0.0301576, -0.219787, -0.318354, -0.0204287, 0.27531, 0.448048, 0.0577804, 0.0854841], "internal": 1}
{"paper_id": "E14-3008", "abstract": "Animacy is the semantic property of nouns denoting whether an entity can act, or is perceived as acting, of its own will. This property is marked grammatically in various languages, albeit rarely in English. It has recently been highlighted as a relevant property for NLP applications such as parsing and anaphora resolution. In order for animacy to be used in conjunction with other semantic features for such applications, appropriate data is necessary. However, the few corpora which do contain animacy annotation, rarely contain much other semantic information. The addition of such an annotation layer to a corpus already containing deep semantic annotation should therefore be of particular interest. The work presented in this paper contains three main contributions. Firstly, we improve upon the state of the art in multiclass animacy classification. Secondly, we use this classifier to contribute to the annotation of an openly available corpus containing deep semantic annotation. Finally, we provide source code, as well as trained models and scripts needed to reproduce the results presented in this paper, or aid in annotation of other texts.1", "title": "Multi-class Animacy Classification with Semantic Features", "venue": "E", "graph_vector": [0.205474, -0.0594891, -0.0317261, 0.0250612, 0.29567, -0.77037, 0.0355552, -0.27361, -1.35728, 0.0852248, -0.0593258, -0.306335, 0.299924, -0.246861, 0.204459, -0.136295, 0.433074, -0.905852, 0.090484, 0.676702, -0.136207, 0.922505, 0.231159, -0.254658, -0.590195, 0.275828, 0.412063, 0.244071, -0.0523203, 0.111759, 0.0212428, -0.0589174, 0.422026, -0.568745, -0.269173, 0.343172, -0.2078, 0.92791, 0.143525, 0.126941, -0.0850479, -0.0670867, 0.104825, 0.314968, 0.662105, -1.05155, 0.0291666, 0.124247, -0.178381, -0.0887745, 0.0653216, 0.28277, 0.863765, 0.741872, 0.181361, 0.364377, -0.291308, -0.520711, -0.212762, 0.116526, -0.136974, 0.47425, 0.0458621, 0.216921, -0.153062, 0.127299, 0.158539, -0.133906, 0.175814, 0.250162, -0.157483, -0.069864, 0.151522, -0.346318, -0.0886076, 0.165093, -0.365778, -0.271326, -0.171927, 0.183568, 0.370708, -0.201289, -0.103452, -0.400409, -0.164161, -0.0814587, 0.0818366, -0.0128095, 0.346977, -0.271708, -0.269369, 0.137155, -0.788555, -0.34367, -0.631882, -0.274612, -0.204121, -0.0128898, -0.0458391, 0.313137, -0.199122, 0.0425044, 0.0892411, 0.126271, -0.0433781, 0.490853, 0.27396, -0.0141224, -0.0323961, 0.208645, 0.316902, 0.103721, 0.306146, -0.00787084, 0.449276, 0.298695, 0.470864, 0.309036, 0.0842768, 0.748747, -0.0232483, -0.240236, -0.189238, 0.000140913, 0.135139, 0.426088, -0.494068, -0.128916], "internal": 1}
