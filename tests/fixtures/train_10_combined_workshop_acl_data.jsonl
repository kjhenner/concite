{"paper_id": "W14-1614", "authors": ["A1/2eljko AgiA++", "Joakim Nivre", "JAPrg Tiedemann"], "last_author": "JAPrg Tiedemann", "date": "2014", "title": "treebank translation for crosslingual parser induction", "workshop": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning", "combined_workshop": "natural_language_learning", "abstract": "Cross-lingual learning has become a popular approach to facilitate the development of resources and tools for low-density languages. Its underlying idea is to make use of existing tools and annotations in resource-rich languages to create similar tools and resources for resource-poor languages. Typically, this is achieved by either projecting annotations across parallel corpora, or by transferring models from one or more source languages to a target language. In this paper, we explore a third strategy by using machine translation to create synthetic training data from the original sourceside annotations. Specifically, we apply this technique to dependency parsing, using a cross-lingually unified treebank for adequate evaluation. Our approach draws on annotation projection but avoids the use of noisy source-side annotation of an unrelated parallel corpus and instead relies on manual treebank annotation in combination with statistical machine translation, which makes it possible to train fully lexicalized parsers. We show that this approach significantly outperforms delexicalized transfer parsing."}
{"paper_id": "W11-2121", "authors": ["Daguang Xu", "Yuan Cao", "Damianos Karakos"], "last_author": "Damianos Karakos", "date": "2011", "title": "description of the jhu system combination scheme for wmt", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper describes the JHU system combination scheme used in WMT-11. The JHU system combination is based on confusion network alignment, and inherited the framework developed by (Karakos et al., 2008). We improved our core system combination algorithm by making use of TER-plus, which was originally designed for string alignment, for alignment of confusion networks. Experimental results on French-English, GermanEnglish, Czech-English and Spanish-English combination tasks show significant improvements on BLEU and TER by up to 2 points on average, compared to the best individual system output, and improvements compared with the results produced by ITG which we used in WMT-10."}
{"paper_id": "W04-0843", "authors": ["Marc El-Beze", "Olivier Kraif", "Boxing Chen", "Gregoire Moreau De Montcheuil"], "last_author": "Gregoire Moreau De Montcheuil", "date": "2004", "title": "using a word sense disambiguation system for translation disambiguation the lialidilem team experiment", "workshop": "the Evaluation of Systems for the Semantic Analysis of Text", "combined_workshop": "semantics", "abstract": "This paper presents an original WSD method, based on a mixture of three algorithms working on the local context of target units. The results on the task    () of Senseval 3 were 60.3% of precision and recall for the T sub-task, and 64.1% for TS. We attempted to improve the method by constituting synonymlike classes from an English-French aligned corpus, but without any gain in the results."}
{"paper_id": "W00-1412", "authors": ["Christopher U. Habel", "Markus Guhe", "Heike Tappe"], "last_author": "Heike Tappe", "date": "2000", "title": "incremental event conceptualization and natural language generation in monitoring enviroments", "workshop": "INLG'2000 Proceedings of the First International Conference on Natural Language Generation", "combined_workshop": "nlg", "abstract": "In this paper we present a psycholinguistically motivated architecture and its prototypical implementation for an incremental conceptualizer, which monitors dynamic changes in the world and simultaneously generates warnings for (possibly) safety-critical developments. It does so by conceptualizing events and building up a hierarchical knowledge representation of the perceived states of affairs. If it detects a safety problem, it selects suitable elements from the representation for a warning, brings them into an appropriate order, and generates incremental preverbal messages (propositional structures) from them, which can be taken by a subsequent component to encode them linguistically."}
{"paper_id": "W04-2309", "authors": ["Kavita Thomas"], "last_author": "Kavita Thomas", "date": "2004", "title": "but what do they mean an exploration into the range of crossturn expectations denied by but", "workshop": "Discourse and Dialogue", "combined_workshop": "dialogue", "abstract": "In this paper we hypothesise that Denial of Expectation (DofE) across turns in dialogue signalled by “but” can involve a range of different expectations, i.e., not just causal expectations, as argued in the literature. We will argue for this hypothesis and outline a methodology to distinguish the relations these denied expectations convey. Finally we will demonstrate the practical utility of this hypothesis by showing how it can improve generation of appropriate responses to DofE and decrease the likelihood of misunderstandings based on incorrectly interpreting these underlying cross-speaker relations."}
{"paper_id": "W11-1406", "authors": ["R Downey", "D Rubin", "J Cheng", "J Bernstein"], "last_author": "J Bernstein", "date": "2011", "title": "performance of automated scoring for children s oral reading", "workshop": "Innovative Use of NLP for Building Educational Applications", "combined_workshop": "educational_applications", "abstract": "For adult readers, an automated system can produce oral reading fluency (ORF) scores (e.g., words read correctly per minute) that are consistent with scores provided by human evaluators (Balogh et al., 2005, and in press). Balogh’s work on NAAL materials used passage-specific data to optimize statistical language models and scoring performance. The current study investigates whether or not an automated system can produce scores for young children’s reading that are consistent with human scores. A novel aspect of the present study is that text-independent rule-based language models were employed (Cheng and Townshend, 2009) to score reading passages that the system had never seen before. Oral reading performances were collected over cell phones from 1st, 2nd, and 3rd grade children (n = 95) in a classroom environment. Readings were scored 1) in situ by teachers in the classroom, 2) later by expert scorers, and 3) by an automated system. Statistical analyses provide evidence that machine Words Correct scores correlate well with scores provided by teachers and expert scorers, with all (Pearson’s correlation coefficient) r’s > 0.98 at the individual response level, and all r’s > 0.99 at the “test” level (i.e., median scores out of 3)."}
{"paper_id": "W10-2704", "authors": ["Carole Adam", "Lawrence Cavedon", "Lin Padgham"], "last_author": "Lin Padgham", "date": "2010", "title": "x201chello emily how are you todayx201d personalised dialogue in a toy to engage children", "workshop": "Companionable Dialogue Systems", "combined_workshop": "dialogue", "abstract": "In line with the growing interest in conversational agents as companions, we are developing a toy companion for children that is capable of engaging interactions and of developing a long-term relationship with them, and is extensible so as to evolve with them. In this paper, we investigate the importance of personalising interaction both for engagement and for long-term relationship development. In particular, we propose a framework for representing, gathering and using personal knowledge about the child during dialogue interaction. 1"}
{"paper_id": "W11-2044", "authors": ["Alysa Taylor", "Jillian Gerten", "Sudeep Gandhe", "David R. Traum"], "last_author": "David R. Traum", "date": "2011", "title": "rapid development of advanced questionanswering characters by nonexperts", "workshop": "Proceedings of the SIGDIAL Conference", "combined_workshop": "dialogue", "abstract": "We demonstrate a dialogue system and the accompanying authoring tools that are designed to allow authors with little or no experience in building dialogue systems to rapidly build advanced question-answering characters. To date seven such virtual characters have been built by non-experts using this architecture and tools. Here we demonstrate one such character, PFC Sean Avery, which was developed by a non-expert in 3 months."}
{"paper_id": "W14-2410", "authors": ["M Roth", "T Diamantopoulos", "E Klein", "A Symeonidis"], "last_author": "A Symeonidis", "date": "2014", "title": "software requirements a new domain for semantic parsers", "workshop": "Semantic Parsing", "combined_workshop": "semantics", "abstract": "Software requirements are commonly written in natural language, making them prone to ambiguity, incompleteness and inconsistency. By converting requirements to formal semantic representations, emerging problems can be detected at an early stage of the development process, thus reducing the number of ensuing errors and the development costs. In this paper, we treat the mapping from requirements to formal representations as a semantic parsing task. We describe a novel data set for this task that involves two contributions: first, we establish an ontology for formally representing requirements; and second, we introduce an iterative annotation scheme, in which formal representations are derived through step-wise refinements."}
{"paper_id": "W03-0409", "authors": ["Diane J. Litman", "Mihai Rotaru"], "last_author": "Mihai Rotaru", "date": "2003", "title": "exceptionality and natural language learning", "workshop": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL", "combined_workshop": "natural_language_learning", "abstract": "Previous work has argued that memory-based learning is better than abstraction-based learning for a set of language learning tasks. In this paper, we first attempt to generalize these results to a new set of language learning tasks from the area of spoken dialog systems and to a different abstraction-based learner. We then examine the utility of various exceptionality measures for predicting where one learner is better than the other. Our results show that generalization of previous results to our tasks is not so obvious and some of the exceptionality measures may be used to characterize the performance of our learners."}
{"paper_id": "W05-1302", "authors": ["Benjamin Wellner", "James Pustejovsky", "Jose M. Castano"], "last_author": "Jose M. Castano", "date": "2005", "title": "adaptive string similarity metrics for biomedical reference resolution", "workshop": "Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics", "combined_workshop": "bio_nlp", "abstract": "In this paper we present the evaluation of a set of string similarity metrics used to resolve the mapping from strings to concepts in the UMLS MetaThesaurus. String similarity is conceived as a single component in a full Reference Resolution System that would resolve such a mapping. Given this qualification, we obtain positive results achieving 73.6 F-measure (76.1 precision and 71.4 recall) for the task of assigning the correct UMLS concept to a given string. Our results demonstrate that adaptive string similarity methods based on Conditional Random Fields outperform standard metrics in this domain."}
{"paper_id": "W14-1807", "authors": ["Gaurav Kharkwal", "Smaranda Muresan"], "last_author": "Smaranda Muresan", "date": "2014", "title": "surprisal as a predictor of essay quality", "workshop": "Innovative Use of NLP for Building Educational Applications", "combined_workshop": "educational_applications", "abstract": "Modern automated essay scoring systems rely on identifying linguistically-relevant features to estimate essay quality. This paper attempts to bridge work in psycholinguistics and natural language processing by proposing sentence processing complexity as a feature for automated essay scoring, in the context of English as a Foreign Language (EFL). To quantify processing complexity we used a psycholinguistic model called surprisal theory. First, we investigated whether essays’ average surprisal values decrease with EFL training. Preliminary results seem to support this idea. Second, we investigated whether surprisal can be effective as a predictor of essay quality. The results indicate an inverse correlation between surprisal and essay scores. Overall, the results are promising and warrant further investigation on the usability of surprisal for essay scoring."}
{"paper_id": "W05-1617", "authors": ["I Androutsopoulos", "S Kallonis", "V Karkaletsis"], "last_author": "V Karkaletsis", "date": "2005", "title": "exploiting owl ontologies in the multilingual generation of object descriptions", "workshop": "Natural Language Generation (ENLG-05)", "combined_workshop": "nlg", "abstract": "We present three ways in which a natural language generator that produces textual descriptions of objects from symbolic information can exploit OWL ontologies, using M-PIRO’s multilingual generation system as a concrete example."}
{"paper_id": "W11-0402", "authors": ["C Chiarcos", "T Erjavec"], "last_author": "T Erjavec", "date": "2011", "title": "owldl formalization of the multexteast morphosyntactic specifications", "workshop": "Linguistic Annotation Workshop", "combined_workshop": "annotation", "abstract": "This paper describes the modeling of the morphosyntactic annotations of the MULTEXT-East corpora and lexicons as an OWL/DL ontology. Formalizing annotation schemes in OWL/DL has the advantages of enabling formally specifying interrelationships between the various features and making logical inferences based on the relationships between them. We show that this approach provides us with a top-down perspective on a large set of morphosyntactic specifications for multiple languages, and that this perspective helps to identify and to resolve conceptual problems in the original specifications. Furthermore, the ontological modeling allows us to link the MULTEXT-East specifications with repositories of annotation terminology such as the General Ontology of Linguistics Descriptions or the ISO TC37/SC4 Data Category Registry."}
{"paper_id": "W10-4304", "authors": ["R Higashinaka", "Y Minami", "K Dohsaka", "T Meguro"], "last_author": "T Meguro", "date": "2010", "title": "modeling user satisfaction transitions in dialogues from overall ratings", "workshop": "Proceedings of the SIGDIAL Conference", "combined_workshop": "dialogue", "abstract": "This paper proposes a novel approach for predicting user satisfaction transitions during a dialogue only from the ratings given to entire dialogues, with the aim of reducing the cost of creating reference ratings for utterances/dialogue-acts that have been necessary in conventional approaches. In our approach, we first train hidden Markov models (HMMs) of dialogue-act sequences associated with each overall rating. Then, we combine such rating-related HMMs into a single HMM to decode a sequence of dialogueacts into state sequences representing to which overall rating each dialogue-act is most related, which leads to our rating predictions. Experimental results in two dialogue domains show that our approach can make reasonable predictions; it significantly outperforms a baseline and nears the upper bound of a supervised approach in some evaluation criteria. We also show that introducing states that represent dialogue-act sequences that occur commonly in all ratings into an HMM significantly improves prediction accuracy."}
{"paper_id": "W02-1610", "authors": ["Benoit Lavoie", "Michael White", "Tanya Korelsky"], "last_author": "Tanya Korelsky", "date": "2002", "title": "learning domainspecific transfer rules an experiment with korean to english translation", "workshop": "COLING-02: Machine Translation in Asia", "combined_workshop": "machine_translation", "abstract": "We describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results. The system learns lexico-structural transfer rules using syntactic pattern matching, statistical co-occurrence and errordriven filtering. In an experiment with domainspecific Korean to English translation, the approach yielded substantial improvements over three baseline systems."}
{"paper_id": "W11-2139", "authors": ["Chris Dyer", "Kevin Gimpel", "Jonathan H Clark", "Noah A Smith"], "last_author": "Noah A Smith", "date": "2011", "title": "the cmuark germanenglish translation system", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper describes the German-English translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation (WMT11). We present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training."}
{"paper_id": "W09-0604", "authors": ["Walter Daelemans", "Iris Hendrickx", "Emiel Krahmer", "Erwin Marsi"], "last_author": "Erwin Marsi", "date": "2009", "title": "is sentence compression an nlg task", "workshop": "NLG", "combined_workshop": "nlg", "abstract": "Data-driven approaches to sentence compression define the task as dropping any subset of words from the input sentence while retaining important information and grammaticality. We show that only 16% of the observed compressed sentences in the domain of subtitling can be accounted for in this way. We argue that part of this is due to evaluation issues and estimate that a deletion model is in fact compatible with approximately 55% of the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work."}
{"paper_id": "W13-2209", "authors": ["Alexey Borisov", "Jacob Dlougach", "Irina Galinskaya"], "last_author": "Irina Galinskaya", "date": "2013", "title": "yandex school of data analysis machine translation systems for wmt13", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper describes the English-Russian and Russian-English statistical machine translation (SMT) systems developed at Yandex School of Data Analysis for the shared translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation. We adopted phrase-based SMT approach and evaluated a number of different techniques, including data filtering, spelling correction, alignment of lemmatized word forms and transliteration. Altogether they yielded +2.0 and +1.5 BLEU improvement for ru-en and enru language pairs. We also report on the experiments that did not have any positive effect and provide an analysis of the problems we encountered during the development of our systems."}
{"paper_id": "W13-1715", "authors": ["Baoli Li"], "last_author": "Baoli Li", "date": "2013", "title": "recognizing english learners native language from their writings", "workshop": "Innovative Use of NLP for Building Educational Applications", "combined_workshop": "educational_applications", "abstract": "Native Language Identification (NLI), which tries to identify the native language (L1) of a second language learner based on their writings, is helpful for advancing second language learning and authorship profiling in forensic linguistics. With the availability of relevant data resources, much work has been done to explore the native language of a foreign language learner. In this report, we present our system for the first shared task in Native Language Identification (NLI). We use a linear SVM classifier and explore features of words, word and character n-grams, style, and metadata. Our official system achieves accuracy of 0.773, which ranks it 18th among the 29 teams in the closed track."}
{"paper_id": "W90-0112", "authors": ["Owen Rambow"], "last_author": "Owen Rambow", "date": "1990", "title": "domain communication knowledge", "workshop": "NLG", "combined_workshop": "nlg", "abstract": "This paper advances the hypothesis that any text planning task relies, explicitly or implicitly, on domainspecific text planning knowledge. This knowledge, &quot;domain communication knowledge&quot;, is different from both domain knowledge and general knowledge about communication. The paper presents the text generation system Joyce, which represents such knowledge explicitly."}
{"paper_id": "W14-3411", "authors": ["A Savkov", "J Carroll", "J Cassell"], "last_author": "J Cassell", "date": "2014", "title": "chunking clinical text containing noncanonical language ", "workshop": "Proceedings of BioNLP Workshop", "combined_workshop": "bio_nlp", "abstract": "Free text notes typed by primary care physicians during patient consultations typically contain highly non-canonical language. Shallow syntactic analysis of free text notes can help to reveal valuable information for the study of disease and treatment. We present an exploratory study into chunking such text using offthe-shelf language processing tools and pre-trained statistical models. We evaluate chunking accuracy with respect to partof-speech tagging quality, choice of chunk representation, and breadth of context features. Our results indicate that narrow context feature windows give the best results, but that chunk representation and minor differences in tagging quality do not have a significant impact on chunking accuracy."}
{"paper_id": "W06-1403", "authors": ["M White"], "last_author": "M White", "date": "2006", "title": "ccg chart realization from disjunctive inputs", "workshop": "Proceedings of the Fourth International Natural Language Generation Conference", "combined_workshop": "nlg", "abstract": "This paper presents a novel algorithm for efficiently generating paraphrases from disjunctive logical forms. The algorithm is couched in the framework of Combinatory Categorial Grammar (CCG) and has been implemented as an extension to the OpenCCG surface realizer. The algorithm makes use of packed representations similar to those initially proposed by Shemtov (1997), generalizing the approach in a more straightforward way than in the algorithm ultimately adopted therein."}
{"paper_id": "W08-0317", "authors": ["Sara Stymne", "Maria Holmqvist", "Lars Ahrenberg"], "last_author": "Lars Ahrenberg", "date": "2008", "title": "effects of morphological analysis in translation between german and english", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "We describe the LIU systems for GermanEnglish and English-German translation submitted to the Shared Task of the Third Workshop of Statistical Machine Translation. The main features of the systems, as compared with the baseline, is the use of morphological pre- and post-processing, and a sequence model for German using morphologically rich parts-of-speech. It is shown that these additions lead to improved translations."}
{"paper_id": "W04-0405", "authors": ["Masahito Takahashi", "Kosho Shudo", "Kenji Yoshimura", "Toshifumi Tanabe"], "last_author": "Toshifumi Tanabe", "date": "2004", "title": "mwes as nonpropositional content indicators", "workshop": "Multiword Expressions", "combined_workshop": "multiword_expressions", "abstract": "We report that a proper employment of MWEs concerned enables us to put forth a tractable framework, which is based on a multiple nesting of semantic operations, for the processing of non-inferential, Nonpropositional Contents (NPCs) of natural Japanese sentences. Our framework is characterized by its broad syntactic and semantic coverage, enabling us to deal with multiply composite modalities and their semantic/pragmatic similarity. Also, the relationship between indirect (Searle, 1975) and direct speech, and equations peculiar to modal logic and its family (Mally, 1926; Prior, 1967) are treated in the similarity paradigm."}
{"paper_id": "W11-1902", "authors": ["Heeyoung Lee", "Yves Peirsman", "Angel Chang", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky"], "last_author": "Dan Jurafsky", "date": "2011", "title": "stanford s multipass sieve coreference resolution system at the conll2011 shared task", "workshop": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task", "combined_workshop": "natural_language_learning", "abstract": "This paper details the coreference resolution system submitted by Stanford at the CoNLL2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track."}
{"paper_id": "W11-0809", "authors": ["Matthieu Constant", "Anthony Sigogne"], "last_author": "Anthony Sigogne", "date": "2011", "title": "mwuaware partofspeech tagging with a crf model and lexical resources", "workshop": "Multiword Expressions", "combined_workshop": "multiword_expressions", "abstract": "This paper describes a new part-of-speech tagger including multiword unit (MWU) identification. It is based on a Conditional Random Field model integrating language-independent features, as well as features computed from external lexical resources. It was implemented in a finite-state framework composed of a preliminary finite-state lexical analysis and a CRF decoding using weighted finitestate transducer composition. We showed that our tagger reaches state-of-the-art results for French in the standard evaluation conditions (i.e. each multiword unit is already merged in a single token). The evaluation of the tagger integrating MWU recognition clearly shows the interest of incorporating features based on MWU resources."}
{"paper_id": "W13-1733", "authors": ["Gabriel Illouz", "Aurelien Max", "Thomas Lavergne", "Ryo Nagata"], "last_author": "Ryo Nagata", "date": "2013", "title": "limsi s participation to the 2013 shared task on native language identification", "workshop": "Innovative Use of NLP for Building Educational Applications", "combined_workshop": "educational_applications", "abstract": "This paper describes LIMSI’s participation to the first shared task on Native Language Identification. Our submission uses a Maximum Entropy classifier, using as features character and chunk n-grams, spelling and grammatical mistakes, and lexical preferences. Performance was slightly improved by using a twostep classifier to better distinguish otherwise easily confused native languages."}
{"paper_id": "W02-1104", "authors": ["Pascale Fung"], "last_author": "Pascale Fung", "date": "2002", "title": "semantic networks the path to profitability", "workshop": "COLING-02: SEMANET: Building and Using Semantic Networks", "combined_workshop": "semantics", "abstract": "In recent years, we have seen a surge of &quot;Question and Answer&quot; Systems (Answer Logic, iPhrase, Lingway, Albert, Baobab, Weniwen), triggered by the phenomenal IPO success of Ask Jeeves (www.ask.com). For the first time in a long time, linguists were being chased by professional headhunters and were commanding sky-high salaries. Then it all came down. Or did it? Surprisingly, there appears to be another &quot;surge&quot; of Q&A companies. The message is a bit confusing. Such systems could make a lot of money, but then maybe not, but then again they ought to, etc. etc. In any case, ontology plays a critical role in such systems. Some systems are based on an ontology/semantic network that is automatically &quot;adaptable&quot; by training on domain-specific data. Others make use of expert-system-like hand-written rules. The difference is not obvious at a demo stage, but emerges more clearly during deployment. The pros and cons of empirical methods vs rules has been argued to death in the 1990s in academia. This talk will discuss how this debate is currently unfolding in the commercial arena."}
{"paper_id": "W04-1218", "authors": ["Marc Rossler"], "last_author": "Marc Rossler", "date": "2004", "title": "adapting an nersystem for german to the biomedical domain", "workshop": "Proceedings of BioNLP Workshop", "combined_workshop": "bio_nlp", "abstract": "In this paper, we report the adaptation of a named entity recognition (NER) system to the biomedical domain in order to participate in the ”Shared Task Bio-Entity Recognition”. The system is originally developed for German NER that shares characteristics with the biomedical task. To facilitate adaptability, the system is knowledge-poor and utilizes unlabeled data. Investigating the adaptability of the single components and the enhancements necessary, we get insights into the task of bioentity recognition."}
{"paper_id": "W08-2127", "authors": ["Hai Zhao", "Chunyu Kit"], "last_author": "Chunyu Kit", "date": "2008", "title": "parsing syntactic and semantic dependencies with two singlestage maximum entropy models", "workshop": "CoNLL", "combined_workshop": "natural_language_learning", "abstract": "This paper describes our system to carry out the joint parsing of syntactic and semantic dependencies for our participation in the shared task of CoNLL-2008. We illustrate that both syntactic parsing and semantic parsing can be transformed into a word-pair classification problem and implemented as a single-stage system with the aid of maximum entropy modeling. Our system ranks the fourth in the closed track for the task with the following performance on the WSJ+Brown test set: 81.44% labeled macro F1 for the overall task, 86.66% labeled attachment for syntactic dependencies, and 76.16% labeled F1 for semantic dependencies."}
{"paper_id": "W12-3152", "authors": ["Matt Post", "Chris Callison-Burch", "Miles Osborne"], "last_author": "Miles Osborne", "date": "2012", "title": "constructing parallel corpora for six indian languages via crowdsourcing", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "Recent work has established the efficacy of Amazon’s Mechanical Turk for constructing parallel corpora for machine translation research. We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent: Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu. These languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation. We conduct a variety of baseline experiments and analysis, and release the data to the community."}
{"paper_id": "W13-3605", "authors": ["J Xing", "L Wang", "D F Wong", "L S Chao", "X Zeng"], "last_author": "X Zeng", "date": "2013", "title": "umchecker a hybrid system for english grammatical error correction", "workshop": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task", "combined_workshop": "natural_language_learning", "abstract": "This paper describes the NLP2CT Grammatical Error Detection and Correction system for the CoNLL 2013 shared task, with a focus on the errors of article or determiner (ArtOrDet), noun number (In), preposition (Prep), verb form (Vform) and subject-verb agreement (SVA). A hybrid model is adopted for this special task. The process starts with spellchecking as a preprocessing step to correct any possible erroneous word. We used a Maximum Entropy classifier together with manually rule-based filters to detect the grammatical errors in English. A language model based on the Google I-gram corpus was employed to select the best correction candidate from a confusion matrix. We also explored a graphbased label propagation approach to overcome the sparsity problem in training the model. Finally, a number of deterministic rules were used to increase the precision and recall. The proposed model was evaluated on the test set consisting of 50 essays and with about 500 words in each essay. Our system achieves the 5th and 3rd F1 scores on official test set among all 17 participating teams based on goldstandard edits before and after revision, respectively."}
{"paper_id": "W00-1403", "authors": ["Maki Watanabe", "Lynn Carlson", "Daniel Marcu"], "last_author": "Daniel Marcu", "date": "2000", "title": "an empirical study of multilingual natural language generation what should a text planner do", "workshop": "INLG'2000 Proceedings of the First International Conference on Natural Language Generation", "combined_workshop": "nlg", "abstract": "We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations. We discuss implications of our empirical findings for the task of text planning in the context of implementing multilingual natural language generation systems."}
{"paper_id": "W04-0825", "authors": ["Eric Crestan"], "last_author": "Eric Crestan", "date": "2004", "title": "contextual semantics for wsd", "workshop": "the Evaluation of Systems for the Semantic Analysis of Text", "combined_workshop": "semantics", "abstract": "For Sinequa’s second participation to the Senseval evaluation, two systems using contextual semantic have been proposed. Based on different approaches, they both share the same data preprocessing and enrichment. The first system is a combined approach using semantic classification trees and information retrieval techniques. For the second system, the words from the context are considered as clues. The final sense is determined by summing the weight assigned to each clue for a given example."}
{"paper_id": "W00-1220", "authors": ["Xiaodan Zhu", "Chunfa Yuan", "Kam-Fai Wong", "Wenjie Li"], "last_author": "Wenjie Li", "date": "2000", "title": "an algorithm for situation classification of chinese verbs", "workshop": "Chinese Language Processing", "combined_workshop": "chinese_language", "abstract": "Temporal information analysis is very important for Chinese Information Process. Comparing with English, Chinese is quite different in temporal information expression. Based on the feature of Chinese a phase-based method is proposed to deal with Chinese temporal information. To this end, an algorithm is put forward to classify verbs into different situation types automatically. About 2981 verbs were tested. The result has shown that the algorithm is effective."}
{"paper_id": "W11-2107", "authors": ["Michael Denkowski", "Alon Lavie"], "last_author": "Alon Lavie", "date": "2011", "title": "meteor 13 automatic metric for reliable optimization and evaluation of machine translation systems", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system."}
{"paper_id": "W04-1106", "authors": ["Chao-Jan Chen"], "last_author": "Chao-Jan Chen", "date": "2004", "title": "charactersense association and compounding template similarity automatic semantic classification of chinese compounds", "workshop": "Chinese Language Processing", "combined_workshop": "chinese_language", "abstract": "This paper presents a character-based model of automatic sense determination for Chinese compounds. The model adopts a sense approximation approach using synonymous compounds retrieved by measuring similarity of semantic template in compounding. The similarity measure is derived from an association network among characters and senses, which is built from a formatted MRD. Adopting the taxonomy of CILIN, a system of deep semantic classification (at least to the small classes) for V-V compounds is implemented and evaluated to test the model. The experiment reports a high precision rate (about 38% in outside test and 61% in inside test) against the baseline one (about 18%)."}
{"paper_id": "W13-1915", "authors": ["Raphael Cohen", "Michael Elhadad"], "last_author": "Michael Elhadad", "date": "2013", "title": "effect of out of vocabulary terms on inferring eligibility criteria for a retrospective study in hebrew ehr", "workshop": "Biomedical Natural Language Processing", "combined_workshop": "bio_nlp", "abstract": "The Electronic Health Record (EHR) contains information useful for clinical, epidemiological and genetic studies. This information of patient symptoms, history, medication and treatment is not completely captured in the structured part of the EHR but is often found in the form of freetext narrative. A major obstacle for clinical studies is finding patients that fit the eligibility criteria of the study. Using EHR in order to automatically identify relevant cohorts can help speed up both clinical trials and retrospective studies (Restificar, Korkontzelos et al. 2013). While the clinical criteria for inclusion and exclusion from the study are explicitly stated in most studies, automating the process using the EHR database of the hospital is often impossible as the structured part of the database (age, gender, ICD9/10 medical codes, etc.’) rarely covers all of the criteria. Many resources such as UMLS (Bodenreider 2004), cTakes (Savova, Masanz et al. 2010), MetaMap (Aronson and Lang 2010) and recently richly annotated corpora and treebanks (Albright, Lanfranchi et al. 2013) are available for processing and representing medical texts in English. Resource poor languages, however, suffer from lack in NLP tools and medical resources. Dictionaries exhaustively mapping medical terms to the UMLS medical meta-thesaurus are only available in a limited number of languages besides English. NLP annotation tools, when they exist for resource poor languages, suffer from heavy loss of accuracy when used outside the domain on which they were trained, as is well documented for English (Tsuruoka, Tateishi et al. 2005; Tateisi, Tsuruoka et al. 2006). In this work we focus on the problem of classifying patient eligibility for inclusion in retrospective study of the epidemiology of epilepsy in Southern Israel. Israel has a centralized structure of medical services which include advanced EHR systems. However, the free text sections of these EHR are written in Hebrew, a resource poor language in both NLP tools and handcrafted medical vocabularies. Epilepsy is a common chronic neurologic disorder characterized by seizures. These seizures are transient signs and/or symptoms of abnormal, excessive, or hyper synchronous neuronal activity in the brain. Epilepsy is one of the most common of the serious neurological disorders (Hirtz, Thurman et al. 2007)."}
{"paper_id": "W10-1010", "authors": ["Xiaoming Xi", "Lei Chen", "Joel R. Tetreault"], "last_author": "Joel R. Tetreault", "date": "2010", "title": "towards using structural events to assess nonnative speech", "workshop": "Innovative Use of NLP for Building Educational Applications", "combined_workshop": "educational_applications", "abstract": "We investigated using structural events, e.g., clause and disfluency structure, from transcriptions of spontaneous non-native speech, to compute features for measuring speaking proficiency. Using a set of transcribed audio files collected from the TOEFL Practice Test Online (TPO), we conducted a sophisticated annotation of structural events, including clause boundaries and types, as well as disfluencies. Based on words and the annotated structural events, we extracted features related to syntactic complexity, e.g., the mean length of clause (MLC) and dependent clause frequency (DEPC), and a feature related to disfluencies, the interruption point frequency per clause (IPC). Among these features, the IPC shows the highest correlation with holistic scores (r = −0.344). Furthermore, we increased the correlation with human scores by normalizing IPC by (1) MLC (r = −0.386), (2) DEPC (r = −0.429), and (3) both (r = −0.462). In this research, the features derived from structural events of speech transcriptions are found to predict holistic scores measuring speaking proficiency. This suggests that structural events estimated on speech word strings provide a potential way for assessing nonnative speech."}
{"paper_id": "W11-2134", "authors": ["Vera Aleksi´c", "Gregor Thurmair"], "last_author": "Gregor Thurmair", "date": "2011", "title": "personal translator at wmt2011 a rulebased mt system with hybrid components", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper presents the Linguatec submission to the WMT 2011 sixth workshop on statistical machine translation. It describes the architecture of our machine translation system ‘Personal Translator’ (hereinafter also referred to as PT), developed by Linguatec, which is a rule-based translation system, enriched by statistical approaches. We participate for the German-English translation direction. For the current submission we have chosen the latest commercial version of the system, PT14. The translation quality improvement for the submission was done mainly by lexicon tuning: detection of unknown words, extracting of possible translations, partly from the wmt11 training corpora, and enlarging the lexicon by manually coding the chosen transfer candidates."}
{"paper_id": "W13-2806", "authors": ["Katsuhito Sudoh", "Masaaki Nagata", "Dan Han", "Yusuke Miyao", "Pascual Martinez-Gomez"], "last_author": "Pascual Martinez-Gomez", "date": "2013", "title": "using unlabeled dependency parsing for prereordering for chinesetojapanese statistical machine translation", "workshop": "Hybrid Approaches to Translation", "combined_workshop": "machine_translation", "abstract": "Chinese and Japanese have a different sentence structure. Reordering methods are effective, but need reliable parsers to extract the syntactic structure of the source sentences. However, Chinese has a loose word order, and Chinese parsers that extract the phrase structure do not perform well. We propose a framework where only POS tags and unlabeled dependency parse trees are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. We show significant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods."}
{"paper_id": "W98-1419", "authors": ["M Stone", "B Webber"], "last_author": "B Webber", "date": "1998", "title": "textual economy through close coupling of syntax and semantics", "workshop": "NLG", "combined_workshop": "nlg", "abstract": "We focus on the production of efficient descriptions of objects, actions and events. We define a type of efficiency, textual economy, that exploits the hearer's recognition of inferential links to material elsewhere within a sentence. Textual economy leads to efficient descriptions because the material that supports such inferences has been included to satisfy independent communicative goals, and is therefore overloaded in the sense of Pollack [18]. We argue that achieving textual economy imposes strong requirements on the representation and reasoning used in generating sentences. The representation must support the generator's simultaneous consideration of syntax and semantics. Reasoning must enable the generator to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its (incomplete) syntax and semantics. We show that these representational and reasoning requirements are met in the SPUD system for sentence planning and realization."}
{"paper_id": "W08-2117", "authors": ["John Lee"], "last_author": "John Lee", "date": "2008", "title": "a nearestneighbor approach to the automatic analysis of ancient greek morphology", "workshop": "CoNLL", "combined_workshop": "natural_language_learning", "abstract": "We propose a data-driven method for automatically analyzing the morphology of ancient Greek. This method improves on existing ancient Greek analyzers in two ways. First, through the use of a nearestneighbor machine learning framework, the analyzer requires no hand-crafted rules. Second, it is able to predict novel roots, and to rerank its predictions by exploiting a large, unlabelled corpus of ancient Greek."}
{"paper_id": "W09-3938", "authors": ["Fabrice Lefevre", "Filip Jurcicek", "Blaise Thomson", "Francois Mairesse", "Simon Keizer", "Steve Young", "Milica Gasic", "Kai Yu"], "last_author": "Kai Yu", "date": "2009", "title": "knearest neighbor montecarlo control algorithm for pomdpbased dialogue systems", "workshop": "Proceedings of the SIGDIAL Conference", "combined_workshop": "dialogue", "abstract": "In real-world applications, modelling dialogue as a POMDP requires the use of a summary space for the dialogue state representation to ensure tractability. Suboptimal estimation of the value function governing the selection of system responses can then be obtained using a gridbased approach on the belief space. In this work, the Monte-Carlo control technique is extended so as to reduce training over-fitting and to improve robustness to semantic noise in the user input. This technique uses a database of belief vector prototypes to choose the optimal system action. A locally weighted k-nearest neighbor scheme is introduced to smooth the decision process by interpolating the value function, resulting in higher user simulation performance."}
{"paper_id": "W06-0102", "authors": ["Benjamin K. T'sou", "Oi Yee Kwong"], "last_author": "Oi Yee Kwong", "date": "2006", "title": "regional variation of domainspecific lexical items toward a panchinese lexical resource", "workshop": "Chinese Language Processing", "combined_workshop": "chinese_language", "abstract": "This paper reports on an initial and necessary step toward the construction of a Pan-Chinese lexical resource. We investigated the regional variation of lexical items in two specific domains, finance and sports; and explored how much of such variation is covered in existing Chinese synonym dictionaries, in particular the Tongyici Cilin. The domain-specific lexical items were obtained from subsections of a synchronous Chinese corpus, LIVAC. Results showed that 20-40% of the words from various subcorpora are unique to the individual communities, and as much as 70% of such unique items are not yet covered in the Tongyici Cilin. The results suggested great potential for building a Pan-Chinese lexical resource for Chinese language processing. Our next step would be to explore automatic means for extracting related lexical items from the corpus, and to incorporate them into existing semantic classifications."}
{"paper_id": "W09-3734", "authors": ["Sumiyo Nishiguchi"], "last_author": "Sumiyo Nishiguchi", "date": "2009", "title": "qualiabased lexical knowledge for the disambiguation of the japanese postposition no", "workshop": "International Conference on Computational Semantics (IWCS)", "combined_workshop": "semantics", "abstract": "This paper proposes the elaboration of the qualia structure of the Generative Lexicon in [5] and the Extended Generative Lexicon theory [3]. My proposal is based on the Japanese genitive postposition no. The Japanese “NP1-no NP2&quot; construction expresses a wider range of relations between two entities than does the English possessive “NP1's NP2,&quot; such that the Pustejovskian qualia roles encoded in NP2 do not supply the necessary relations between two entities, which [7] succeeded to certain degree. Possessive relation disambiguation requires enriching lexical entries by incorporating the HAVE relation into the CONSTITUTIVE role and listing other qualia such as the ACTIVITY and the SPATIO-TEMPORAL role with the subcategories of LOCATION and TIME."}
{"paper_id": "W11-2148", "authors": ["Chang Hu", "Philip Resnik", "Yakov Kronrod", "Vladimir Eidelman", "Olivia Buzek", "Benjamin B Bederson"], "last_author": "Benjamin B Bederson", "date": "2011", "title": "the value of monolingual crowdsourcing in a realworld translation scenario simulation using haitian creole emergency sms messages", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "MonoTrans2 is a translation system that combines machine translation (MT) with human computation using two crowds of monolingual source (Haitian Creole) and target (English) speakers. We report on its use in the WMT 2011 Haitian Creole to English translation task, showing that MonoTrans2 translated 38% of the sentences well compared to Google Translate’s 25%."}
{"paper_id": "W13-1902", "authors": ["Meliha Yetisgen-Yildiz", "Mark Wurfel", "Cosmin Adrian Bejan"], "last_author": "Cosmin Adrian Bejan", "date": "2013", "title": "identification of patients with acute lung injury from freetext chest xray reports", "workshop": "Biomedical Natural Language Processing", "combined_workshop": "bio_nlp", "abstract": "Identification of complex clinical phenotypes among critically ill patients is a major challenge in clinical research. The overall research goal of our work is to develop automated approaches that accurately identify critical illness phenotypes to prevent the resource intensive manual abstraction approach. In this paper, we describe a text processing method that uses Natural Language Processing (NLP) and supervised text classification methods to identify patients who are positive for Acute Lung Injury (ALI) based on the information available in free-text chest x-ray reports. To increase the classification performance we enhanced the baseline unigram representation with bigram and trigram features, enriched the n-gram features with assertion analysis, and applied statistical feature selection. We used 10-fold cross validation for evaluation and our best performing classifier achieved 81.70% precision (positive predictive value), 75.59% recall (sensitivity), 78.53% f-score, 74.61% negative predictive value, 76.80% specificity in identifying patients with ALI."}
{"paper_id": "W01-0725", "authors": ["Molina", "Ferran Pla"], "last_author": "Ferran Pla", "date": "2001", "title": "clause detection using hmm", "workshop": "CoNLL", "combined_workshop": "natural_language_learning", "abstract": "In this work, we apply a specialized HMM approach to the shared task: clause identification (Tjong Kim Sang and Dejean, 2001). The HMM formalism (Rabiner and Juang, 1986) has widely been used to solve other NLP problems, such as POS tagging, chunking, partial parsing, etc. A similar technique (Lexicalized HMM), that takes into account certain words to lexicalize the contextual language model, was previously applied for solving POS tagging and chunking problems (Pla et al., 2000b). Usually, for these tasks, specialized HMMs perform better than non-specialized HMMs (Pla, 2000a). We used specialized HMMs to solve the three parts of the task. Part 1 (clause start detection) and Part 2 (clause end detection) are performed as a tagging problem, that is, we assign the more probable tag to each token of the input. Part 3 (embedded clause detection) is a more complex task because the output must be correctly balanced. Therefore, it is carried out in two phases: first, we find the best sequence of tags (segmentation in clauses) for the input sentence; second, we correct some balancing inconsistencies observed in the output by applying some rules."}
{"paper_id": "W04-0209", "authors": ["Maximiliano Saiz-Noeda", "Borja Navarro", "Ruben Izquierdo"], "last_author": "Ruben Izquierdo", "date": "2004", "title": "exploiting semantic information for manual anaphoric annotation in cast3lb corpus", "workshop": "Discourse Annotation", "combined_workshop": "annotation", "abstract": "This paper presents the discourse annotation followed in Cast3LB, a Spanish corpus annotated with several information sources (morphological, syntactic, semantic and coreferential) at syntactic, semantic and discourse level. 3LB annotation scheme has been developed for three languages (Spanish, Catalan and Basque). Human annotators have used a set of tagging techniques and protocols. Several tools have provided them with a friendly annotation scheme. At discourse level, anaphoric and coreference expressions are annotated. One of the most interesting contributions to this annotation scenario is the enriched anaphora resolution module that is based on the previously defined semantic annotation phase to expand the discourse information and use it to suggest the correct antecedent of an anaphora to the annotator. This paper describes the relevance of the semantic tags in the discourse annotation in Spanish corpus Cast3LB and shows both levels and tools in the mentioned discourse annotation scheme."}
{"paper_id": "W04-0853", "authors": ["B. Prithviraj", "Pushpak Bhattacharyya", "Ganesh Ramakrishnan"], "last_author": "Ganesh Ramakrishnan", "date": "2004", "title": "a glosscentered algorithm for disambiguation", "workshop": "the Evaluation of Systems for the Semantic Analysis of Text", "combined_workshop": "semantics", "abstract": "The task of word sense disambiguation is to assign a sense label to a word in a passage. We report our algorithms and experiments for the two tasks that we participated in viz. the task of WSD of WordNet glosses and the task of WSD of English lexical sample. For both the tasks, we explore a method of sense disambiguation through a process of “comparing” the current context for a word against a repository of contextual clues or glosses for each sense of each word. We compile these glosses in two different ways for the two tasks. For the first task, these glosses are all compiled using WordNet and are of various types viz. hypernymy glosses, holonymy mixture, descriptive glosses and some hybrid mixtures of these glosses. The “comparison” could be done in a variety of ways that could include/exclude stemming, expansion of one gloss type with another gloss type, etc. The results show that the system does best when stemming is used and glosses are expanded. However, it appears that the evidence for word-senses ,accumulated through WordNet, in the form of glosses, are quite sparse. Generating dense glosses for all WordNet senses requires a massive sense tagged corpus - which is currently unavailable. Hence, as part of the English lexical sample task, we try the same approach on densely populated glosses accumulated from the training data for this task."}
{"paper_id": "W11-2159", "authors": ["Sara Stymne"], "last_author": "Sara Stymne", "date": "2011", "title": "spell checking techniques for replacement of unknown words and data cleaning for haitian creole sms translation", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "We report results on translation of SMS messages from Haitian Creole to English. We show improvements by applying spell checking techniques to unknown words and creating a lattice with the best known spelling equivalents. We also used a small cleaned corpus to train a cleaning model that we applied to the noisy corpora."}
{"paper_id": "W05-1208", "authors": ["Ido Dagan", "Oren Glickman"], "last_author": "Oren Glickman", "date": "2005", "title": "a probabilistic setting and lexical coocurrence model for textual entailment", "workshop": "Empirical Modeling of Semantic Equivalence and Entailment", "combined_workshop": "semantics", "abstract": "This paper proposes a general probabilistic setting that formalizes a probabilistic notion of textual entailment. We further describe a particular preliminary model for lexical-level entailment, based on document cooccurrence probabilities, which follows the general setting. The model was evaluated on two application independent datasets, suggesting the relevance of such probabilistic approaches for entailment modeling."}
{"paper_id": "W12-0111", "authors": ["Andreas SA Kirkedal"], "last_author": "Andreas SA Kirkedal", "date": "2012", "title": "treebased hybrid machine translation", "workshop": "Hybrid Approaches to Translation (HyTra6)", "combined_workshop": "machine_translation", "abstract": "I present an automatic post-editing approach that combines translation systems which produce syntactic trees as output. The nodes in the generation tree and targetside SCFG tree are aligned and form the basis for computing structural similarity. Structural similarity computation aligns subtrees and based on this alignment, subtrees are substituted to create more accurate translations. Two different techniques have been implemented to compute structural similarity: leaves and tree-edit distance. I report on the translation quality of a machine translation (MT) system where both techniques are implemented. The approach shows significant improvement over the baseline for MT systems with limited training data and structural improvement for MT systems trained on Europarl."}
{"paper_id": "W13-2815", "authors": ["Michael Gasser", "Alex Rudnick"], "last_author": "Alex Rudnick", "date": "2013", "title": "lexical selection for hybrid mt with sequence labeling", "workshop": "Hybrid Approaches to Translation", "combined_workshop": "machine_translation", "abstract": "We present initial work on an inexpensive approach for building largevocabulary lexical selection modules for hybrid RBMT systems by framing lexical selection as a sequence labeling problem. We submit that Maximum Entropy Markov Models (MEMMs) are a sensible formalism for this problem, due to their ability to take into account many features of the source text, and show how we can build a combination MEMM/HMM system that allows MT system implementors flexibility regarding which words have their lexical choices modeled with classifiers. We present initial results showing successful use of this system both in translating English to Spanish and Spanish to Guarani."}
{"paper_id": "W10-4163", "authors": ["Zhenzhong Zhang", "Le Sun", "Qiang Dong"], "last_author": "Qiang Dong", "date": "2010", "title": "overview of the chinese word sense induction task at clp2010", "workshop": "Chinese Language Processing", "combined_workshop": "chinese_language", "abstract": "In this paper, we describe the Chinese word sense induction task at CLP2010. Seventeen teams participated in this task and nineteen system results were submitted. All participant systems are evaluated on a dataset containing 100 target words and 5000 instances using the standard cluster evaluation. We will describe the participating systems and the evaluation results, and then find the most suitable method by comparing the different Chinese word sense induction systems."}
{"paper_id": "W13-2219", "authors": ["Evgeny Matusov", "Gregor Leusch"], "last_author": "Gregor Leusch", "date": "2013", "title": "omnifluent englishtofrench and russiantoenglish systems for the 2013 workshop on statistical machine translation", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper describes OmnifluentTM Translate – a state-of-the-art hybrid MT system capable of high-quality, high-speed translations of text and speech. The system participated in the English-to-French and Russian-to-English WMT evaluation tasks with competitive results. The features which contributed the most to high translation quality were training data sub-sampling methods, document-specific models, as well as rule-based morphological normalization for Russian. The latter improved the baseline Russian-to-English BLEU score from 30.1 to 31.3% on a heldout test set."}
{"paper_id": "W14-3329", "authors": ["Tsuyoshi Okita", "Ali Hosseinzadeh Vahid", "Andy Way", "Qun Liu"], "last_author": "Qun Liu", "date": "2014", "title": "dcu terminology translation system for medical query subtask at wmt14", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper describes the Dublin City University terminology translation system used for our participation in the query translation subtask in the medical translation task in the Workshop on Statistical Machine Translation (WMT14). We deployed six different kinds of terminology extraction methods, and participated in three different tasks: FR–EN and EN– FR query tasks, and the CLIR task. We obtained 36.2 BLEU points absolute for FR–EN and 28.8 BLEU points absolute for EN–FR tasks where we obtained the first place in both tasks. We obtained 51.8 BLEU points absolute for the CLIR task."}
{"paper_id": "W06-3113", "authors": ["Nicola Bertoldi", "Marcello Federico"], "last_author": "Marcello Federico", "date": "2006", "title": "how many bits are needed to store probabilities for phrasebased translation", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "State of the art in statistical machine translation is currently represented by phrasebased models, which typically incorporate a large number of probabilities of phrase-pairs and word n-grams. In this work, we investigate data compression methods for efficiently encoding n-gram and phrase-pair probabilities, that are usually encoded in 32-bit floating point numbers. We measured the impact of compression on translation quality through a phrase-based decoder trained on two distinct tasks: the translation of European Parliament speeches from Spanish to English, and the translation of news agencies from Chinese to English. We show that with a very simple quantization scheme all probabilities can be encoded in just 4 bits with a relative loss in BLEU score on the two tasks by 1.0% and 1.6%, respectively."}
{"paper_id": "W08-0621", "authors": ["Wendy Chapman", "Danielle Mowery", "Henk Harkema"], "last_author": "Henk Harkema", "date": "2008", "title": "temporal annotation of clinical text", "workshop": "Current Trends in Biomedical Natural Language Processing", "combined_workshop": "bio_nlp", "abstract": "We developed a temporal annotation schema that provides a structured method to capture contextual and temporal features of clinical conditions found in clinical reports. In this poster we describe the elements of the annotation schema and provide results of an initial annotation study on a document set comprising six different types of clinical reports."}
{"paper_id": "W08-0907", "authors": ["James C. Lester", "Mladen Vouk", "Kristy Elizabeth Boyer", "Robert Phillips", "Michael Wallis"], "last_author": "Michael Wallis", "date": "2008", "title": "learner characteristics and feedback in tutorial dialogue", "workshop": "Innovative Use of NLP for Building Educational Applications", "combined_workshop": "educational_applications", "abstract": "Tutorial dialogue has been the subject of increasing attention in recent years, and it has become evident that empirical studies of human-human tutorial dialogue can contribute important insights to the design of computational models of dialogue. This paper reports on a corpus study of human-human tutorial dialogue transpiring in the course of problemsolving in a learning environment for introductory computer science. Analyses suggest that the choice of corrective tutorial strategy makes a significant difference in the outcomes of both student learning gains and selfefficacy gains. The findings reveal that tutorial strategies intended to maximize student motivational outcomes (e.g., self-efficacy gain) may not be the same strategies that maximize cognitive outcomes (i.e., learning gain). In light of recent findings that learner characteristics influence the structure of tutorial dialogue, we explore the importance of understanding the interaction between learner characteristics and tutorial dialogue strategy choice when designing tutorial dialogue systems."}
{"paper_id": "W12-3130", "authors": ["Bushra Jawaid", "Ondrej Bojar", "Amir Kamran"], "last_author": "Amir Kamran", "date": "2012", "title": "probes in a taxonomy of factored phrasebased models", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "We introduce a taxonomy of factored phrasebased translation scenarios and conduct a range of experiments in this taxonomy. We point out several common pitfalls when designing factored setups. The paper also describes our WMT12 submissions CU-BOJAR and CU-POOR-COMB."}
{"paper_id": "W10-4229", "authors": ["Sivaji Bandyopadhyay", "Tapabrata Mondal", "Amitava Das", "Tanik Saikh"], "last_author": "Tanik Saikh", "date": "2010", "title": "jucsegrec10 named entity generation at grec 2010", "workshop": "Proceedings of the 6th International Natural Language Generation Conference", "combined_workshop": "nlg", "abstract": "This paper presents the experiments carried out at Jadavpur University as part of the participation in the GREC Named Entity Generation Challenge 2010. The Baseline system is based on the SEMCAT, SYNCAT and SYNFUNC features of REF and REG08-TYPE and CASE features of REFEX elements. The discourse level system is based on the additional positional features: paragraph number, sentence number, word position in the sentence and mention number of a particular named entity in the document. The inclusion of discourse level features has improved the performance of the system."}
{"paper_id": "W10-1007", "authors": ["Adam Skory", "Maxine Eskenazi"], "last_author": "Maxine Eskenazi", "date": "2010", "title": "predicting cloze task quality for vocabulary training", "workshop": "Innovative Use of NLP for Building Educational Applications", "combined_workshop": "educational_applications", "abstract": "Computer generation of cloze tasks still falls short of full automation; most current systems are used by teachers as authoring aids. Improved methods to estimate cloze quality are needed for full automation. We investigated lexical reading difficulty as a novel automatic estimator of cloze quality, to which cooccurrence frequency of words was compared as an alternate estimator. Rather than relying on expert evaluation of cloze quality, we submitted open cloze tasks to workers on Amazon Mechanical Turk (AMT) and discuss ways to measure of the results of these tasks. Results show one statistically significant correlation between the above measures and estimators, which was lexical co-occurrence and Cloze Easiness. Reading difficulty was not found to correlate significantly. We gave subsets of cloze sentences to an English teacher as a gold standard. Sentences selected by co-occurrence and Cloze Easiness were ranked most highly, corroborating the evidence from AMT."}
{"paper_id": "W13-2253", "authors": ["Derek F. Wong", "Lidia S. Chao", "Liangye He", "Yiming Wang", "Jiaji Zhou", "Yi Lu", "Aaron Li-Feng Han"], "last_author": "Aaron Li-Feng Han", "date": "2013", "title": "a description of tunable machine translation evaluation systems in wmt13 metrics task", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper is to describe our machine translation evaluation systems used for participation in the WMT13 shared Metrics Task. In the Metrics task, we submitted two automatic MT evaluation systems nLEPOR_baseline and LEPOR_v3.1. nLEPOR_baseline is an n-gram based language independent MT evaluation metric employing the factors of modified sentence length penalty, position difference penalty, n-gram precision and n-gram recall. nLEPOR_baseline measures the similarity of the system output translations and the reference translations only on word sequences. LEPOR_v3.1 is a new version of LEPOR metric using the mathematical harmonic mean to group the factors and employing some linguistic features, such as the part-of-speech information. The evaluation results of WMT13 show LEPOR_v3.1 yields the highest averagescore 0.86 with human judgments at systemlevel using Pearson correlation criterion on English-to-other (FR, DE, ES, CS, RU) language pairs."}
{"paper_id": "W04-2304", "authors": ["Joakim Gustafson", "Linda Bell", "Johan Boye", "Anders Lindstr¨om", "Mats Wir´en"], "last_author": "Mats Wir´en", "date": "2004", "title": "the nice fairytale game system", "workshop": "Discourse and Dialogue", "combined_workshop": "dialogue", "abstract": "This paper presents the NICE fairy-tale game system, in which adults and children can interact with various animated characters in a 3D world. Computer games is an interesting application for spoken and multimodal dialogue systems. Moreover, for the development of future computer games, multimodal dialogue has the potential to greatly enrichen the user’s experience. In this paper, we also present some requirements that have to be fulfilled to successfully integrate spoken dialogue technology with a computer game application."}
{"paper_id": "W08-2231", "authors": ["Nils Reiter", "Matthias Hartung", "Anette Frank"], "last_author": "Anette Frank", "date": "2008", "title": "a resourcepoor approach for linking ontology classes to wikipedia articles", "workshop": "Semantics in Text Processing. STEP Conference Proceedings", "combined_workshop": "semantics", "abstract": "The applicability of ontologies for natural language processing depends on the ability to link ontological concepts and relations to their realisations in texts. We present a general, resource-poor account to create such a linking automatically by extracting Wikipedia articles corresponding to ontology classes. We evaluate our approach in an experiment with the Music Ontology. We consider linking as a promising starting point for subsequent steps of information extraction."}
{"paper_id": "W10-4148", "authors": ["Chengjie Sun", "Yong Cheng", "Bingquan Liu", "Lei Lin"], "last_author": "Lei Lin", "date": "2010", "title": "crf tagging for head recognition based on stanford parser", "workshop": "Chinese Language Processing", "combined_workshop": "chinese_language", "abstract": "Chinese parsing has received more and more attention, and in this paper, we use toolkit to perform parsing on the data of Tsinghua Chinese Treebank (TCT) used in CIPS, and we use Conditional Random Fields (CRFs) to train specific model for the head recognition. At last, we compare different results on different POS results."}
{"paper_id": "W03-0312", "authors": ["Hideki Kashioka", "Sadao Kurohashi", "Hideki Tanaka", "Eiji Aramaki"], "last_author": "Eiji Aramaki", "date": "2003", "title": "word selection for ebmt based on monolingual similarity and translation confidence", "workshop": "Building and Using Parallel Texts: Data Driven Machine Translation and Beyond", "combined_workshop": "machine_translation", "abstract": "We propose a method of constructing an example-based machine translation (EBMT) system that exploits a content-aligned bilingual corpus. First, the sentences and phrases in the corpus are aligned across the two languages, and the pairs with high translation confidence are selected and stored in the translation memory. Then, for a given input sentences, the system searches for fitting examples based on both the monolingual similarity and the translation confidence of the pair, and the obtained results are then combined to generate the translation. Our experiments on translation selection showed the accuracy of 85% demonstrating the basic feasibility of our approach."}
{"paper_id": "W12-3149", "authors": ["D Vilar"], "last_author": "D Vilar", "date": "2012", "title": "dfki s smt system for wmt", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "We describe DFKI’s statistical based submission to the 2012 WMT evaluation. The submission is based on the freely available machine translation toolkit Jane, which supports phrase-based and hierarchical phrase-based translation models. Different setups have been tested and combined using a sentence selection method."}
{"paper_id": "W11-0120", "authors": ["Rada Mihalcea", "Chee Wee Leong"], "last_author": "Chee Wee Leong", "date": "2011", "title": "measuring the semantic relatedness between words and images", "workshop": "International Conference on Computational Semantics (IWCS)", "combined_workshop": "semantics", "abstract": "Measures of similarity have traditionally focused on computing the semantic relatedness between pairs of words and texts. In this paper, we construct an evaluation framework to quantify cross-modal semantic relationships that exist between arbitrary pairs of words and images. We study the effectiveness of a corpus-based approach to automatically derive the semantic relatedness between words and images, and perform empirical evaluations by measuring its correlation with human annotators."}
{"paper_id": "W08-0308", "authors": ["Ding Liu", "Daniel Gildea"], "last_author": "Daniel Gildea", "date": "2008", "title": "improved treetostring transducer for machine translation", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "We propose three enhancements to the treeto-string (TTS) transducer for machine translation: first-level expansion-based normalization for TTS templates, a syntactic alignment framework integrating the insertion of unaligned target words, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system."}
{"paper_id": "W03-1710", "authors": ["Guodong Zhou"], "last_author": "Guodong Zhou", "date": "2003", "title": "modeling of long distance context dependency in chinese", "workshop": "Chinese Language Processing", "combined_workshop": "chinese_language", "abstract": "Ngram modeling is simple in language modeling and has been widely used in many applications. However, it can only capture the short distance context dependency within an N-word window where the largest practical N for natural language is three. In the meantime, much of context dependency in natural language occurs beyond a three-word window. In order to incorporate this kind of long distance context dependency, this paper proposes a new MI-Ngram modeling approach. The MI-Ngram model consists of two components: an ngram model and an MI model. The ngram model captures the short distance context dependency within an N-word window while the MI model captures the long distance context dependency between the word pairs beyond the N-word window by using the concept of mutual information. It is found that MI-Ngram modeling has much better performance than ngram modeling. Evaluation on the XINHUA new corpus of 29 million words shows that inclusion of the best 1,600,000 word pairs decreases the perplexity of the MI-Trigram model by 20 percent compared with the trigram model. In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model."}
{"paper_id": "W11-2040", "authors": ["Paul Piwek", "Svetlana Stoyanchev"], "last_author": "Svetlana Stoyanchev", "date": "2011", "title": "the coda system for monologuetodialogue generation", "workshop": "Proceedings of the SIGDIAL Conference", "combined_workshop": "dialogue", "abstract": "This paper describes the CODA system,1 a Text-toText generation system that converts text parsed with discourse relations (Mann and Thompson, 1988) into information-delivering dialogue between two characters. By information-delivering dialogue, we mean dialogue (akin to that used by Plato) that is used primarily to convey information and possibly also to make an argument; this in contrast with dramatic dialogue which focuses on character development and narrative. Several empirical studies show that delivering information as dialogue, rather than monologue, can be particularly effective for education (Craig et al., 2000; Lee et al., 1998) and persuasion (Suzuki and Yamada, 2004). Information-delivering dialogue also lends itself well for presentation through computer-animated agents (Prendinger and Ishizuka, 2004)."}
{"paper_id": "W14-3346", "authors": ["B Chen", "C Cherry"], "last_author": "C Cherry", "date": "2014", "title": "a systematic comparison of smoothing techniques for sentencelevel bleu", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "BLEU is the de facto standard machine translation (MT) evaluation metric. However, because BLEU computes a geometric mean of n-gram precisions, it often correlates poorly with human judgment on the sentence-level. Therefore, several smoothing techniques have been proposed. This paper systematically compares 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning."}
{"paper_id": "W01-1615", "authors": ["Eiichiro Sumita", "Michael Paul"], "last_author": "Michael Paul", "date": "2001", "title": "integration of referential scope limitations into japanese pronoun resolution", "workshop": "Discourse and Dialogue", "combined_workshop": "dialogue", "abstract": "We propose a practical approach to the anaphora resolution of Japanese pronouns incorporating knowledge about referential scope limitations extracted from an annotated corpus. A machine learning approach (decision tree) is utilized for the classification of the coreference relation of a given anaphor and antecedent candidates. The resolution scope of each pronoun is limited according to the relative distance distribution of the training data, resulting in increases in the classification accuracy and analysis speed by causing only a minor decrease in the recall performance."}
{"paper_id": "W09-0419", "authors": ["Lo¨ıc Dugast", "Jean Senellart", "Philipp Koehn"], "last_author": "Philipp Koehn", "date": "2009", "title": "statistical post editing and dictionary extraction systranedinburgh submissions for aclwmt2009", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "We describe here the two Systran/University of Edinburgh submissions for WMT2009. They involve a statistical post-editing model with a particular handling of named entities (English to French and German to English) and the extraction of phrasal rules (English to French)."}
{"paper_id": "W10-1902", "authors": ["Jingchen Liu", "Minlie Huang", "Xiaoyan Zhu"], "last_author": "Xiaoyan Zhu", "date": "2010", "title": "recognizing biomedical named entities using skipchain conditional random fields", "workshop": "Biomedical Natural Language Processing", "combined_workshop": "bio_nlp", "abstract": "Linear-chain Conditional Random Fields (CRF) has been applied to perform the Named Entity Recognition (NER) task in many biomedical text mining and information extraction systems. However, the linear-chain CRF cannot capture long distance dependency, which is very common in the biomedical literature. In this paper, we propose a novel study of capturing such long distance dependency by defining two principles of constructing skipedges for a skip-chain CRF: linking similar words and linking words having typed dependencies. The approach is applied to recognize gene/protein mentions in the literature. When tested on the BioCreAtIvE II Gene Mention dataset and GENIA corpus, the approach contributes significant improvements over the linear-chain CRF. We also present in-depth error analysis on inconsistent labeling and study the influence of the quality of skip edges on the labeling performance."}
{"paper_id": "W12-1511", "authors": ["Allan Third"], "last_author": "Allan Third", "date": "2012", "title": "hidden semantics what can we learn from the names in an ontology", "workshop": "International Natural Language Generation Conference", "combined_workshop": "nlg", "abstract": "Despite their flat, semantics-free structure, ontology identifiers are often given names or labels corresponding to natural language words or phrases which are very dense with information as to their intended referents. We argue that by taking advantage of this information density, NLG systems applied to ontologies can guide the choice and construction of sentences to express useful ontological information, solely through the verbalisations of identifier names, and that by doing so, they can replace the extremely fussy and repetitive texts produced by ontology verbalisers with shorter and simpler texts which are clearer and easier for human readers to understand. We specify which axioms in an ontology are “defining axioms” for linguistically-complex identifiers and analyse a large corpus of OWL ontologies to identify common patterns among all defining axioms. By generating texts from ontologies, and selectively including or omitting these defining axioms, we show by surveys that human readers are typically capable of inferring information implicitly encoded in identifier phrases, and that texts which do not make such “obvious” information explicit are preferred by readers and yet communicate the same information as the longer texts in which such information is spelled out explicitly."}
{"paper_id": "W06-1420", "authors": ["Kees van Deemter", "Ielka van der Sluis", "Albert Gatt"], "last_author": "Albert Gatt", "date": "2006", "title": "building a semantically transparent corpus for the generation of referring expressions", "workshop": "Proceedings of the Fourth International Natural Language Generation Conference", "combined_workshop": "nlg", "abstract": "This paper discusses the construction of a corpus for the evaluation of algorithms that generate referring expressions. It is argued that such an evaluation task requires a semantically transparent corpus, and controlled experiments are the best way to create such a resource. We address a number of issues that have arisen in an ongoing evaluation study, among which is the problem of judging the output of GRE algorithms against a human gold standard."}
{"paper_id": "W06-0135", "authors": ["Shuaixiang Dai", "Xin Li"], "last_author": "Xin Li", "date": "2006", "title": "netease automatic chinese word segmentation", "workshop": "Chinese Language Processing", "combined_workshop": "chinese_language", "abstract": "This document analyses the bakeoff results from NetEase Co. in the SIGHAN5 Word Segmentation Task and Named Entity Recognition Task. The NetEase WS system is designed to facilitate research in natural language processing and information retrieval. It supports Chinese and English word segmentation, Chinese named entity recognition, Chinese part of speech tagging and phrase conglutination. Evaluation result shows our WS system has a passable precision in word segmentation except for the unknown words recognition."}
{"paper_id": "W10-3815", "authors": ["Yvette Graham", "Josef van Genabith"], "last_author": "Josef van Genabith", "date": "2010", "title": "deep syntax language models and statistical machine translation", "workshop": "Syntax and Structure in Statistical Translation", "combined_workshop": "machine_translation", "abstract": "Hierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005), for example. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the string, as in traditional language models, but instead using the deeper context of a word. In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model. We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems."}
{"paper_id": "W14-0201", "authors": ["Sven Reichel", "Ute Ehrlich", "Andr´e Berton", "Michael Weber"], "last_author": "Michael Weber", "date": "2014", "title": "incar multidomain spoken dialogs a wizard of oz study", "workshop": "Dialogue in Motion", "combined_workshop": "dialogue", "abstract": "Mobile Internet access via smartphones puts demands on in-car infotainment systems, as more and more drivers like to access the Internet while driving. Spoken dialog systems support the user by less distracting interaction than visual/hapticbased dialog systems. To develop an intuitive and usable spoken dialog system, an extensive analysis of the interaction concept is necessary. We conducted a Wizard of Oz study to investigate how users will carry out tasks which involve multiple applications in a speech-only, user-initiative infotainment system while driving. Results show that users are not aware of different applications and use anaphoric expressions in task switches. Speaking styles vary and depend on type of task and dialog state. Users interact efficiently and provide multiple semantic concepts in one utterance. This sets high demands for future spoken dialog systems."}
{"paper_id": "W11-1304", "authors": ["Chris Biemann", "Eugenie Giesbrecht"], "last_author": "Eugenie Giesbrecht", "date": "2011", "title": "distributional semantics and compositionality 2011 shared task description and results", "workshop": "Distributional Semantics and Compositionality", "combined_workshop": "semantics", "abstract": "This paper gives an overview of the shared task at the ACL-HLT 2011 DiSCo (Distributional Semantics and Compositionality) workshop. We describe in detail the motivation for the shared task, the acquisition of datasets, the evaluation methodology and the results of participating systems. The task of assigning a numerical score for a phrase according to its compositionality showed to be hard. Many groups reported features that intuitively should work, yet showed no correlation with the training data. The evaluation reveals that most systems outperform simple baselines, yet have difficulties in reliably assigning a compositionality score that closely matches the gold standard. Overall, approaches based on word space models performed slightly better than methods relying solely on statistical association measures."}
{"paper_id": "W07-0720", "authors": ["Maxim Khalilov", "Rafael E. Banches", "Jose A. R. Fonollosa", "Josee B. Marino", "Patrik Lambert", "Josep M. Crego", "Marta Ruiz Costa-Jussa"], "last_author": "Marta Ruiz Costa-Jussa", "date": "2007", "title": "ngrambased statistical machine translation enhanced with multiple weighted reordering hypotheses", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper describes the 2007 Ngram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polit`ecnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the previous years system, being highlighted and empirically compared. Mainly, these include a novel word ordering strategy based on: (1) statistically monotonizing the training source corpus and (2) a novel reordering approach based on weighted reordering graphs. In addition, this system introduces a target language model based on statistical classes, a feature for out-of-domain units and an improved optimization procedure. The paper provides details of this system participation in the ACL 2007 SECOND WORKSHOP ON STATISTICAL MACHINE TRANSLATION. Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks."}
{"paper_id": "W04-2415", "authors": ["X Carreras", "L M`arquez", "G Chrupała"], "last_author": "G Chrupała", "date": "2004", "title": "hierarchical recognition of propositional arguments with perceptrons", "workshop": "CoNLL", "combined_workshop": "natural_language_learning", "abstract": "We describe a system for the CoNLL-2004 Shared Task on Semantic Role Labeling (Carreras and M`arquez, 2004a). The system implements a two-layer learning architecture to recognize arguments in a sentence and predict the role they play in the propositions. The exploration strategy visits possible arguments bottom-up, navigating through the clause hierarchy. The learning components in the architecture are implemented as Perceptrons, and are trained simultaneously online, adapting their behavior to the global target of the system. The learning algorithm follows the global strategy introduced in (Collins, 2002) and adapted in (Carreras and M`arquez, 2004b) for partial parsing tasks."}
{"paper_id": "W08-1137", "authors": ["Josh King"], "last_author": "Josh King", "date": "2008", "title": "osugp attribute selection using genetic programming", "workshop": "Proceedings of the Fifth International Natural Language Generation Conference", "combined_workshop": "nlg", "abstract": "This system’s approach to the attribute selection task was to use a genetic programming algorithm to search for a solution to the task. The evolved programs for the furniture and people domain exhibit quite naive behavior, and the DICE and MASI scores on the training sets reflect the poor humanlikeness of the programs."}
{"paper_id": "W09-1111", "authors": ["M Paul", "C Girju", "C Li"], "last_author": "C Li", "date": "2009", "title": "mining the web for reciprocal relationships", "workshop": "CoNLL", "combined_workshop": "natural_language_learning", "abstract": "In this paper we address the problem of identifying reciprocal relationships in English. In particular we introduce an algorithm that semi-automatically discovers patterns encoding reciprocity based on a set of simple but effective pronoun templates. Using a set of most frequently occurring patterns, we extract pairs of reciprocal pattern instances by searching the web. Then we apply two unsupervised clustering procedures to form meaningful clusters of such reciprocal instances. The pattern discovery procedure yields an accuracy of 97%, while the clustering procedures indicate accuracies of 91% and 82%. Moreover, the resulting set of 10,882 reciprocal instances represent a broad-coverage resource."}
{"paper_id": "W11-0317", "authors": ["Ying Liu", "Bridget T. McInnes", "Genevieve B. Melton", "Ted Pedersen", "Serguei V. Pakhomov"], "last_author": "Serguei V. Pakhomov", "date": "2011", "title": "using secondorder vectors in a knowledgebased method for acronym disambiguation", "workshop": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning", "combined_workshop": "natural_language_learning", "abstract": "In this paper, we introduce a knowledge-based method to disambiguate biomedical acronyms using second-order co-occurrence vectors. We create these vectors using information about a long-form obtained from the Unified Medical Language System and Medline. We evaluate this method on a dataset of 18 acronyms found in biomedical text. Our method achieves an overall accuracy of 89%. The results show that using second-order features provide a distinct representation of the long-form and potentially enhances automated disambiguation."}
{"paper_id": "W13-4068", "authors": ["Jason Williams"], "last_author": "Jason Williams", "date": "2013", "title": "multidomain learning and generalization in dialog state tracking", "workshop": "Proceedings of the SIGDIAL Conference", "combined_workshop": "dialogue", "abstract": "Statistical approaches to dialog state tracking synthesize information across multiple turns in the dialog, overcoming some speech recognition errors. When training a dialog state tracker, there is typically only a small corpus of well-matched dialog data available. However, often there is a large corpus of mis-matched but related data – perhaps pertaining to different semantic concepts, or from a different dialog system. It would be desirable to use this related dialog data to supplement the small corpus of well-matched dialog data. This paper addresses this task as multi-domain learning, presenting 3 methods which synthesize data from different slots and different dialog systems. Since deploying a new dialog state tracker often changes the resulting dialogs in ways that are difficult to predict, we study how well each method generalizes to unseen distributions of dialog data. Our main result is the finding that a simple method for multi-domain learning substantially improves performance in highly mis-matched conditions."}
{"paper_id": "W11-2032", "authors": ["Reid Simmons", "Maxim Makatchev"], "last_author": "Maxim Makatchev", "date": "2011", "title": "perception of personality and naturalness through dialogues by native speakers of american english and arabic", "workshop": "Proceedings of the SIGDIAL Conference", "combined_workshop": "dialogue", "abstract": "Linguistic markers of personality traits have been studied extensively, but few crosscultural studies exist. In this paper, we evaluate how native speakers of American English and Arabic perceive personality traits and naturalness of English utterances that vary along the dimensions of verbosity, hedging, lexical and syntactic alignment, and formality. The utterances are the turns within dialogue fragments that are presented as text transcripts to the workers of Amazon’s Mechanical Turk. The results of the study suggest that all four dimensions can be used as linguistic markers of all personality traits by both language communities. A further comparative analysis shows cross-cultural differences for some combinations of measures of personality traits and naturalness, the dimensions of linguistic variability and dialogue acts."}
{"paper_id": "W03-2127", "authors": ["Richard Craggs", "Mary McGee Wood"], "last_author": "Mary McGee Wood", "date": "2003", "title": "annotating emotion in dialogue", "workshop": "Discourse and Dialogue", "combined_workshop": "dialogue", "abstract": "Communication behaviour is affected by emotion. Here we discuss how dialogue is affected by participants’ emotion and how expressions of emotion are manifested in its content."}
{"paper_id": "W07-0737", "authors": ["Behrang Mohit", "Rebecca Hwa"], "last_author": "Rebecca Hwa", "date": "2007", "title": "localization of difficulttotranslate phrases", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper studies the impact that difficult-totranslate source-language phrases might have on the machine translation process. We formulate the notion of difficulty as a measurable quantity; we show that a classifier can be trained to predict whether a phrase might be difficult to translate; and we develop a framework that makes use of the classifier and external resources (such as human translators) to improve the overall translation quality. Through experimental work, we verify that by isolating difficult-to-translate phrases and processing them as special cases, their negative impact on the translation of the rest of the sentences can be reduced."}
{"paper_id": "W12-2021", "authors": ["Su-Youn Yoon", "Suma Bhat", "Klaus Zechner"], "last_author": "Klaus Zechner", "date": "2012", "title": "vocabulary profile as a measure of vocabulary sophistication", "workshop": "Innovative Use of NLP for Building Educational Applications", "combined_workshop": "educational_applications", "abstract": "This study presents a method that assesses ESL learners’ vocabulary usage to improve an automated scoring system of spontaneous speech responses by non-native English speakers. Focusing on vocabulary sophistication, we estimate the difficulty of each word in the vocabulary based on its frequency in a reference corpus and assess the mean difficulty level of the vocabulary usage across the responses (vocabulary profile). Three different classes of features were generated based on the words in a spoken response: coverage-related, average word rank and the average word frequency and the extent to which they influence human-assigned language proficiency scores was studied. Among these three types of features, the average word frequency showed the most predictive power. We then explored the impact of vocabulary profile features in an automated speech scoring context, with particular focus on the impact of two factors: genre of reference corpora and the characteristics of item-types. The contribution of the current study lies in the use of vocabulary profile as a measure of lexical sophistication for spoken language assessment, an aspect heretofore unexplored in the context of automated speech scoring."}
{"paper_id": "W14-3304", "authors": ["A Borisov", "I Galinskaya"], "last_author": "I Galinskaya", "date": "2014", "title": "yandex school of data analysis russianenglish machine translation system for wmt14", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "This paper describes the Yandex School of Data Analysis Russian-English system submitted to the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task. We start with the system that we developed last year and investigate a few methods that were successful at the previous translation task including unpruned language model, operation sequence model and the new reparameterization of IBM Model 2. Next we propose a {simple yet practical} algorithm to transform Russian sentence into a more easily translatable form before decoding. The algorithm is based on the linguistic intuition of native Russian speakers, also fluent in English."}
{"paper_id": "W13-2203", "authors": ["A Birch", "B Haddow", "U Germann", "M Nadejde", "C Buck", "P Koehn"], "last_author": "Philipp Koehn", "date": "2013", "title": "the feasibility of hmeant as a human mt evaluation metric", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "There has been a recent surge of interest in semantic machine translation, which standard automatic metrics struggle to evaluate. A family of measures called MEANT has been proposed which uses semantic role labels (SRL) to overcome this problem. The human variant, HMEANT, has largely been evaluated using correlation with human contrastive evaluations, the standard human evaluation metric for the WMT shared tasks. In this paper we claim that for a human metric to be useful, it needs to be evaluated on intrinsic properties. It needs to be reliable; it needs to work across different language pairs; and it needs to be lightweight. Most importantly, however, a human metric must be discerning. We conclude that HMEANT is a step in the right direction, but has some serious flaws. The reliance on verbs as heads of frames, and the assumption that annotators need minimal guidelines are particularly problematic."}
{"paper_id": "W10-1916", "authors": ["Roser Morante"], "last_author": "Roser Morante", "date": "2010", "title": "semantic role labeling of gene regulation events preliminary results", "workshop": "Biomedical Natural Language Processing", "combined_workshop": "bio_nlp", "abstract": "This abstract describes work in progress on semantic role labeling of gene regulation events. We present preliminary results of a supervised semantic role labeler that has been trained and tested on the GREC corpus."}
{"paper_id": "W11-2138", "authors": ["Ondrej Bojar", "Ales Tamchyna"], "last_author": "Ales Tamchyna", "date": "2011", "title": "improving translation model by monolingual data", "workshop": "Machine Translation", "combined_workshop": "machine_translation", "abstract": "We use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation. This method called “reverse self-training” improves the decoder’s ability to produce grammatically correct translations into languages with morphology richer than the source language esp. in small-data setting. We empirically evaluate the gains for several pairs of European languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words. We also provide a description of the systems we submitted to WMT11 Shared Task."}
{"paper_id": "W05-0203", "authors": ["Ayako Hoshino", "Hiroshi Nakagawa"], "last_author": "Hiroshi Nakagawa", "date": "2005", "title": "a realtime multiplechoice question generation for language testing a preliminary study", "workshop": "Innovative Use of NLP for Building Educational Applications", "combined_workshop": "educational_applications", "abstract": "An automatic generation of multiplechoice questions is one of the promising examples of educational applications of NLP techniques. A machine learning approach seems to be useful for this purpose because some of the processes can be done by classification. Using basic machine learning algorithms as Naive Bayes and K-Nearest Neighbors, we have developed a real-time system which generates questions on English grammar and vocabulary from on-line news articles. This paper describes the current version of our system and discusses some of the issues on constructing this kind of system."}
